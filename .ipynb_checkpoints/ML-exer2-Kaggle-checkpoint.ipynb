{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dzenan Hamzic BSc, TU Wien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data : 184.702 TU ML WS 18 - Student Performance (Kaggle)\n",
    "#### Group 15\n",
    "https://www.kaggle.com/c/184702-tu-ml-ws-18-student-performance/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statistics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Performance:\n",
    "    @staticmethod\n",
    "    def get_perf(y, y_pred):\n",
    "        ''' This method outputs several performance metrics for classification. '''\n",
    "\n",
    "        # ‘explained_variance’\n",
    "        explained_variance = metrics.explained_variance_score(y_true = y, y_pred = y_pred)\n",
    "\n",
    "        # R2\n",
    "        r2 = metrics.r2_score(y_true = y, y_pred = y_pred)\n",
    "\n",
    "        # ‘neg_mean_absolute_error’\n",
    "        mean_abs_err = metrics.mean_absolute_error(y_true = y, y_pred = y_pred)\n",
    "        \n",
    "        # ‘neg_mean_squared_error’\n",
    "        mean_sq_err = np.sqrt(metrics.mean_squared_error(y_true = y, y_pred = y_pred))\n",
    "        \n",
    "        return {'expl_variance': np.round(explained_variance, 6), 'R2': np.round(r2, 6),\n",
    "                'mean_abs_err': np.round(mean_abs_err,6), 'r_mean_sq_err': np.round(mean_sq_err,6)}\n",
    "   \n",
    "\n",
    "''' Run tests with default algorithm settings '''\n",
    "def run_tests(test_size, setting, X_train, X_test, y_train, y_test, log = False):\n",
    "    rstate = 1234\n",
    "    # algorithms\n",
    "    algo_names = [\"BaggingRegressor\",\n",
    "                   \"RandomForestRegressor\",\n",
    "                  \"BayesianRidge-Score\",\n",
    "                  \"GradientBoosting\",\n",
    "                  \"LinearRegression-NonNorm\",\n",
    "                  \"LinearRegression-Norm-NoInterc\",\n",
    "                  \"DecisionTreeRegressor-\",\n",
    "                  \"Ridge\",\n",
    "                  \"Ridge-Alpha001\",\n",
    "                  \"KNRegressor-distanceN5\",\n",
    "                  \"KNRegressor-distanceN50\",\n",
    "                 \"KNRegressor-uniformN5\"]\n",
    "    \n",
    "    algo_list = [BaggingRegressor(),\n",
    "                RandomForestRegressor(criterion='mse', oob_score=True),\n",
    "                BayesianRidge(compute_score=True),\n",
    "                 GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=5, random_state=1607, loss='ls'),\n",
    "                 LinearRegression(normalize=False),\n",
    "                 LinearRegression(normalize=True, fit_intercept=False),\n",
    "                tree.DecisionTreeRegressor(),\n",
    "                linear_model.Ridge(),\n",
    "                 linear_model.Ridge(alpha=.001),\n",
    "                neighbors.KNeighborsRegressor(5, weights=\"distance\"),\n",
    "                neighbors.KNeighborsRegressor(50, weights=\"distance\"),\n",
    "                neighbors.KNeighborsRegressor(5, weights=\"uniform\")]\n",
    "    \n",
    "    assert len(algo_names) == len(algo_list)\n",
    "    \n",
    "    # save prediction values\n",
    "    predictionDf = pd.DataFrame()\n",
    "    predictionDf[\"actual\"] = y_test.cumsum()\n",
    "    \n",
    "    performanceDf = pd.DataFrame()\n",
    "    for algo,name in zip(algo_list, algo_names):\n",
    "        if log: print(algo,setting)\n",
    "        algo_instance = algo\n",
    "\n",
    "        t0=time.time()\n",
    "        if log: print(name,\"...fit\")\n",
    "        algo.fit(X_train, y_train)\n",
    "        fit_time = round(time.time()-t0, 3)\n",
    "\n",
    "        ## And score it on your testing data.\n",
    "        if log: print(name,\"...score\")\n",
    "        score = algo.score(X_test, y_test)\n",
    "\n",
    "        ## prediction quality measures\n",
    "        t0=time.time()\n",
    "        if log: print(name,\"...predict\")\n",
    "        y_predict = algo.predict(X_test)\n",
    "        pred_time = round(time.time()-t0, 3)\n",
    "        \n",
    "        if log: \n",
    "            print(name,\"...performance\")\n",
    "            print(Performance.get_perf(y_test, y_predict))\n",
    "            \n",
    "        predictionDf[name] = y_predict.cumsum()    \n",
    "            \n",
    "        pp = Performance.get_perf(y_test, y_predict)\n",
    "        pp[\"Algorithm\"] = name\n",
    "        pp[\"Setting\"] = setting\n",
    "        pp[\"fit_time\"] = fit_time\n",
    "        pp[\"pred_time\"] = pred_time\n",
    "        pp[\"score\"] = score\n",
    "        pp[\"testSize\"] = test_size\n",
    "        pp[\"Xsize\"] = str(X_train.shape)\n",
    "        \n",
    "        pd2 = pd.DataFrame(pp, index=[0])\n",
    "        performanceDf = pd.concat([performanceDf ,pd2])\n",
    "        performanceDf.sort_values(by=['R2'], ascending=False, inplace=True)\n",
    "        \n",
    "        \n",
    "    return [performanceDf, predictionDf]\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/xxx/MScBI/S4/ML/exer2/data/all/StudentPerformance.shuf.train.csv', sep = ',')\n",
    "# shuffle\n",
    "df = df.sample(frac=1, random_state=1607).reset_index(drop=True)\n",
    "df_test = pd.read_csv('/home/xxx/MScBI/S4/ML/exer2/data/all/StudentPerformance.shuf.test.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Grade</th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>16</td>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>10</td>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>19</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Grade school sex  age address famsize Pstatus  Medu  Fedu   ...     \\\n",
       "0  360     16     MS   F   18       U     LE3       T     1     1   ...      \n",
       "1  394     10     MS   M   18       R     LE3       T     3     2   ...      \n",
       "2  287     19     GP   F   18       U     GT3       T     2     2   ...      \n",
       "3   20     10     GP   M   16       U     LE3       T     4     3   ...      \n",
       "4  317      0     GP   F   18       U     GT3       T     2     1   ...      \n",
       "\n",
       "  higher internet romantic famrel  freetime  goout  Dalc Walc health absences  \n",
       "0    yes      yes       no      5         3      2     1    1      4        0  \n",
       "1    yes      yes       no      4         4      1     3    4      5        0  \n",
       "2    yes      yes       no      4         3      3     1    2      2        5  \n",
       "3    yes      yes       no      3         1      3     1    3      5        4  \n",
       "4    yes      yes       no      5         3      3     1    2      1        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 31) (197, 31)\n",
      "(395, 31)\n"
     ]
    }
   ],
   "source": [
    "train_grade_values = df[\"Grade\"]\n",
    "df_copy = df.drop([\"Grade\"], axis=1)\n",
    "\n",
    "# align with test frame\n",
    "print(df_copy.shape, df_test.shape)\n",
    "dfb = pd.concat([df_copy,df_test], axis=0)\n",
    "print(dfb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_no</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0  360   18     1     1           2          3         0       5         3   \n",
       "1  394   18     3     2           3          1         0       4         4   \n",
       "2  287   18     2     2           1          3         0       4         3   \n",
       "3   20   16     4     3           1          1         0       3         1   \n",
       "4  317   18     2     1           2          2         0       5         3   \n",
       "\n",
       "   goout      ...       activities_no  activities_yes  nursery_no  \\\n",
       "0      2      ...                   1               0           0   \n",
       "1      1      ...                   1               0           1   \n",
       "2      3      ...                   1               0           0   \n",
       "3      3      ...                   0               1           0   \n",
       "4      3      ...                   0               1           0   \n",
       "\n",
       "   nursery_yes  higher_no  higher_yes  internet_no  internet_yes  romantic_no  \\\n",
       "0            1          0           1            0             1            1   \n",
       "1            0          0           1            0             1            1   \n",
       "2            1          0           1            0             1            1   \n",
       "3            1          0           1            0             1            1   \n",
       "4            1          0           1            0             1            1   \n",
       "\n",
       "   romantic_yes  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy = pd.get_dummies(dfb)\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 57)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 58) : (197, 57)\n"
     ]
    }
   ],
   "source": [
    "## split df_dummy back to train and test\n",
    "df_dummy_train = df_dummy.iloc[0:198]\n",
    "df_dummy_test = df_dummy.iloc[198:395]\n",
    "# return y to train dummy df\n",
    "df_dummy_train[\"Grade\"] = train_grade_values\n",
    "print(df_dummy_train.shape,\":\",df_dummy_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0  360   18     1     1           2          3         0       5         3   \n",
       "1  394   18     3     2           3          1         0       4         4   \n",
       "2  287   18     2     2           1          3         0       4         3   \n",
       "3   20   16     4     3           1          1         0       3         1   \n",
       "4  317   18     2     1           2          2         0       5         3   \n",
       "\n",
       "   goout  ...    activities_yes  nursery_no  nursery_yes  higher_no  \\\n",
       "0      2  ...                 0           0            1          0   \n",
       "1      1  ...                 0           1            0          0   \n",
       "2      3  ...                 0           0            1          0   \n",
       "3      3  ...                 1           0            1          0   \n",
       "4      3  ...                 1           0            1          0   \n",
       "\n",
       "   higher_yes  internet_no  internet_yes  romantic_no  romantic_yes  Grade  \n",
       "0           1            0             1            1             0     16  \n",
       "1           1            0             1            1             0     10  \n",
       "2           1            0             1            1             0     19  \n",
       "3           1            0             1            1             0     10  \n",
       "4           1            0             1            1             0      0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save dummified to scv for R stepwise algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy_train.to_csv(\"/home/xxx/MScBI/S4/ML/exer2/data/all/oversampled_student_data_train.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 58)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split to seen and unseen \n",
    "X_test_unseen = df_dummy_train.iloc[148:198]\n",
    "y_test_unseen = X_test_unseen[\"Grade\"]\n",
    "\n",
    "X_train_seen = df_dummy_train.iloc[0:148]\n",
    "y_train_seen = X_train_seen[\"Grade\"]\n",
    "\n",
    "X_train_seen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Predictor variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X_train_seen[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X_test_unseen[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n",
      "4 1\n",
      "5 1\n",
      "6 7\n",
      "7 5\n",
      "8 11\n",
      "9 11\n",
      "10 21\n",
      "11 19\n",
      "12 12\n",
      "13 7\n",
      "14 11\n",
      "15 14\n",
      "16 7\n",
      "17 1\n",
      "18 3\n",
      "19 2\n"
     ]
    }
   ],
   "source": [
    "number_grades = []\n",
    "for g in list(set(X_train_seen['Grade'])):\n",
    "    number_grades.insert(0,len(X_train_seen.loc[df['Grade'] == g]))\n",
    "    print(g, len(X_train_seen.loc[X_train_seen['Grade'] == g]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "most_frequent = 10\n",
    "most_frequent_len = len(X_train_seen.loc[X_train_seen['Grade'] == most_frequent])\n",
    "print(most_frequent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 58)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df = X_train_seen.loc[X_train_seen['Grade'] == most_frequent]\n",
    "for grade in set(X_train_seen['Grade']):\n",
    "    if grade != most_frequent:\n",
    "        #print(grade,len(X_train_seen.loc[X_train_seen['Grade'] == grade]))\n",
    "        subdf = X_train_seen.loc[X_train_seen['Grade'] == grade].sample(n=most_frequent_len, replace=True)\n",
    "        oversampled_df = oversampled_df.append(subdf)\n",
    "        \n",
    "# shuffle\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=1607).reset_index(drop=True)\n",
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21\n",
      "4 21\n",
      "5 21\n",
      "6 21\n",
      "7 21\n",
      "8 21\n",
      "9 21\n",
      "10 21\n",
      "11 21\n",
      "12 21\n",
      "13 21\n",
      "14 21\n",
      "15 21\n",
      "16 21\n",
      "17 21\n",
      "18 21\n",
      "19 21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for grade in set(oversampled_df['Grade']):\n",
    "    print(grade,len(oversampled_df.loc[oversampled_df['Grade'] == grade]))\n",
    "    \n",
    "sns.distplot(oversampled_df[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 58)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save oversampled to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>339</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>217</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0   93   16     3     1           1          2         0       3         3   \n",
       "1  130   16     4     4           1          1         0       3         5   \n",
       "2  357   17     4     4           2          2         0       4         3   \n",
       "3  339   18     3     3           1          4         0       5         3   \n",
       "4  217   17     4     3           1          2         2       3         4   \n",
       "\n",
       "   goout  ...    activities_yes  nursery_no  nursery_yes  higher_no  \\\n",
       "0      3  ...                 0           0            1          0   \n",
       "1      5  ...                 1           0            1          0   \n",
       "2      3  ...                 1           0            1          0   \n",
       "3      3  ...                 0           0            1          0   \n",
       "4      5  ...                 0           0            1          0   \n",
       "\n",
       "   higher_yes  internet_no  internet_yes  romantic_no  romantic_yes  Grade  \n",
       "0           1            1             0            1             0      6  \n",
       "1           1            0             1            1             0     18  \n",
       "2           1            0             1            1             0     13  \n",
       "3           1            0             1            1             0     17  \n",
       "4           1            0             1            0             1      4  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampled_df[\"Grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = balanced_df\n",
    "y_train = oversampled_df[\"Grade\"]\n",
    "oversampled_df.drop([\"Grade\",\"id\"], axis=1, inplace=True)\n",
    "X_test_unseen.drop([\"Grade\",\"id\"], axis=1, inplace=True)\n",
    "X_train = oversampled_df\n",
    "X_test = X_test_unseen\n",
    "y_test = y_test_unseen\n",
    "test_ID_rows = df_test[\"id\"]\n",
    "df_test.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 56) (357,) (50, 56) (50,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "Medu                 0\n",
       "Fedu                 0\n",
       "traveltime           0\n",
       "studytime            0\n",
       "failures             0\n",
       "famrel               0\n",
       "freetime             0\n",
       "goout                0\n",
       "Dalc                 0\n",
       "Walc                 0\n",
       "health               0\n",
       "absences             0\n",
       "school_GP            0\n",
       "school_MS            0\n",
       "sex_F                0\n",
       "sex_M                0\n",
       "address_R            0\n",
       "address_U            0\n",
       "famsize_GT3          0\n",
       "famsize_LE3          0\n",
       "Pstatus_A            0\n",
       "Pstatus_T            0\n",
       "Mjob_at_home         0\n",
       "Mjob_health          0\n",
       "Mjob_other           0\n",
       "Mjob_services        0\n",
       "Mjob_teacher         0\n",
       "Fjob_at_home         0\n",
       "Fjob_health          0\n",
       "Fjob_other           0\n",
       "Fjob_services        0\n",
       "Fjob_teacher         0\n",
       "reason_course        0\n",
       "reason_home          0\n",
       "reason_other         0\n",
       "reason_reputation    0\n",
       "guardian_father      0\n",
       "guardian_mother      0\n",
       "guardian_other       0\n",
       "schoolsup_no         0\n",
       "schoolsup_yes        0\n",
       "famsup_no            0\n",
       "famsup_yes           0\n",
       "paid_no              0\n",
       "paid_yes             0\n",
       "activities_no        0\n",
       "activities_yes       0\n",
       "nursery_no           0\n",
       "nursery_yes          0\n",
       "higher_no            0\n",
       "higher_yes           0\n",
       "internet_no          0\n",
       "internet_yes         0\n",
       "romantic_no          0\n",
       "romantic_yes         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Regression summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Grade</td>      <th>  R-squared:         </th> <td>   0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.64</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 07 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>2.52e-63</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:30:20</td>     <th>  Log-Likelihood:    </th> <td> -877.68</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   357</td>      <th>  AIC:               </th> <td>   1835.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   317</td>      <th>  BIC:               </th> <td>   1990.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>               <td>   -0.2924</td> <td>    0.229</td> <td>   -1.280</td> <td> 0.202</td> <td>   -0.742</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu</th>              <td>    1.1130</td> <td>    0.387</td> <td>    2.877</td> <td> 0.004</td> <td>    0.352</td> <td>    1.874</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu</th>              <td>   -0.6911</td> <td>    0.291</td> <td>   -2.378</td> <td> 0.018</td> <td>   -1.263</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime</th>        <td>   -1.0562</td> <td>    0.333</td> <td>   -3.176</td> <td> 0.002</td> <td>   -1.711</td> <td>   -0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime</th>         <td>    1.6720</td> <td>    0.240</td> <td>    6.975</td> <td> 0.000</td> <td>    1.200</td> <td>    2.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>failures</th>          <td>   -2.2486</td> <td>    0.301</td> <td>   -7.475</td> <td> 0.000</td> <td>   -2.840</td> <td>   -1.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel</th>            <td>    0.0807</td> <td>    0.248</td> <td>    0.325</td> <td> 0.745</td> <td>   -0.407</td> <td>    0.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime</th>          <td>    0.3712</td> <td>    0.269</td> <td>    1.381</td> <td> 0.168</td> <td>   -0.157</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout</th>             <td>   -1.2457</td> <td>    0.289</td> <td>   -4.308</td> <td> 0.000</td> <td>   -1.815</td> <td>   -0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dalc</th>              <td>   -0.8562</td> <td>    0.350</td> <td>   -2.443</td> <td> 0.015</td> <td>   -1.546</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Walc</th>              <td>    0.4326</td> <td>    0.290</td> <td>    1.491</td> <td> 0.137</td> <td>   -0.138</td> <td>    1.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health</th>            <td>   -0.5187</td> <td>    0.159</td> <td>   -3.270</td> <td> 0.001</td> <td>   -0.831</td> <td>   -0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>          <td>   -0.0408</td> <td>    0.035</td> <td>   -1.155</td> <td> 0.249</td> <td>   -0.110</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school_GP</th>         <td>    1.8668</td> <td>    0.406</td> <td>    4.593</td> <td> 0.000</td> <td>    1.067</td> <td>    2.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school_MS</th>         <td>    0.4270</td> <td>    0.577</td> <td>    0.740</td> <td> 0.460</td> <td>   -0.708</td> <td>    1.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_F</th>             <td>   -0.3460</td> <td>    0.436</td> <td>   -0.794</td> <td> 0.428</td> <td>   -1.203</td> <td>    0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_M</th>             <td>    2.6398</td> <td>    0.336</td> <td>    7.854</td> <td> 0.000</td> <td>    1.979</td> <td>    3.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address_R</th>         <td>    0.9761</td> <td>    0.466</td> <td>    2.097</td> <td> 0.037</td> <td>    0.060</td> <td>    1.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address_U</th>         <td>    1.3177</td> <td>    0.372</td> <td>    3.541</td> <td> 0.000</td> <td>    0.585</td> <td>    2.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize_GT3</th>       <td>    0.8347</td> <td>    0.402</td> <td>    2.075</td> <td> 0.039</td> <td>    0.043</td> <td>    1.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize_LE3</th>       <td>    1.4591</td> <td>    0.392</td> <td>    3.727</td> <td> 0.000</td> <td>    0.689</td> <td>    2.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus_A</th>         <td>    1.5724</td> <td>    0.442</td> <td>    3.553</td> <td> 0.000</td> <td>    0.702</td> <td>    2.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus_T</th>         <td>    0.7214</td> <td>    0.515</td> <td>    1.401</td> <td> 0.162</td> <td>   -0.292</td> <td>    1.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_at_home</th>      <td>   -0.2553</td> <td>    0.700</td> <td>   -0.365</td> <td> 0.716</td> <td>   -1.633</td> <td>    1.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_health</th>       <td>    0.8039</td> <td>    0.918</td> <td>    0.875</td> <td> 0.382</td> <td>   -1.003</td> <td>    2.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_other</th>        <td>    0.1733</td> <td>    0.470</td> <td>    0.369</td> <td> 0.713</td> <td>   -0.751</td> <td>    1.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_services</th>     <td>    0.4886</td> <td>    0.476</td> <td>    1.026</td> <td> 0.306</td> <td>   -0.449</td> <td>    1.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_teacher</th>      <td>    1.0834</td> <td>    0.641</td> <td>    1.691</td> <td> 0.092</td> <td>   -0.177</td> <td>    2.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_at_home</th>      <td>    4.2910</td> <td>    0.822</td> <td>    5.217</td> <td> 0.000</td> <td>    2.673</td> <td>    5.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_health</th>       <td>   -0.1791</td> <td>    0.784</td> <td>   -0.228</td> <td> 0.820</td> <td>   -1.722</td> <td>    1.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_other</th>        <td>   -1.7075</td> <td>    0.472</td> <td>   -3.617</td> <td> 0.000</td> <td>   -2.636</td> <td>   -0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_services</th>     <td>    0.0069</td> <td>    0.481</td> <td>    0.014</td> <td> 0.989</td> <td>   -0.939</td> <td>    0.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_teacher</th>      <td>   -0.1175</td> <td>    0.773</td> <td>   -0.152</td> <td> 0.879</td> <td>   -1.637</td> <td>    1.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_course</th>     <td>    1.4716</td> <td>    0.422</td> <td>    3.486</td> <td> 0.001</td> <td>    0.641</td> <td>    2.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_home</th>       <td>    0.1070</td> <td>    0.446</td> <td>    0.240</td> <td> 0.810</td> <td>   -0.770</td> <td>    0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_other</th>      <td>    0.9379</td> <td>    0.604</td> <td>    1.553</td> <td> 0.122</td> <td>   -0.251</td> <td>    2.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_reputation</th> <td>   -0.2228</td> <td>    0.408</td> <td>   -0.546</td> <td> 0.585</td> <td>   -1.025</td> <td>    0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_father</th>   <td>    0.4658</td> <td>    0.526</td> <td>    0.885</td> <td> 0.377</td> <td>   -0.569</td> <td>    1.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_mother</th>   <td>   -0.8027</td> <td>    0.347</td> <td>   -2.314</td> <td> 0.021</td> <td>   -1.485</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_other</th>    <td>    2.6307</td> <td>    0.862</td> <td>    3.053</td> <td> 0.002</td> <td>    0.935</td> <td>    4.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup_no</th>      <td>    3.2205</td> <td>    0.522</td> <td>    6.172</td> <td> 0.000</td> <td>    2.194</td> <td>    4.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup_yes</th>     <td>   -0.9268</td> <td>    0.446</td> <td>   -2.079</td> <td> 0.038</td> <td>   -1.804</td> <td>   -0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup_no</th>         <td>    2.0075</td> <td>    0.427</td> <td>    4.704</td> <td> 0.000</td> <td>    1.168</td> <td>    2.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup_yes</th>        <td>    0.2862</td> <td>    0.336</td> <td>    0.851</td> <td> 0.395</td> <td>   -0.375</td> <td>    0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid_no</th>           <td>    0.6055</td> <td>    0.378</td> <td>    1.603</td> <td> 0.110</td> <td>   -0.138</td> <td>    1.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid_yes</th>          <td>    1.6883</td> <td>    0.391</td> <td>    4.318</td> <td> 0.000</td> <td>    0.919</td> <td>    2.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities_no</th>     <td>    1.6468</td> <td>    0.371</td> <td>    4.435</td> <td> 0.000</td> <td>    0.916</td> <td>    2.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities_yes</th>    <td>    0.6470</td> <td>    0.396</td> <td>    1.634</td> <td> 0.103</td> <td>   -0.132</td> <td>    1.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery_no</th>        <td>    0.6513</td> <td>    0.411</td> <td>    1.586</td> <td> 0.114</td> <td>   -0.157</td> <td>    1.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery_yes</th>       <td>    1.6425</td> <td>    0.433</td> <td>    3.789</td> <td> 0.000</td> <td>    0.790</td> <td>    2.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher_no</th>         <td>    0.2982</td> <td>    0.965</td> <td>    0.309</td> <td> 0.758</td> <td>   -1.601</td> <td>    2.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher_yes</th>        <td>    1.9955</td> <td>    0.791</td> <td>    2.522</td> <td> 0.012</td> <td>    0.439</td> <td>    3.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet_no</th>       <td>    0.9034</td> <td>    0.454</td> <td>    1.989</td> <td> 0.048</td> <td>    0.010</td> <td>    1.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet_yes</th>      <td>    1.3904</td> <td>    0.382</td> <td>    3.636</td> <td> 0.000</td> <td>    0.638</td> <td>    2.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic_no</th>       <td>    1.9044</td> <td>    0.413</td> <td>    4.608</td> <td> 0.000</td> <td>    1.091</td> <td>    2.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic_yes</th>      <td>    0.3894</td> <td>    0.425</td> <td>    0.917</td> <td> 0.360</td> <td>   -0.446</td> <td>    1.225</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>49.795</td> <th>  Durbin-Watson:     </th> <td>   1.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 100.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.753</td> <th>  Prob(JB):          </th> <td>1.18e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.126</td> <th>  Cond. No.          </th> <td>1.24e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 9.99e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Grade   R-squared:                       0.707\n",
       "Model:                            OLS   Adj. R-squared:                  0.671\n",
       "Method:                 Least Squares   F-statistic:                     19.64\n",
       "Date:                Mon, 07 Jan 2019   Prob (F-statistic):           2.52e-63\n",
       "Time:                        21:30:20   Log-Likelihood:                -877.68\n",
       "No. Observations:                 357   AIC:                             1835.\n",
       "Df Residuals:                     317   BIC:                             1990.\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "age                  -0.2924      0.229     -1.280      0.202      -0.742       0.157\n",
       "Medu                  1.1130      0.387      2.877      0.004       0.352       1.874\n",
       "Fedu                 -0.6911      0.291     -2.378      0.018      -1.263      -0.119\n",
       "traveltime           -1.0562      0.333     -3.176      0.002      -1.711      -0.402\n",
       "studytime             1.6720      0.240      6.975      0.000       1.200       2.144\n",
       "failures             -2.2486      0.301     -7.475      0.000      -2.840      -1.657\n",
       "famrel                0.0807      0.248      0.325      0.745      -0.407       0.568\n",
       "freetime              0.3712      0.269      1.381      0.168      -0.157       0.900\n",
       "goout                -1.2457      0.289     -4.308      0.000      -1.815      -0.677\n",
       "Dalc                 -0.8562      0.350     -2.443      0.015      -1.546      -0.167\n",
       "Walc                  0.4326      0.290      1.491      0.137      -0.138       1.003\n",
       "health               -0.5187      0.159     -3.270      0.001      -0.831      -0.207\n",
       "absences             -0.0408      0.035     -1.155      0.249      -0.110       0.029\n",
       "school_GP             1.8668      0.406      4.593      0.000       1.067       2.666\n",
       "school_MS             0.4270      0.577      0.740      0.460      -0.708       1.562\n",
       "sex_F                -0.3460      0.436     -0.794      0.428      -1.203       0.511\n",
       "sex_M                 2.6398      0.336      7.854      0.000       1.979       3.301\n",
       "address_R             0.9761      0.466      2.097      0.037       0.060       1.892\n",
       "address_U             1.3177      0.372      3.541      0.000       0.585       2.050\n",
       "famsize_GT3           0.8347      0.402      2.075      0.039       0.043       1.626\n",
       "famsize_LE3           1.4591      0.392      3.727      0.000       0.689       2.229\n",
       "Pstatus_A             1.5724      0.442      3.553      0.000       0.702       2.443\n",
       "Pstatus_T             0.7214      0.515      1.401      0.162      -0.292       1.735\n",
       "Mjob_at_home         -0.2553      0.700     -0.365      0.716      -1.633       1.122\n",
       "Mjob_health           0.8039      0.918      0.875      0.382      -1.003       2.611\n",
       "Mjob_other            0.1733      0.470      0.369      0.713      -0.751       1.098\n",
       "Mjob_services         0.4886      0.476      1.026      0.306      -0.449       1.426\n",
       "Mjob_teacher          1.0834      0.641      1.691      0.092      -0.177       2.344\n",
       "Fjob_at_home          4.2910      0.822      5.217      0.000       2.673       5.909\n",
       "Fjob_health          -0.1791      0.784     -0.228      0.820      -1.722       1.364\n",
       "Fjob_other           -1.7075      0.472     -3.617      0.000      -2.636      -0.779\n",
       "Fjob_services         0.0069      0.481      0.014      0.989      -0.939       0.953\n",
       "Fjob_teacher         -0.1175      0.773     -0.152      0.879      -1.637       1.403\n",
       "reason_course         1.4716      0.422      3.486      0.001       0.641       2.302\n",
       "reason_home           0.1070      0.446      0.240      0.810      -0.770       0.984\n",
       "reason_other          0.9379      0.604      1.553      0.122      -0.251       2.126\n",
       "reason_reputation    -0.2228      0.408     -0.546      0.585      -1.025       0.579\n",
       "guardian_father       0.4658      0.526      0.885      0.377      -0.569       1.501\n",
       "guardian_mother      -0.8027      0.347     -2.314      0.021      -1.485      -0.120\n",
       "guardian_other        2.6307      0.862      3.053      0.002       0.935       4.326\n",
       "schoolsup_no          3.2205      0.522      6.172      0.000       2.194       4.247\n",
       "schoolsup_yes        -0.9268      0.446     -2.079      0.038      -1.804      -0.050\n",
       "famsup_no             2.0075      0.427      4.704      0.000       1.168       2.847\n",
       "famsup_yes            0.2862      0.336      0.851      0.395      -0.375       0.948\n",
       "paid_no               0.6055      0.378      1.603      0.110      -0.138       1.349\n",
       "paid_yes              1.6883      0.391      4.318      0.000       0.919       2.458\n",
       "activities_no         1.6468      0.371      4.435      0.000       0.916       2.377\n",
       "activities_yes        0.6470      0.396      1.634      0.103      -0.132       1.426\n",
       "nursery_no            0.6513      0.411      1.586      0.114      -0.157       1.459\n",
       "nursery_yes           1.6425      0.433      3.789      0.000       0.790       2.495\n",
       "higher_no             0.2982      0.965      0.309      0.758      -1.601       2.197\n",
       "higher_yes            1.9955      0.791      2.522      0.012       0.439       3.552\n",
       "internet_no           0.9034      0.454      1.989      0.048       0.010       1.797\n",
       "internet_yes          1.3904      0.382      3.636      0.000       0.638       2.143\n",
       "romantic_no           1.9044      0.413      4.608      0.000       1.091       2.717\n",
       "romantic_yes          0.3894      0.425      0.917      0.360      -0.446       1.225\n",
       "==============================================================================\n",
       "Omnibus:                       49.795   Durbin-Watson:                   1.882\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.978\n",
       "Skew:                          -0.753   Prob(JB):                     1.18e-22\n",
       "Kurtosis:                       5.126   Cond. No.                     1.24e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 9.99e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = sm.add_constant(X)\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute for no oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 56) (148,) (50, 56) (50,)\n"
     ]
    }
   ],
   "source": [
    "# no oversampling\n",
    "y_train = y_train_seen\n",
    "X_train = X_train_seen.drop([\"Grade\",\"id\"], axis=1)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented above\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=1607)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1:\n",
    " - test data is 25% of the complete dataframe\n",
    " - X has dimension of (148, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.323225</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.323618</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.255712</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.973012</td>\n",
       "      <td>0.323225</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.321786</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.322149</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.236000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.977235</td>\n",
       "      <td>0.321786</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.311805</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.312163</td>\n",
       "      <td>0.358</td>\n",
       "      <td>2.938091</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.006393</td>\n",
       "      <td>0.311805</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.291953</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.300029</td>\n",
       "      <td>0.033</td>\n",
       "      <td>3.146000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.063767</td>\n",
       "      <td>0.291953</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.235744</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.237196</td>\n",
       "      <td>0.038</td>\n",
       "      <td>3.168000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.221990</td>\n",
       "      <td>0.235744</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.227120</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3.345408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.248402</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.536831</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.309439</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199069</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.563308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334599</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199059</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.563337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334627</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199059</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.563337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334627</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.157112</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.158725</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.390357</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.433867</td>\n",
       "      <td>0.157112</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.046734</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.804165</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2   Setting      Xsize  \\\n",
       "0          KNRegressor-distanceN5  0.323225  raw_data  (148, 56)   \n",
       "0           KNRegressor-uniformN5  0.321786  raw_data  (148, 56)   \n",
       "0                GradientBoosting  0.311805  raw_data  (148, 56)   \n",
       "0           RandomForestRegressor  0.291953  raw_data  (148, 56)   \n",
       "0                BaggingRegressor  0.235744  raw_data  (148, 56)   \n",
       "0             BayesianRidge-Score  0.226152  raw_data  (148, 56)   \n",
       "0                           Ridge  0.203756  raw_data  (148, 56)   \n",
       "0                  Ridge-Alpha001  0.194432  raw_data  (148, 56)   \n",
       "0        LinearRegression-NonNorm  0.194421  raw_data  (148, 56)   \n",
       "0  LinearRegression-Norm-NoInterc  0.194421  raw_data  (148, 56)   \n",
       "0         KNRegressor-distanceN50  0.157112  raw_data  (148, 56)   \n",
       "0          DecisionTreeRegressor-  0.010444  raw_data  (148, 56)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.323618     0.001      3.255712      0.001       3.973012  0.323225   \n",
       "0       0.322149     0.001      3.236000      0.001       3.977235  0.321786   \n",
       "0       0.312163     0.358      2.938091      0.001       4.006393  0.311805   \n",
       "0       0.300029     0.033      3.146000      0.002       4.063767  0.291953   \n",
       "0       0.237196     0.038      3.168000      0.002       4.221990  0.235744   \n",
       "0       0.227120     0.005      3.345408      0.000       4.248402  0.226152   \n",
       "0       0.208310     0.001      3.536831      0.000       4.309439  0.203756   \n",
       "0       0.199069     0.001      3.563308      0.000       4.334599  0.194432   \n",
       "0       0.199059     0.001      3.563337      0.000       4.334627  0.194421   \n",
       "0       0.199059     0.002      3.563337      0.000       4.334627  0.194421   \n",
       "0       0.158725     0.001      3.390357      0.002       4.433867  0.157112   \n",
       "0       0.046734     0.002      3.360000      0.000       4.804165  0.010444   \n",
       "\n",
       "   testSize  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdfs = run_tests(0.25, \"raw_data\", X_train, X_test, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2: regression tests with squared polynomial transformed data\n",
    " - test data is 25% of the complete dataframe\n",
    " - X has dimension of (148, 1653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.373514</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.373771</td>\n",
       "      <td>5.361</td>\n",
       "      <td>2.949451</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.822551</td>\n",
       "      <td>0.373514</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.229179</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.229205</td>\n",
       "      <td>0.304</td>\n",
       "      <td>3.320407</td>\n",
       "      <td>0.006</td>\n",
       "      <td>4.240085</td>\n",
       "      <td>0.229179</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.186622</td>\n",
       "      <td>0.014</td>\n",
       "      <td>3.348000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>4.362843</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.179482</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.182555</td>\n",
       "      <td>0.017</td>\n",
       "      <td>3.379868</td>\n",
       "      <td>0.043</td>\n",
       "      <td>4.374636</td>\n",
       "      <td>0.179482</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.145964</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.145981</td>\n",
       "      <td>0.210</td>\n",
       "      <td>3.204000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>4.463093</td>\n",
       "      <td>0.145964</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.144042</td>\n",
       "      <td>0.007</td>\n",
       "      <td>3.378447</td>\n",
       "      <td>0.036</td>\n",
       "      <td>4.478165</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.122725</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.126381</td>\n",
       "      <td>0.169</td>\n",
       "      <td>3.484000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.523406</td>\n",
       "      <td>0.122725</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.524636</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-0.509201</td>\n",
       "      <td>0.092</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.963221</td>\n",
       "      <td>-0.524636</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>-2.224096</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.189584</td>\n",
       "      <td>0.114</td>\n",
       "      <td>5.981832</td>\n",
       "      <td>0.007</td>\n",
       "      <td>8.671650</td>\n",
       "      <td>-2.224096</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>-2.269733</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.233243</td>\n",
       "      <td>0.217</td>\n",
       "      <td>6.015415</td>\n",
       "      <td>0.010</td>\n",
       "      <td>8.732809</td>\n",
       "      <td>-2.269733</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>-2.270315</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.233819</td>\n",
       "      <td>0.098</td>\n",
       "      <td>6.015812</td>\n",
       "      <td>0.007</td>\n",
       "      <td>8.733585</td>\n",
       "      <td>-2.270315</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-2.293503</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.259092</td>\n",
       "      <td>0.097</td>\n",
       "      <td>6.012510</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.764493</td>\n",
       "      <td>-2.293503</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2 Setting        Xsize  \\\n",
       "0                GradientBoosting  0.373514  poly^2  (148, 1653)   \n",
       "0             BayesianRidge-Score  0.229179  poly^2  (148, 1653)   \n",
       "0           KNRegressor-uniformN5  0.183900  poly^2  (148, 1653)   \n",
       "0          KNRegressor-distanceN5  0.179482  poly^2  (148, 1653)   \n",
       "0                BaggingRegressor  0.145964  poly^2  (148, 1653)   \n",
       "0         KNRegressor-distanceN50  0.140186  poly^2  (148, 1653)   \n",
       "0           RandomForestRegressor  0.122725  poly^2  (148, 1653)   \n",
       "0          DecisionTreeRegressor- -0.524636  poly^2  (148, 1653)   \n",
       "0                           Ridge -2.224096  poly^2  (148, 1653)   \n",
       "0        LinearRegression-NonNorm -2.269733  poly^2  (148, 1653)   \n",
       "0                  Ridge-Alpha001 -2.270315  poly^2  (148, 1653)   \n",
       "0  LinearRegression-Norm-NoInterc -2.293503  poly^2  (148, 1653)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.373771     5.361      2.949451      0.001       3.822551  0.373514   \n",
       "0       0.229205     0.304      3.320407      0.006       4.240085  0.229179   \n",
       "0       0.186622     0.014      3.348000      0.027       4.362843  0.183900   \n",
       "0       0.182555     0.017      3.379868      0.043       4.374636  0.179482   \n",
       "0       0.145981     0.210      3.204000      0.003       4.463093  0.145964   \n",
       "0       0.144042     0.007      3.378447      0.036       4.478165  0.140186   \n",
       "0       0.126381     0.169      3.484000      0.001       4.523406  0.122725   \n",
       "0      -0.509201     0.092      4.720000      0.000       5.963221 -0.524636   \n",
       "0      -2.189584     0.114      5.981832      0.007       8.671650 -2.224096   \n",
       "0      -2.233243     0.217      6.015415      0.010       8.732809 -2.269733   \n",
       "0      -2.233819     0.098      6.015812      0.007       8.733585 -2.270315   \n",
       "0      -2.259092     0.097      6.012510      0.000       8.764493 -2.293503   \n",
       "\n",
       "   testSize  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly2 = poly.fit_transform(X_train)\n",
    "X_test_poly2 = poly.transform(X_test)\n",
    "\n",
    "resultdfs = run_tests(0.25, \"poly^\"+str(degree), X_train_poly2, X_test_poly2, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-2b55e830b35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mCV_dtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid_bayesian_ridge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mCV_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrr_best_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GradientBoostingRegressor - best params: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrr_best_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[0;32m--> 547\u001b[0;31m                                              **self._backend_args)\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_timeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(self, n_jobs, parallel, **backend_args)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Make sure to free as much memory as possible before forking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemmapingPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbackend_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, temp_folder, max_nbytes, mmap_mode, forward_reducers, backward_reducers, verbose, context_id, prewarm, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             backward_reducers=backward_reducers)\n\u001b[1;32m    599\u001b[0m         \u001b[0mpoolargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpoolargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, forward_reducers, backward_reducers, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpoolargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mpoolargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPicklingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpoolargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         self._worker_handler = threading.Thread(\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PoolWorker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added worker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snakes/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "param_grid_bayesian_ridge = {\n",
    "    \"n_estimators\":[200,500,1000,1500],\n",
    "    \"loss\":[\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "    \"subsample\": [1.0,0.5],\n",
    "    \"max_features\":[2,5,10,None],\n",
    "    \"learning_rate\":[0.1,0.2,0.01,0.05],\n",
    "    \"max_depth\":[2,5,7],\n",
    "    \"random_state\":[1607]\n",
    "}\n",
    "\n",
    "mod = GradientBoostingRegressor()\n",
    "CV_dtc = GridSearchCV(estimator=mod, param_grid=param_grid_bayesian_ridge, cv = 5, n_jobs=-1)\n",
    "CV_dtc.fit(X_train2, y_train)\n",
    "rr_best_params = CV_dtc.best_params_\n",
    "print(\"GradientBoostingRegressor - best params: \",rr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3: regression tests with Principal Components\n",
    " - test data is 33% of the complete dataframe\n",
    " - X has dimension of (148, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09874854 0.06221322 0.05948353 0.05483858 0.04929281]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.554436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514577</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.554436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514577</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.554474</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514899</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.094131</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.094294</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.563466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.596533</td>\n",
       "      <td>0.094131</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.094996</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.471908</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.604059</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>-0.023307</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.021306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.744000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.885407</td>\n",
       "      <td>-0.023307</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.030241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.717268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.903641</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.109187</td>\n",
       "      <td>0.236</td>\n",
       "      <td>3.729957</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.086649</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.152970</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.152005</td>\n",
       "      <td>0.022</td>\n",
       "      <td>3.918000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.185692</td>\n",
       "      <td>-0.152970</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.263973</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.262392</td>\n",
       "      <td>0.022</td>\n",
       "      <td>4.156000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.429586</td>\n",
       "      <td>-0.263973</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.780171</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.762610</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.443601</td>\n",
       "      <td>-0.780171</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-4.351696</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.219561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.172324</td>\n",
       "      <td>-4.351696</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2 Setting     Xsize  expl_variance  \\\n",
       "0        LinearRegression-NonNorm  0.126147   PCA10  (148, 5)       0.126147   \n",
       "0                  Ridge-Alpha001  0.126147   PCA10  (148, 5)       0.126147   \n",
       "0                           Ridge  0.126022   PCA10  (148, 5)       0.126022   \n",
       "0             BayesianRidge-Score  0.094131   PCA10  (148, 5)       0.094294   \n",
       "0         KNRegressor-distanceN50  0.091163   PCA10  (148, 5)       0.094996   \n",
       "0           KNRegressor-uniformN5 -0.023307   PCA10  (148, 5)      -0.021306   \n",
       "0          KNRegressor-distanceN5 -0.030960   PCA10  (148, 5)      -0.030241   \n",
       "0                GradientBoosting -0.109349   PCA10  (148, 5)      -0.109187   \n",
       "0                BaggingRegressor -0.152970   PCA10  (148, 5)      -0.152005   \n",
       "0           RandomForestRegressor -0.263973   PCA10  (148, 5)      -0.262392   \n",
       "0          DecisionTreeRegressor- -0.780171   PCA10  (148, 5)      -0.762610   \n",
       "0  LinearRegression-Norm-NoInterc -4.351696   PCA10  (148, 5)       0.126147   \n",
       "\n",
       "   fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  testSize  \n",
       "0     0.001      3.554436      0.000       4.514577  0.126147       0.7  \n",
       "0     0.000      3.554436      0.000       4.514577  0.126147       0.7  \n",
       "0     0.001      3.554474      0.000       4.514899  0.126022       0.7  \n",
       "0     0.002      3.563466      0.000       4.596533  0.094131       0.7  \n",
       "0     0.000      3.471908      0.001       4.604059  0.091163       0.7  \n",
       "0     0.000      3.744000      0.000       4.885407 -0.023307       0.7  \n",
       "0     0.000      3.717268      0.000       4.903641 -0.030960       0.7  \n",
       "0     0.236      3.729957      0.001       5.086649 -0.109349       0.7  \n",
       "0     0.022      3.918000      0.001       5.185692 -0.152970       0.7  \n",
       "0     0.022      4.156000      0.001       5.429586 -0.263973       0.7  \n",
       "0     0.001      4.880000      0.000       6.443601 -0.780171       0.7  \n",
       "0     0.000     10.219561      0.000      11.172324 -4.351696       0.7  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# scale\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_pca_scaled = scaler.transform(X_train)\n",
    "X_test_pca_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)  \n",
    "\n",
    "# project to PCs\n",
    "X_train_pca = pca.fit_transform(X_train_pca_scaled)  \n",
    "X_test_pca = pca.transform(X_test_pca_scaled) \n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "resultdfs = run_tests(0.7, \"PCA10\", X_train_pca, X_test_pca, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... FIT.....\n",
      "[WARNING] [2019-01-06 21:59:42,270:AutoMLSMBO(1)::35c7353c3ab8368acb9f8df4b1e41259] Could not find meta-data directory /home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/autosklearn/metalearning/files/r2_regression_dense\n",
      "[WARNING] [2019-01-06 21:59:42,282:EnsembleBuilder(1):35c7353c3ab8368acb9f8df4b1e41259] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-01-06 21:59:42,309:EnsembleBuilder(1):35c7353c3ab8368acb9f8df4b1e41259] No models better than random - using Dummy Score!\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:32] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:32] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 2 trees, weight = 0.5\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 3 trees, weight = 0.333333\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:09:49] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:09:49] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      ".... FIT... END ...\n",
      "[(0.400000, SimpleRegressionPipeline({'preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:decision_tree:max_features': 1.0, 'regressor:__choice__': 'decision_tree', 'preprocessor:feature_agglomeration:linkage': 'complete', 'regressor:decision_tree:max_depth_factor': 1.6755483147950971, 'regressor:decision_tree:max_leaf_nodes': 'None', 'imputation:strategy': 'median', 'regressor:decision_tree:min_samples_leaf': 6, 'rescaling:quantile_transformer:n_quantiles': 1981, 'preprocessor:feature_agglomeration:affinity': 'euclidean', 'categorical_encoding:__choice__': 'no_encoding', 'rescaling:__choice__': 'quantile_transformer', 'regressor:decision_tree:min_weight_fraction_leaf': 0.0, 'regressor:decision_tree:min_impurity_decrease': 0.0, 'preprocessor:__choice__': 'feature_agglomeration', 'regressor:decision_tree:min_samples_split': 12, 'preprocessor:feature_agglomeration:n_clusters': 52, 'rescaling:quantile_transformer:output_distribution': 'normal', 'regressor:decision_tree:criterion': 'friedman_mse'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.240000, SimpleRegressionPipeline({'preprocessor:extra_trees_preproc_for_regression:max_features': 0.2697561591073123, 'preprocessor:extra_trees_preproc_for_regression:min_samples_split': 5, 'regressor:__choice__': 'ridge_regression', 'preprocessor:extra_trees_preproc_for_regression:max_depth': 'None', 'preprocessor:extra_trees_preproc_for_regression:n_estimators': 100, 'preprocessor:extra_trees_preproc_for_regression:min_samples_leaf': 13, 'regressor:ridge_regression:alpha': 1.356943709560768e-05, 'preprocessor:extra_trees_preproc_for_regression:min_weight_fraction_leaf': 0.0, 'regressor:ridge_regression:fit_intercept': 'True', 'preprocessor:extra_trees_preproc_for_regression:criterion': 'mse', 'preprocessor:extra_trees_preproc_for_regression:bootstrap': 'False', 'regressor:ridge_regression:tol': 0.00043986933231399616, 'categorical_encoding:__choice__': 'no_encoding', 'rescaling:__choice__': 'quantile_transformer', 'rescaling:quantile_transformer:n_quantiles': 1870, 'preprocessor:extra_trees_preproc_for_regression:max_leaf_nodes': 'None', 'rescaling:quantile_transformer:output_distribution': 'uniform', 'preprocessor:__choice__': 'extra_trees_preproc_for_regression', 'imputation:strategy': 'most_frequent'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.180000, SimpleRegressionPipeline({'regressor:xgradient_boosting:subsample': 0.49582225210790437, 'regressor:xgradient_boosting:reg_alpha': 1.182355135011492e-06, 'preprocessor:fast_ica:fun': 'cube', 'regressor:xgradient_boosting:colsample_bytree': 0.18861669871428016, 'regressor:xgradient_boosting:min_child_weight': 5, 'regressor:xgradient_boosting:base_score': 0.5, 'regressor:xgradient_boosting:gamma': 0, 'categorical_encoding:__choice__': 'no_encoding', 'preprocessor:fast_ica:n_components': 1623, 'preprocessor:__choice__': 'fast_ica', 'imputation:strategy': 'most_frequent', 'regressor:xgradient_boosting:max_depth': 1, 'regressor:__choice__': 'xgradient_boosting', 'regressor:xgradient_boosting:learning_rate': 0.18678214427775974, 'preprocessor:fast_ica:whiten': 'True', 'regressor:xgradient_boosting:max_delta_step': 0, 'regressor:xgradient_boosting:n_estimators': 512, 'rescaling:__choice__': 'none', 'regressor:xgradient_boosting:reg_lambda': 0.00020034750693665557, 'regressor:xgradient_boosting:colsample_bylevel': 0.22118867736622339, 'regressor:xgradient_boosting:booster': 'gbtree', 'regressor:xgradient_boosting:scale_pos_weight': 1, 'preprocessor:fast_ica:algorithm': 'parallel'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.080000, SimpleRegressionPipeline({'regressor:sgd:l1_ratio': 1.7881226172399005e-07, 'regressor:__choice__': 'sgd', 'regressor:sgd:penalty': 'elasticnet', 'regressor:sgd:learning_rate': 'constant', 'preprocessor:polynomial:interaction_only': 'True', 'regressor:sgd:tol': 1.885852799827104e-05, 'regressor:sgd:epsilon': 0.0011952344009921246, 'regressor:sgd:loss': 'epsilon_insensitive', 'rescaling:quantile_transformer:n_quantiles': 1000, 'preprocessor:polynomial:include_bias': 'False', 'regressor:sgd:alpha': 0.09787115626511754, 'regressor:sgd:average': 'False', 'categorical_encoding:__choice__': 'no_encoding', 'regressor:sgd:eta0': 0.0027389919861891036, 'rescaling:__choice__': 'quantile_transformer', 'regressor:sgd:fit_intercept': 'True', 'preprocessor:__choice__': 'polynomial', 'rescaling:quantile_transformer:output_distribution': 'uniform', 'preprocessor:polynomial:degree': 2, 'imputation:strategy': 'median'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.080000, SimpleRegressionPipeline({'categorical_encoding:one_hot_encoding:minimum_fraction': 0.026647218020812026, 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'preprocessor:feature_agglomeration:linkage': 'ward', 'regressor:gradient_boosting:min_impurity_decrease': 0.0, 'regressor:gradient_boosting:max_features': 0.5527448538321074, 'regressor:gradient_boosting:n_estimators': 477, 'regressor:gradient_boosting:max_leaf_nodes': 'None', 'regressor:gradient_boosting:min_weight_fraction_leaf': 0.0, 'preprocessor:__choice__': 'feature_agglomeration', 'regressor:gradient_boosting:learning_rate': 0.05651703628466033, 'imputation:strategy': 'most_frequent', 'preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:gradient_boosting:loss': 'ls', 'regressor:__choice__': 'gradient_boosting', 'regressor:gradient_boosting:subsample': 0.884086883773331, 'regressor:gradient_boosting:min_samples_leaf': 14, 'regressor:gradient_boosting:max_depth': 6, 'preprocessor:feature_agglomeration:affinity': 'euclidean', 'rescaling:__choice__': 'none', 'regressor:gradient_boosting:min_samples_split': 2, 'preprocessor:feature_agglomeration:n_clusters': 355, 'categorical_encoding:__choice__': 'one_hot_encoding'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.020000, SimpleRegressionPipeline({'categorical_encoding:one_hot_encoding:minimum_fraction': 0.26332650675390973, 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'regressor:xgradient_boosting:subsample': 0.29934548938990285, 'regressor:xgradient_boosting:reg_alpha': 0.0019351903914494113, 'regressor:xgradient_boosting:colsample_bytree': 0.962707551371452, 'regressor:xgradient_boosting:min_child_weight': 6, 'regressor:xgradient_boosting:base_score': 0.5, 'preprocessor:pca:whiten': 'False', 'regressor:xgradient_boosting:gamma': 0, 'categorical_encoding:__choice__': 'one_hot_encoding', 'preprocessor:__choice__': 'pca', 'imputation:strategy': 'most_frequent', 'regressor:xgradient_boosting:max_depth': 10, 'regressor:__choice__': 'xgradient_boosting', 'regressor:xgradient_boosting:learning_rate': 0.1957203816974481, 'regressor:xgradient_boosting:max_delta_step': 0, 'regressor:xgradient_boosting:n_estimators': 512, 'preprocessor:pca:keep_variance': 0.7581707883325663, 'rescaling:__choice__': 'none', 'regressor:xgradient_boosting:reg_lambda': 0.001206800913309577, 'regressor:xgradient_boosting:colsample_bylevel': 0.11455154225516638, 'regressor:xgradient_boosting:scale_pos_weight': 1, 'regressor:xgradient_boosting:booster': 'gbtree'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.17531314939689646\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.regression\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import autosklearn.regression\n",
    "\n",
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=4000,\n",
    "    per_run_time_limit=40,\n",
    "    #tmp_folder='/tmp/autosklearn_regression_example_tmp',\n",
    "    #output_folder='/tmp/autosklearn_regression_example_out'\n",
    ")\n",
    "print(\".... FIT.....\")\n",
    "automl.fit(X_train, y_train)\n",
    "print(\".... FIT... END ...\")\n",
    "\n",
    "print(automl.show_models())\n",
    "predictions = automl.predict(X_test)\n",
    "print(\"R2 score:\", sklearn.metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4: Stepwise algorithm - attribute selection (see R notebook file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stepwise algorithm in R, we came to the most important features for this data set.\n",
    "\n",
    "failures + goout + Mjobhealth + Fjobother + freetime + \n",
    "    log(studytime) + addressR + sexF + famsupno + schoolsupno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = [\"failures\",\"goout\",\"Mjob_health\",\"Fjob_other\",\"freetime\", \"studytime\", \"address_R\", \"sex_F\",\n",
    "         \"famsup_no\",\"schoolsup_no\"]\n",
    "\n",
    "#important_features = [\"failures\" , \"Mjob_health\" , \"traveltime\" , \"address_R\" , \"freetime\" , \n",
    "#    \"goout\" , \"famsup_no\" , \"reason_reputation\" , \"Fjob_services\" , \"studytime\" ,\n",
    "#    \"sex_F\" , \"Dalc\"]\n",
    "\n",
    "X_train2 = X_train[important_features]\n",
    "X_test2 = X_test[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failures</th>\n",
       "      <th>goout</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>freetime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>address_R</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>famsup_no</th>\n",
       "      <th>schoolsup_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   failures  goout  Mjob_health  Fjob_other  freetime  studytime  address_R  \\\n",
       "0         0      2            0           0         3          3          0   \n",
       "1         0      1            0           1         4          1          1   \n",
       "2         0      3            0           0         3          3          0   \n",
       "3         0      3            1           1         1          1          0   \n",
       "4         0      3            0           1         3          2          0   \n",
       "\n",
       "   sex_F  famsup_no  schoolsup_no  \n",
       "0      1          1             1  \n",
       "1      0          1             1  \n",
       "2      1          0             1  \n",
       "3      0          1             1  \n",
       "4      1          0             1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.386107</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.889000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.784887</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.383919</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911848</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790676</td>\n",
       "      <td>0.383919</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790684</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.378729</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.879489</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.808692</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.268415</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.269930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.244000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.130763</td>\n",
       "      <td>0.268415</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>0.239556</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.244175</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.320701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.211448</td>\n",
       "      <td>0.239556</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.123633</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.126613</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.493954</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.521065</td>\n",
       "      <td>0.123633</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.058211</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.719038</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.691056</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.031306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.473752</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.757171</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.056434</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.828033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.963855</td>\n",
       "      <td>-0.056434</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.524850</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.505604</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.390000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.963640</td>\n",
       "      <td>-0.524850</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.637960</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.635530</td>\n",
       "      <td>0.286</td>\n",
       "      <td>4.704676</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.180867</td>\n",
       "      <td>-0.637960</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2   Setting      Xsize  \\\n",
       "0                           Ridge  0.385799  raw_data  (148, 10)   \n",
       "0                  Ridge-Alpha001  0.383919  raw_data  (148, 10)   \n",
       "0        LinearRegression-NonNorm  0.383917  raw_data  (148, 10)   \n",
       "0             BayesianRidge-Score  0.378049  raw_data  (148, 10)   \n",
       "0           KNRegressor-uniformN5  0.268415  raw_data  (148, 10)   \n",
       "0  LinearRegression-Norm-NoInterc  0.239556  raw_data  (148, 10)   \n",
       "0          KNRegressor-distanceN5  0.123633  raw_data  (148, 10)   \n",
       "0           RandomForestRegressor  0.056492  raw_data  (148, 10)   \n",
       "0         KNRegressor-distanceN50  0.029709  raw_data  (148, 10)   \n",
       "0                BaggingRegressor -0.056434  raw_data  (148, 10)   \n",
       "0          DecisionTreeRegressor- -0.524850  raw_data  (148, 10)   \n",
       "0                GradientBoosting -0.637960  raw_data  (148, 10)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.386107     0.001      2.889000      0.000       3.784887  0.385799   \n",
       "0       0.384167     0.001      2.911848      0.000       3.790676  0.383919   \n",
       "0       0.384165     0.001      2.911873      0.000       3.790684  0.383917   \n",
       "0       0.378729     0.003      2.879489      0.000       3.808692  0.378049   \n",
       "0       0.269930     0.000      3.244000      0.001       4.130763  0.268415   \n",
       "0       0.244175     0.001      3.320701      0.000       4.211448  0.239556   \n",
       "0       0.126613     0.000      3.493954      0.001       4.521065  0.123633   \n",
       "0       0.058211     0.026      3.719038      0.001       4.691056  0.056492   \n",
       "0       0.031306     0.000      3.473752      0.001       4.757171  0.029709   \n",
       "0      -0.047546     0.027      3.828033      0.002       4.963855 -0.056434   \n",
       "0      -0.505604     0.001      4.390000      0.000       5.963640 -0.524850   \n",
       "0      -0.635530     0.286      4.704676      0.002       6.180867 -0.637960   \n",
       "\n",
       "   testSize  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdfs = run_tests(0.33, \"raw_data\", X_train2, X_test2, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5: Stepwise algorithm with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790684</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.383916</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384164</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790685</td>\n",
       "      <td>0.383916</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.383408</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.383669</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.912039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.792247</td>\n",
       "      <td>0.383408</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.935405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.856468</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.022754</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.521739</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.777887</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.019</td>\n",
       "      <td>3.764633</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.831004</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>-0.098493</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.864000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.061699</td>\n",
       "      <td>-0.098493</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.097369</td>\n",
       "      <td>0.021</td>\n",
       "      <td>4.043167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.069375</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>-0.186040</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.179801</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.002726</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.259536</td>\n",
       "      <td>-0.186040</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.636195</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.633688</td>\n",
       "      <td>0.272</td>\n",
       "      <td>4.700318</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.177536</td>\n",
       "      <td>-0.636195</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.934521</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.933424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.717142</td>\n",
       "      <td>-0.934521</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-4.157632</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.295204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.967887</td>\n",
       "      <td>-4.157632</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2               Setting      Xsize  \\\n",
       "0        LinearRegression-NonNorm  0.383917  bestPredictorsScaled  (148, 10)   \n",
       "0                  Ridge-Alpha001  0.383916  bestPredictorsScaled  (148, 10)   \n",
       "0                           Ridge  0.383408  bestPredictorsScaled  (148, 10)   \n",
       "0             BayesianRidge-Score  0.362348  bestPredictorsScaled  (148, 10)   \n",
       "0         KNRegressor-distanceN50  0.021240  bestPredictorsScaled  (148, 10)   \n",
       "0           RandomForestRegressor -0.000643  bestPredictorsScaled  (148, 10)   \n",
       "0           KNRegressor-uniformN5 -0.098493  bestPredictorsScaled  (148, 10)   \n",
       "0                BaggingRegressor -0.101826  bestPredictorsScaled  (148, 10)   \n",
       "0          KNRegressor-distanceN5 -0.186040  bestPredictorsScaled  (148, 10)   \n",
       "0                GradientBoosting -0.636195  bestPredictorsScaled  (148, 10)   \n",
       "0          DecisionTreeRegressor- -0.934521  bestPredictorsScaled  (148, 10)   \n",
       "0  LinearRegression-Norm-NoInterc -4.157632  bestPredictorsScaled  (148, 10)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.384165     0.001      2.911873      0.000       3.790684  0.383917   \n",
       "0       0.384164     0.001      2.911873      0.000       3.790685  0.383916   \n",
       "0       0.383669     0.001      2.912039      0.000       3.792247  0.383408   \n",
       "0       0.362927     0.002      2.935405      0.000       3.856468  0.362348   \n",
       "0       0.022754     0.000      3.521739      0.001       4.777887  0.021240   \n",
       "0       0.002488     0.019      3.764633      0.001       4.831004 -0.000643   \n",
       "0      -0.083058     0.000      3.864000      0.000       5.061699 -0.098493   \n",
       "0      -0.097369     0.021      4.043167      0.001       5.069375 -0.101826   \n",
       "0      -0.179801     0.000      4.002726      0.001       5.259536 -0.186040   \n",
       "0      -0.633688     0.272      4.700318      0.002       6.177536 -0.636195   \n",
       "0      -0.933424     0.000      5.020000      0.000       6.717142 -0.934521   \n",
       "0       0.384165     0.000     10.295204      0.000      10.967887 -4.157632   \n",
       "\n",
       "   testSize  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"Grade\" in X_train2:\n",
    "    X_train2.drop([\"Grade\"], axis=1, inplace=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train2)\n",
    "X_train2_scaled = scaler.transform(X_train2)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=X_train2.columns, index=X_train2.index)\n",
    "X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=X_test2.columns, index=X_test2.index)\n",
    "\n",
    "\n",
    "resultdfs = run_tests(0.33, \"bestPredictorsScaled\", X_train2_scaled, X_test2_scaled, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression CV - with best setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - best params:  {'fit_intercept': True, 'normalize': True, 'max_iter': 1500, 'alpha': 0.3, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "param_grid_bayesian_ridge = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter' : [1500,2000],\n",
    "    'alpha' : [1.0,0.99,0.95,0.9,0.8,0.7,0.5,0.3,0.1,0.01],\n",
    "    'fit_intercept' : [True, False],\n",
    "    'normalize' : [True, False],\n",
    "    'tol' : [0.001,0.0001,0.002, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "mod = Ridge()\n",
    "CV_dtc = GridSearchCV(estimator=mod, param_grid=param_grid_bayesian_ridge, cv = 10, n_jobs=-1)\n",
    "CV_dtc.fit(X_train2, y_train)\n",
    "rr_best_params = CV_dtc.best_params_\n",
    "print(\"Ridge Regression - best params: \",rr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Coefficients: \n",
      " [-1.61212128 -0.4903169   2.15730588 -0.99224454  0.41652223  0.84901308\n",
      " -1.65484677 -1.1928781   0.53986413  0.99268073]\n",
      "RR Mean absolute error: 2.97137\n",
      "RR Mean squared error: 3.91269\n",
      "RR Variance score: 0.34\n"
     ]
    }
   ],
   "source": [
    "if \"Grade\" in X_train2_scaled:\n",
    "    X_train2_scaled.drop([\"Grade\"], axis=1, inplace=True)\n",
    "\n",
    "# bayesian regression\n",
    "bregr = Ridge(**rr_best_params)\n",
    "bregr.fit(X_train2, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2)\n",
    "\n",
    "\n",
    "print('RR Coefficients: \\n', bregr.coef_)\n",
    "print(\"RR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"RR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('RR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Coefficients: \n",
      " [-1.61212128 -0.4903169   2.15730588 -0.99224454  0.41652223  0.84901308\n",
      " -1.65484677 -1.1928781   0.53986413  0.99268073]\n",
      "RR Mean absolute error: 2.97137\n",
      "RR Mean squared error: 3.91269\n",
      "RR Variance score: 0.34\n"
     ]
    }
   ],
   "source": [
    "# bayesian regression\n",
    "bregr = Ridge(**rr_best_params)\n",
    "bregr.fit(X_train2, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2)\n",
    "\n",
    "\n",
    "print('RR Coefficients: \\n', bregr.coef_)\n",
    "print(\"RR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"RR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('RR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Coefficients: \n",
      " [-1.61233661 -0.81799009  0.66505167 -0.68552816  0.64651496  0.95385323\n",
      " -0.91095412 -0.82917865  0.45935026  0.44024104]\n",
      "LR Mean absolute error: 2.91187\n",
      "LR Mean squared error: 3.79068\n",
      "LR Variance score: 0.38\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "bregr = LinearRegression(normalize=False)\n",
    "bregr.fit(X_train2_scaled, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2_scaled)\n",
    "\n",
    "\n",
    "print('LR Coefficients: \\n', bregr.coef_)\n",
    "print(\"LR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"LR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('LR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failures</th>\n",
       "      <th>goout</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>freetime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>address_R</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>famsup_no</th>\n",
       "      <th>schoolsup_no</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-1.082210</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>-1.114641</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>1.150271</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-2.024369</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>0.838166</td>\n",
       "      <td>-1.214175</td>\n",
       "      <td>2.025697</td>\n",
       "      <td>-1.161553</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>-1.114641</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>1.150271</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>-0.757937</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>3.929942</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>-2.315612</td>\n",
       "      <td>-1.214175</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>-1.161553</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>-0.031952</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>-0.757937</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   failures     goout  Mjob_health  Fjob_other  freetime  studytime  \\\n",
       "0 -0.448353 -1.082210    -0.254457   -1.114641 -0.213093   1.150271   \n",
       "1 -0.448353 -2.024369    -0.254457    0.897150  0.838166  -1.214175   \n",
       "2 -0.448353 -0.140051    -0.254457   -1.114641 -0.213093   1.150271   \n",
       "3 -0.448353 -0.140051     3.929942    0.897150 -2.315612  -1.214175   \n",
       "4 -0.448353 -0.140051    -0.254457    0.897150 -0.213093  -0.031952   \n",
       "\n",
       "   address_R     sex_F  famsup_no  schoolsup_no  Grade  \n",
       "0  -0.493657  0.860916   1.319371      0.360237     16  \n",
       "1   2.025697 -1.161553   1.319371      0.360237     10  \n",
       "2  -0.493657  0.860916  -0.757937      0.360237     19  \n",
       "3  -0.493657 -1.161553   1.319371      0.360237     10  \n",
       "4  -0.493657  0.860916  -0.757937      0.360237      0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2_scaled[\"Grade\"]=y_train\n",
    "X_train2_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draws: 15\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 50\n",
      "LR Mean Squared Error : 3.93000\n",
      "LR Variance score: 0.34\n",
      "draws: 70\n",
      "LR Mean Squared Error : 3.87000\n",
      "LR Variance score: 0.36\n",
      "draws: 100\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 150\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 200\n",
      "LR Mean Squared Error : 3.78000\n",
      "LR Variance score: 0.39\n",
      "draws: 500\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 781\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 1000\n",
      "LR Mean Squared Error : 3.79000\n",
      "LR Variance score: 0.39\n",
      "draws: 1500\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 2001\n",
      "LR Mean Squared Error : 3.78000\n",
      "LR Variance score: 0.39\n",
      "draws: 3001\n",
      "LR Mean Squared Error : 3.77000\n",
      "LR Variance score: 0.39\n",
      "3.815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_train2_scaled[\"Grade\"] = y_train\n",
    "\n",
    "draws = [15,50,70,100,150,200,500,781,1000,1500,2001,3001]\n",
    "result = {}\n",
    "sample_size=0.7\n",
    "\n",
    "for draw_nr in draws:\n",
    "    print(\"draws:\", draw_nr)\n",
    "    ensamble_dframe = pd.DataFrame()\n",
    "    for i in range(0,draw_nr):\n",
    "        ssample = X_train2_scaled.sample(frac=sample_size, replace=True)\n",
    "        sampleX = ssample.drop([\"Grade\"], axis=1)\n",
    "        sampleY = ssample[\"Grade\"]\n",
    "        #print(sampleX.shape)\n",
    "        bregr = LinearRegression(normalize=False)\n",
    "        bregr.fit(sampleX, sampleY)\n",
    "\n",
    "        y_pred2 = bregr.predict(X_test2_scaled)\n",
    "        ensamble_dframe[i] = y_pred2\n",
    "        #print(\"BR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "        #print('BR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))\n",
    "    \n",
    "    \n",
    "    ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "    # round up gives even better result\n",
    "    ensamble_predict = np.ceil(ensamble_predict)\n",
    "    mae = metrics.mean_absolute_error(y_test, ensamble_predict)\n",
    "    r2 = metrics.r2_score(y_test, ensamble_predict)\n",
    "    mse = np.round(np.sqrt(metrics.mean_squared_error(y_test, ensamble_predict)),2)\n",
    "    #print(\"LR Mean Absolute Error : %.5f\"% mae)\n",
    "    print(\"LR Mean Squared Error : %.5f\"% mse)\n",
    "    print('LR Variance score: %.2f' % r2)\n",
    "    result[draw_nr] = mse\n",
    "    \n",
    "## Plot\n",
    "lists = sorted(result.items(), reverse=True) # sorted by key, return a list of tuples\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "plt.plot(x, y)\n",
    "plt.xticks(draws)\n",
    "numbers = [result[key] for key in result]\n",
    "mean_ = statistics.mean(numbers)\n",
    "print(mean_)\n",
    "#plt.yticks(result.values())\n",
    "plt.title(\"Linear Reg. Ensamble - Sample frac:\"+str(sample_size)+\", mean \"+str(np.round(mean_,2)))\n",
    "plt.xlabel(\"Sample draws\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Ensamble results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Root Squared Error: 3.77000\n",
      "LR Mean absolute error: 2.84000\n",
      "LR Variance score: 0.39\n"
     ]
    }
   ],
   "source": [
    "ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "# round up gives even better result\n",
    "ensamble_predict = np.ceil(ensamble_predict)\n",
    "print(\"LR Root Squared Error: %.5f\"% np.round(np.sqrt(metrics.mean_squared_error(y_test, ensamble_predict)),2))\n",
    "print(\"LR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, ensamble_predict))\n",
    "print('LR Variance score: %.2f' % metrics.r2_score(y_test, ensamble_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensamble_predict<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = df_dummy_train[important_features]\n",
    "X_Test = df_dummy_test[important_features]\n",
    "\n",
    "# scale train and test data\n",
    "scaler = preprocessing.StandardScaler().fit(X_Train)\n",
    "X_Train_scaled = scaler.transform(X_Train)\n",
    "X_Test_scaled = scaler.transform(X_Test)\n",
    "\n",
    "X_Train_sdf = pd.DataFrame(X_Train_scaled, columns=X_Train.columns, index=X_Train.index)\n",
    "X_Test_sdf = pd.DataFrame(X_Test_scaled, columns=X_Test.columns, index=X_Test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian regression\n",
    "#X_Train_sdf.drop([\"Grade\"], axis=1, inplace=True)\n",
    "bregr = Ridge(fit_intercept=True, normalize=True,max_iter=1500 ,alpha=0.3, tol=0.001)\n",
    "bregr.fit(X_Train_sdf, train_grade_values)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_submit = bregr.predict(X_Test_sdf)\n",
    "# round\n",
    "y_pred_submit = np.ceil(y_pred_submit)\n",
    "y_pred_submit[y_pred_submit < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensamble submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 0\n",
      "predict: 1\n",
      "predict: 2\n",
      "predict: 3\n",
      "predict: 4\n",
      "predict: 5\n",
      "predict: 6\n",
      "predict: 7\n",
      "predict: 8\n",
      "predict: 9\n",
      "predict: 10\n",
      "predict: 11\n",
      "predict: 12\n",
      "predict: 13\n",
      "predict: 14\n",
      "predict: 15\n",
      "predict: 16\n",
      "predict: 17\n",
      "predict: 18\n",
      "predict: 19\n",
      "predict: 20\n",
      "predict: 21\n",
      "predict: 22\n",
      "predict: 23\n",
      "predict: 24\n",
      "predict: 25\n",
      "predict: 26\n",
      "predict: 27\n",
      "predict: 28\n",
      "predict: 29\n",
      "predict: 30\n",
      "predict: 31\n",
      "predict: 32\n",
      "predict: 33\n",
      "predict: 34\n",
      "predict: 35\n",
      "predict: 36\n",
      "predict: 37\n",
      "predict: 38\n",
      "predict: 39\n",
      "predict: 40\n",
      "predict: 41\n",
      "predict: 42\n",
      "predict: 43\n",
      "predict: 44\n",
      "predict: 45\n",
      "predict: 46\n",
      "predict: 47\n",
      "predict: 48\n",
      "predict: 49\n",
      "predict: 50\n",
      "predict: 51\n",
      "predict: 52\n",
      "predict: 53\n",
      "predict: 54\n",
      "predict: 55\n",
      "predict: 56\n",
      "predict: 57\n",
      "predict: 58\n",
      "predict: 59\n",
      "predict: 60\n",
      "predict: 61\n",
      "predict: 62\n",
      "predict: 63\n",
      "predict: 64\n",
      "predict: 65\n",
      "predict: 66\n",
      "predict: 67\n",
      "predict: 68\n",
      "predict: 69\n",
      "predict: 70\n",
      "predict: 71\n",
      "predict: 72\n",
      "predict: 73\n",
      "predict: 74\n",
      "predict: 75\n",
      "predict: 76\n",
      "predict: 77\n",
      "predict: 78\n",
      "predict: 79\n",
      "predict: 80\n",
      "predict: 81\n",
      "predict: 82\n",
      "predict: 83\n",
      "predict: 84\n",
      "predict: 85\n",
      "predict: 86\n",
      "predict: 87\n",
      "predict: 88\n",
      "predict: 89\n",
      "predict: 90\n",
      "predict: 91\n",
      "predict: 92\n",
      "predict: 93\n",
      "predict: 94\n",
      "predict: 95\n",
      "predict: 96\n",
      "predict: 97\n",
      "predict: 98\n",
      "predict: 99\n",
      "predict: 100\n",
      "predict: 101\n",
      "predict: 102\n",
      "predict: 103\n",
      "predict: 104\n",
      "predict: 105\n",
      "predict: 106\n",
      "predict: 107\n",
      "predict: 108\n",
      "predict: 109\n",
      "predict: 110\n",
      "predict: 111\n",
      "predict: 112\n",
      "predict: 113\n",
      "predict: 114\n",
      "predict: 115\n",
      "predict: 116\n",
      "predict: 117\n",
      "predict: 118\n",
      "predict: 119\n",
      "predict: 120\n",
      "predict: 121\n",
      "predict: 122\n",
      "predict: 123\n",
      "predict: 124\n",
      "predict: 125\n",
      "predict: 126\n",
      "predict: 127\n",
      "predict: 128\n",
      "predict: 129\n",
      "predict: 130\n",
      "predict: 131\n",
      "predict: 132\n",
      "predict: 133\n",
      "predict: 134\n",
      "predict: 135\n",
      "predict: 136\n",
      "predict: 137\n",
      "predict: 138\n",
      "predict: 139\n",
      "predict: 140\n",
      "predict: 141\n",
      "predict: 142\n",
      "predict: 143\n",
      "predict: 144\n",
      "predict: 145\n",
      "predict: 146\n",
      "predict: 147\n",
      "predict: 148\n",
      "predict: 149\n",
      "predict: 150\n",
      "predict: 151\n",
      "predict: 152\n",
      "predict: 153\n",
      "predict: 154\n",
      "predict: 155\n",
      "predict: 156\n",
      "predict: 157\n",
      "predict: 158\n",
      "predict: 159\n",
      "predict: 160\n",
      "predict: 161\n",
      "predict: 162\n",
      "predict: 163\n",
      "predict: 164\n",
      "predict: 165\n",
      "predict: 166\n",
      "predict: 167\n",
      "predict: 168\n",
      "predict: 169\n",
      "predict: 170\n",
      "predict: 171\n",
      "predict: 172\n",
      "predict: 173\n",
      "predict: 174\n",
      "predict: 175\n",
      "predict: 176\n",
      "predict: 177\n",
      "predict: 178\n",
      "predict: 179\n",
      "predict: 180\n",
      "predict: 181\n",
      "predict: 182\n",
      "predict: 183\n",
      "predict: 184\n",
      "predict: 185\n",
      "predict: 186\n",
      "predict: 187\n",
      "predict: 188\n",
      "predict: 189\n",
      "predict: 190\n",
      "predict: 191\n",
      "predict: 192\n",
      "predict: 193\n",
      "predict: 194\n",
      "predict: 195\n",
      "predict: 196\n",
      "predict: 197\n",
      "predict: 198\n",
      "predict: 199\n",
      "predict: 200\n",
      "predict: 201\n",
      "predict: 202\n",
      "predict: 203\n",
      "predict: 204\n",
      "predict: 205\n",
      "predict: 206\n",
      "predict: 207\n",
      "predict: 208\n",
      "predict: 209\n",
      "predict: 210\n",
      "predict: 211\n",
      "predict: 212\n",
      "predict: 213\n",
      "predict: 214\n",
      "predict: 215\n",
      "predict: 216\n",
      "predict: 217\n",
      "predict: 218\n",
      "predict: 219\n",
      "predict: 220\n",
      "predict: 221\n",
      "predict: 222\n",
      "predict: 223\n",
      "predict: 224\n",
      "predict: 225\n",
      "predict: 226\n",
      "predict: 227\n",
      "predict: 228\n",
      "predict: 229\n",
      "predict: 230\n",
      "predict: 231\n",
      "predict: 232\n",
      "predict: 233\n",
      "predict: 234\n",
      "predict: 235\n",
      "predict: 236\n",
      "predict: 237\n",
      "predict: 238\n",
      "predict: 239\n",
      "predict: 240\n",
      "predict: 241\n",
      "predict: 242\n",
      "predict: 243\n",
      "predict: 244\n",
      "predict: 245\n",
      "predict: 246\n",
      "predict: 247\n",
      "predict: 248\n",
      "predict: 249\n",
      "predict: 250\n",
      "predict: 251\n",
      "predict: 252\n",
      "predict: 253\n",
      "predict: 254\n",
      "predict: 255\n",
      "predict: 256\n",
      "predict: 257\n",
      "predict: 258\n",
      "predict: 259\n",
      "predict: 260\n",
      "predict: 261\n",
      "predict: 262\n",
      "predict: 263\n",
      "predict: 264\n",
      "predict: 265\n",
      "predict: 266\n",
      "predict: 267\n",
      "predict: 268\n",
      "predict: 269\n",
      "predict: 270\n",
      "predict: 271\n",
      "predict: 272\n",
      "predict: 273\n",
      "predict: 274\n",
      "predict: 275\n",
      "predict: 276\n",
      "predict: 277\n",
      "predict: 278\n",
      "predict: 279\n",
      "predict: 280\n",
      "predict: 281\n",
      "predict: 282\n",
      "predict: 283\n",
      "predict: 284\n",
      "predict: 285\n",
      "predict: 286\n",
      "predict: 287\n",
      "predict: 288\n",
      "predict: 289\n",
      "predict: 290\n",
      "predict: 291\n",
      "predict: 292\n",
      "predict: 293\n",
      "predict: 294\n",
      "predict: 295\n",
      "predict: 296\n",
      "predict: 297\n",
      "predict: 298\n",
      "predict: 299\n",
      "predict: 300\n",
      "predict: 301\n",
      "predict: 302\n",
      "predict: 303\n",
      "predict: 304\n",
      "predict: 305\n",
      "predict: 306\n",
      "predict: 307\n",
      "predict: 308\n",
      "predict: 309\n",
      "predict: 310\n",
      "predict: 311\n",
      "predict: 312\n",
      "predict: 313\n",
      "predict: 314\n",
      "predict: 315\n",
      "predict: 316\n",
      "predict: 317\n",
      "predict: 318\n",
      "predict: 319\n",
      "predict: 320\n",
      "predict: 321\n",
      "predict: 322\n",
      "predict: 323\n",
      "predict: 324\n",
      "predict: 325\n",
      "predict: 326\n",
      "predict: 327\n",
      "predict: 328\n",
      "predict: 329\n",
      "predict: 330\n",
      "predict: 331\n",
      "predict: 332\n",
      "predict: 333\n",
      "predict: 334\n",
      "predict: 335\n",
      "predict: 336\n",
      "predict: 337\n",
      "predict: 338\n",
      "predict: 339\n",
      "predict: 340\n",
      "predict: 341\n",
      "predict: 342\n",
      "predict: 343\n",
      "predict: 344\n",
      "predict: 345\n",
      "predict: 346\n",
      "predict: 347\n",
      "predict: 348\n",
      "predict: 349\n",
      "predict: 350\n",
      "predict: 351\n",
      "predict: 352\n",
      "predict: 353\n",
      "predict: 354\n",
      "predict: 355\n",
      "predict: 356\n",
      "predict: 357\n",
      "predict: 358\n",
      "predict: 359\n",
      "predict: 360\n",
      "predict: 361\n",
      "predict: 362\n",
      "predict: 363\n",
      "predict: 364\n",
      "predict: 365\n",
      "predict: 366\n",
      "predict: 367\n",
      "predict: 368\n",
      "predict: 369\n",
      "predict: 370\n",
      "predict: 371\n",
      "predict: 372\n",
      "predict: 373\n",
      "predict: 374\n",
      "predict: 375\n",
      "predict: 376\n",
      "predict: 377\n",
      "predict: 378\n",
      "predict: 379\n",
      "predict: 380\n",
      "predict: 381\n",
      "predict: 382\n",
      "predict: 383\n",
      "predict: 384\n",
      "predict: 385\n",
      "predict: 386\n",
      "predict: 387\n",
      "predict: 388\n",
      "predict: 389\n",
      "predict: 390\n",
      "predict: 391\n",
      "predict: 392\n",
      "predict: 393\n",
      "predict: 394\n",
      "predict: 395\n",
      "predict: 396\n",
      "predict: 397\n",
      "predict: 398\n",
      "predict: 399\n",
      "predict: 400\n",
      "predict: 401\n",
      "predict: 402\n",
      "predict: 403\n",
      "predict: 404\n",
      "predict: 405\n",
      "predict: 406\n",
      "predict: 407\n",
      "predict: 408\n",
      "predict: 409\n",
      "predict: 410\n",
      "predict: 411\n",
      "predict: 412\n",
      "predict: 413\n",
      "predict: 414\n",
      "predict: 415\n",
      "predict: 416\n",
      "predict: 417\n",
      "predict: 418\n",
      "predict: 419\n",
      "predict: 420\n",
      "predict: 421\n",
      "predict: 422\n",
      "predict: 423\n",
      "predict: 424\n",
      "predict: 425\n",
      "predict: 426\n",
      "predict: 427\n",
      "predict: 428\n",
      "predict: 429\n",
      "predict: 430\n",
      "predict: 431\n",
      "predict: 432\n",
      "predict: 433\n",
      "predict: 434\n",
      "predict: 435\n",
      "predict: 436\n",
      "predict: 437\n",
      "predict: 438\n",
      "predict: 439\n",
      "predict: 440\n",
      "predict: 441\n",
      "predict: 442\n",
      "predict: 443\n",
      "predict: 444\n",
      "predict: 445\n",
      "predict: 446\n",
      "predict: 447\n",
      "predict: 448\n",
      "predict: 449\n",
      "predict: 450\n",
      "predict: 451\n",
      "predict: 452\n",
      "predict: 453\n",
      "predict: 454\n",
      "predict: 455\n",
      "predict: 456\n",
      "predict: 457\n",
      "predict: 458\n",
      "predict: 459\n",
      "predict: 460\n",
      "predict: 461\n",
      "predict: 462\n",
      "predict: 463\n",
      "predict: 464\n",
      "predict: 465\n",
      "predict: 466\n",
      "predict: 467\n",
      "predict: 468\n",
      "predict: 469\n",
      "predict: 470\n",
      "predict: 471\n",
      "predict: 472\n",
      "predict: 473\n",
      "predict: 474\n",
      "predict: 475\n",
      "predict: 476\n",
      "predict: 477\n",
      "predict: 478\n",
      "predict: 479\n",
      "predict: 480\n",
      "predict: 481\n",
      "predict: 482\n",
      "predict: 483\n",
      "predict: 484\n",
      "predict: 485\n",
      "predict: 486\n",
      "predict: 487\n",
      "predict: 488\n",
      "predict: 489\n",
      "predict: 490\n",
      "predict: 491\n",
      "predict: 492\n",
      "predict: 493\n",
      "predict: 494\n",
      "predict: 495\n",
      "predict: 496\n",
      "predict: 497\n",
      "predict: 498\n",
      "predict: 499\n",
      "predict: 500\n",
      "predict: 501\n",
      "predict: 502\n",
      "predict: 503\n",
      "predict: 504\n",
      "predict: 505\n",
      "predict: 506\n",
      "predict: 507\n",
      "predict: 508\n",
      "predict: 509\n",
      "predict: 510\n",
      "predict: 511\n",
      "predict: 512\n",
      "predict: 513\n",
      "predict: 514\n",
      "predict: 515\n",
      "predict: 516\n",
      "predict: 517\n",
      "predict: 518\n",
      "predict: 519\n",
      "predict: 520\n",
      "predict: 521\n",
      "predict: 522\n",
      "predict: 523\n",
      "predict: 524\n",
      "predict: 525\n",
      "predict: 526\n",
      "predict: 527\n",
      "predict: 528\n",
      "predict: 529\n",
      "predict: 530\n",
      "predict: 531\n",
      "predict: 532\n",
      "predict: 533\n",
      "predict: 534\n",
      "predict: 535\n",
      "predict: 536\n",
      "predict: 537\n",
      "predict: 538\n",
      "predict: 539\n",
      "predict: 540\n",
      "predict: 541\n",
      "predict: 542\n",
      "predict: 543\n",
      "predict: 544\n",
      "predict: 545\n",
      "predict: 546\n",
      "predict: 547\n",
      "predict: 548\n",
      "predict: 549\n",
      "predict: 550\n",
      "predict: 551\n",
      "predict: 552\n",
      "predict: 553\n",
      "predict: 554\n",
      "predict: 555\n",
      "predict: 556\n",
      "predict: 557\n",
      "predict: 558\n",
      "predict: 559\n",
      "predict: 560\n",
      "predict: 561\n",
      "predict: 562\n",
      "predict: 563\n",
      "predict: 564\n",
      "predict: 565\n",
      "predict: 566\n",
      "predict: 567\n",
      "predict: 568\n",
      "predict: 569\n",
      "predict: 570\n",
      "predict: 571\n",
      "predict: 572\n",
      "predict: 573\n",
      "predict: 574\n",
      "predict: 575\n",
      "predict: 576\n",
      "predict: 577\n",
      "predict: 578\n",
      "predict: 579\n",
      "predict: 580\n",
      "predict: 581\n",
      "predict: 582\n",
      "predict: 583\n",
      "predict: 584\n",
      "predict: 585\n",
      "predict: 586\n",
      "predict: 587\n",
      "predict: 588\n",
      "predict: 589\n",
      "predict: 590\n",
      "predict: 591\n",
      "predict: 592\n",
      "predict: 593\n",
      "predict: 594\n",
      "predict: 595\n",
      "predict: 596\n",
      "predict: 597\n",
      "predict: 598\n",
      "predict: 599\n",
      "predict: 600\n",
      "predict: 601\n",
      "predict: 602\n",
      "predict: 603\n",
      "predict: 604\n",
      "predict: 605\n",
      "predict: 606\n",
      "predict: 607\n",
      "predict: 608\n",
      "predict: 609\n",
      "predict: 610\n",
      "predict: 611\n",
      "predict: 612\n",
      "predict: 613\n",
      "predict: 614\n",
      "predict: 615\n",
      "predict: 616\n",
      "predict: 617\n",
      "predict: 618\n",
      "predict: 619\n",
      "predict: 620\n",
      "predict: 621\n",
      "predict: 622\n",
      "predict: 623\n",
      "predict: 624\n",
      "predict: 625\n",
      "predict: 626\n",
      "predict: 627\n",
      "predict: 628\n",
      "predict: 629\n",
      "predict: 630\n",
      "predict: 631\n",
      "predict: 632\n",
      "predict: 633\n",
      "predict: 634\n",
      "predict: 635\n",
      "predict: 636\n",
      "predict: 637\n",
      "predict: 638\n",
      "predict: 639\n",
      "predict: 640\n",
      "predict: 641\n",
      "predict: 642\n",
      "predict: 643\n",
      "predict: 644\n",
      "predict: 645\n",
      "predict: 646\n",
      "predict: 647\n",
      "predict: 648\n",
      "predict: 649\n",
      "predict: 650\n",
      "predict: 651\n",
      "predict: 652\n",
      "predict: 653\n",
      "predict: 654\n",
      "predict: 655\n",
      "predict: 656\n",
      "predict: 657\n",
      "predict: 658\n",
      "predict: 659\n",
      "predict: 660\n",
      "predict: 661\n",
      "predict: 662\n",
      "predict: 663\n",
      "predict: 664\n",
      "predict: 665\n",
      "predict: 666\n",
      "predict: 667\n",
      "predict: 668\n",
      "predict: 669\n",
      "predict: 670\n",
      "predict: 671\n",
      "predict: 672\n",
      "predict: 673\n",
      "predict: 674\n",
      "predict: 675\n",
      "predict: 676\n",
      "predict: 677\n",
      "predict: 678\n",
      "predict: 679\n",
      "predict: 680\n",
      "predict: 681\n",
      "predict: 682\n",
      "predict: 683\n",
      "predict: 684\n",
      "predict: 685\n",
      "predict: 686\n",
      "predict: 687\n",
      "predict: 688\n",
      "predict: 689\n",
      "predict: 690\n",
      "predict: 691\n",
      "predict: 692\n",
      "predict: 693\n",
      "predict: 694\n",
      "predict: 695\n",
      "predict: 696\n",
      "predict: 697\n",
      "predict: 698\n",
      "predict: 699\n",
      "predict: 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 701\n",
      "predict: 702\n",
      "predict: 703\n",
      "predict: 704\n",
      "predict: 705\n",
      "predict: 706\n",
      "predict: 707\n",
      "predict: 708\n",
      "predict: 709\n",
      "predict: 710\n",
      "predict: 711\n",
      "predict: 712\n",
      "predict: 713\n",
      "predict: 714\n",
      "predict: 715\n",
      "predict: 716\n",
      "predict: 717\n",
      "predict: 718\n",
      "predict: 719\n",
      "predict: 720\n",
      "predict: 721\n",
      "predict: 722\n",
      "predict: 723\n",
      "predict: 724\n",
      "predict: 725\n",
      "predict: 726\n",
      "predict: 727\n",
      "predict: 728\n",
      "predict: 729\n",
      "predict: 730\n",
      "predict: 731\n",
      "predict: 732\n",
      "predict: 733\n",
      "predict: 734\n",
      "predict: 735\n",
      "predict: 736\n",
      "predict: 737\n",
      "predict: 738\n",
      "predict: 739\n",
      "predict: 740\n",
      "predict: 741\n",
      "predict: 742\n",
      "predict: 743\n",
      "predict: 744\n",
      "predict: 745\n",
      "predict: 746\n",
      "predict: 747\n",
      "predict: 748\n",
      "predict: 749\n",
      "predict: 750\n",
      "predict: 751\n",
      "predict: 752\n",
      "predict: 753\n",
      "predict: 754\n",
      "predict: 755\n",
      "predict: 756\n",
      "predict: 757\n",
      "predict: 758\n",
      "predict: 759\n",
      "predict: 760\n",
      "predict: 761\n",
      "predict: 762\n",
      "predict: 763\n",
      "predict: 764\n",
      "predict: 765\n",
      "predict: 766\n",
      "predict: 767\n",
      "predict: 768\n",
      "predict: 769\n",
      "predict: 770\n",
      "predict: 771\n",
      "predict: 772\n",
      "predict: 773\n",
      "predict: 774\n",
      "predict: 775\n",
      "predict: 776\n",
      "predict: 777\n",
      "predict: 778\n",
      "predict: 779\n",
      "predict: 780\n",
      "predict: 781\n",
      "predict: 782\n",
      "predict: 783\n",
      "predict: 784\n",
      "predict: 785\n",
      "predict: 786\n",
      "predict: 787\n",
      "predict: 788\n",
      "predict: 789\n",
      "predict: 790\n",
      "predict: 791\n",
      "predict: 792\n",
      "predict: 793\n",
      "predict: 794\n",
      "predict: 795\n",
      "predict: 796\n",
      "predict: 797\n",
      "predict: 798\n",
      "predict: 799\n",
      "predict: 800\n",
      "predict: 801\n",
      "predict: 802\n",
      "predict: 803\n",
      "predict: 804\n",
      "predict: 805\n",
      "predict: 806\n",
      "predict: 807\n",
      "predict: 808\n",
      "predict: 809\n",
      "predict: 810\n",
      "predict: 811\n",
      "predict: 812\n",
      "predict: 813\n",
      "predict: 814\n",
      "predict: 815\n",
      "predict: 816\n",
      "predict: 817\n",
      "predict: 818\n",
      "predict: 819\n",
      "predict: 820\n",
      "predict: 821\n",
      "predict: 822\n",
      "predict: 823\n",
      "predict: 824\n",
      "predict: 825\n",
      "predict: 826\n",
      "predict: 827\n",
      "predict: 828\n",
      "predict: 829\n",
      "predict: 830\n",
      "predict: 831\n",
      "predict: 832\n",
      "predict: 833\n",
      "predict: 834\n",
      "predict: 835\n",
      "predict: 836\n",
      "predict: 837\n",
      "predict: 838\n",
      "predict: 839\n",
      "predict: 840\n",
      "predict: 841\n",
      "predict: 842\n",
      "predict: 843\n",
      "predict: 844\n",
      "predict: 845\n",
      "predict: 846\n",
      "predict: 847\n",
      "predict: 848\n",
      "predict: 849\n",
      "predict: 850\n",
      "predict: 851\n",
      "predict: 852\n",
      "predict: 853\n",
      "predict: 854\n",
      "predict: 855\n",
      "predict: 856\n",
      "predict: 857\n",
      "predict: 858\n",
      "predict: 859\n",
      "predict: 860\n",
      "predict: 861\n",
      "predict: 862\n",
      "predict: 863\n",
      "predict: 864\n",
      "predict: 865\n",
      "predict: 866\n",
      "predict: 867\n",
      "predict: 868\n",
      "predict: 869\n",
      "predict: 870\n",
      "predict: 871\n",
      "predict: 872\n",
      "predict: 873\n",
      "predict: 874\n",
      "predict: 875\n",
      "predict: 876\n",
      "predict: 877\n",
      "predict: 878\n",
      "predict: 879\n",
      "predict: 880\n",
      "predict: 881\n",
      "predict: 882\n",
      "predict: 883\n",
      "predict: 884\n",
      "predict: 885\n",
      "predict: 886\n",
      "predict: 887\n",
      "predict: 888\n",
      "predict: 889\n",
      "predict: 890\n",
      "predict: 891\n",
      "predict: 892\n",
      "predict: 893\n",
      "predict: 894\n",
      "predict: 895\n",
      "predict: 896\n",
      "predict: 897\n",
      "predict: 898\n",
      "predict: 899\n",
      "predict: 900\n",
      "predict: 901\n",
      "predict: 902\n",
      "predict: 903\n",
      "predict: 904\n",
      "predict: 905\n",
      "predict: 906\n",
      "predict: 907\n",
      "predict: 908\n",
      "predict: 909\n",
      "predict: 910\n",
      "predict: 911\n",
      "predict: 912\n",
      "predict: 913\n",
      "predict: 914\n",
      "predict: 915\n",
      "predict: 916\n",
      "predict: 917\n",
      "predict: 918\n",
      "predict: 919\n",
      "predict: 920\n",
      "predict: 921\n",
      "predict: 922\n",
      "predict: 923\n",
      "predict: 924\n",
      "predict: 925\n",
      "predict: 926\n",
      "predict: 927\n",
      "predict: 928\n",
      "predict: 929\n",
      "predict: 930\n",
      "predict: 931\n",
      "predict: 932\n",
      "predict: 933\n",
      "predict: 934\n",
      "predict: 935\n",
      "predict: 936\n",
      "predict: 937\n",
      "predict: 938\n",
      "predict: 939\n",
      "predict: 940\n",
      "predict: 941\n",
      "predict: 942\n",
      "predict: 943\n",
      "predict: 944\n",
      "predict: 945\n",
      "predict: 946\n",
      "predict: 947\n",
      "predict: 948\n",
      "predict: 949\n",
      "predict: 950\n",
      "predict: 951\n",
      "predict: 952\n",
      "predict: 953\n",
      "predict: 954\n",
      "predict: 955\n",
      "predict: 956\n",
      "predict: 957\n",
      "predict: 958\n",
      "predict: 959\n",
      "predict: 960\n",
      "predict: 961\n",
      "predict: 962\n",
      "predict: 963\n",
      "predict: 964\n",
      "predict: 965\n",
      "predict: 966\n",
      "predict: 967\n",
      "predict: 968\n",
      "predict: 969\n",
      "predict: 970\n",
      "predict: 971\n",
      "predict: 972\n",
      "predict: 973\n",
      "predict: 974\n",
      "predict: 975\n",
      "predict: 976\n",
      "predict: 977\n",
      "predict: 978\n",
      "predict: 979\n",
      "predict: 980\n",
      "predict: 981\n",
      "predict: 982\n",
      "predict: 983\n",
      "predict: 984\n",
      "predict: 985\n",
      "predict: 986\n",
      "predict: 987\n",
      "predict: 988\n",
      "predict: 989\n",
      "predict: 990\n",
      "predict: 991\n",
      "predict: 992\n",
      "predict: 993\n",
      "predict: 994\n",
      "predict: 995\n",
      "predict: 996\n",
      "predict: 997\n",
      "predict: 998\n",
      "predict: 999\n",
      "predict: 1000\n",
      "predict: 1001\n",
      "predict: 1002\n",
      "predict: 1003\n",
      "predict: 1004\n",
      "predict: 1005\n",
      "predict: 1006\n",
      "predict: 1007\n",
      "predict: 1008\n",
      "predict: 1009\n",
      "predict: 1010\n",
      "predict: 1011\n",
      "predict: 1012\n",
      "predict: 1013\n",
      "predict: 1014\n",
      "predict: 1015\n",
      "predict: 1016\n",
      "predict: 1017\n",
      "predict: 1018\n",
      "predict: 1019\n",
      "predict: 1020\n",
      "predict: 1021\n",
      "predict: 1022\n",
      "predict: 1023\n",
      "predict: 1024\n",
      "predict: 1025\n",
      "predict: 1026\n",
      "predict: 1027\n",
      "predict: 1028\n",
      "predict: 1029\n",
      "predict: 1030\n",
      "predict: 1031\n",
      "predict: 1032\n",
      "predict: 1033\n",
      "predict: 1034\n",
      "predict: 1035\n",
      "predict: 1036\n",
      "predict: 1037\n",
      "predict: 1038\n",
      "predict: 1039\n",
      "predict: 1040\n",
      "predict: 1041\n",
      "predict: 1042\n",
      "predict: 1043\n",
      "predict: 1044\n",
      "predict: 1045\n",
      "predict: 1046\n",
      "predict: 1047\n",
      "predict: 1048\n",
      "predict: 1049\n",
      "predict: 1050\n",
      "predict: 1051\n",
      "predict: 1052\n",
      "predict: 1053\n",
      "predict: 1054\n",
      "predict: 1055\n",
      "predict: 1056\n",
      "predict: 1057\n",
      "predict: 1058\n",
      "predict: 1059\n",
      "predict: 1060\n",
      "predict: 1061\n",
      "predict: 1062\n",
      "predict: 1063\n",
      "predict: 1064\n",
      "predict: 1065\n",
      "predict: 1066\n",
      "predict: 1067\n",
      "predict: 1068\n",
      "predict: 1069\n",
      "predict: 1070\n",
      "predict: 1071\n",
      "predict: 1072\n",
      "predict: 1073\n",
      "predict: 1074\n",
      "predict: 1075\n",
      "predict: 1076\n",
      "predict: 1077\n",
      "predict: 1078\n",
      "predict: 1079\n",
      "predict: 1080\n",
      "predict: 1081\n",
      "predict: 1082\n",
      "predict: 1083\n",
      "predict: 1084\n",
      "predict: 1085\n",
      "predict: 1086\n",
      "predict: 1087\n",
      "predict: 1088\n",
      "predict: 1089\n",
      "predict: 1090\n",
      "predict: 1091\n",
      "predict: 1092\n",
      "predict: 1093\n",
      "predict: 1094\n",
      "predict: 1095\n",
      "predict: 1096\n",
      "predict: 1097\n",
      "predict: 1098\n",
      "predict: 1099\n",
      "predict: 1100\n",
      "predict: 1101\n",
      "predict: 1102\n",
      "predict: 1103\n",
      "predict: 1104\n",
      "predict: 1105\n",
      "predict: 1106\n",
      "predict: 1107\n",
      "predict: 1108\n",
      "predict: 1109\n",
      "predict: 1110\n",
      "predict: 1111\n",
      "predict: 1112\n",
      "predict: 1113\n",
      "predict: 1114\n",
      "predict: 1115\n",
      "predict: 1116\n",
      "predict: 1117\n",
      "predict: 1118\n",
      "predict: 1119\n",
      "predict: 1120\n",
      "predict: 1121\n",
      "predict: 1122\n",
      "predict: 1123\n",
      "predict: 1124\n",
      "predict: 1125\n",
      "predict: 1126\n",
      "predict: 1127\n",
      "predict: 1128\n",
      "predict: 1129\n",
      "predict: 1130\n",
      "predict: 1131\n",
      "predict: 1132\n",
      "predict: 1133\n",
      "predict: 1134\n",
      "predict: 1135\n",
      "predict: 1136\n",
      "predict: 1137\n",
      "predict: 1138\n",
      "predict: 1139\n",
      "predict: 1140\n",
      "predict: 1141\n",
      "predict: 1142\n",
      "predict: 1143\n",
      "predict: 1144\n",
      "predict: 1145\n",
      "predict: 1146\n",
      "predict: 1147\n",
      "predict: 1148\n",
      "predict: 1149\n",
      "predict: 1150\n",
      "predict: 1151\n",
      "predict: 1152\n",
      "predict: 1153\n",
      "predict: 1154\n",
      "predict: 1155\n",
      "predict: 1156\n",
      "predict: 1157\n",
      "predict: 1158\n",
      "predict: 1159\n",
      "predict: 1160\n",
      "predict: 1161\n",
      "predict: 1162\n",
      "predict: 1163\n",
      "predict: 1164\n",
      "predict: 1165\n",
      "predict: 1166\n",
      "predict: 1167\n",
      "predict: 1168\n",
      "predict: 1169\n",
      "predict: 1170\n",
      "predict: 1171\n",
      "predict: 1172\n",
      "predict: 1173\n",
      "predict: 1174\n",
      "predict: 1175\n",
      "predict: 1176\n",
      "predict: 1177\n",
      "predict: 1178\n",
      "predict: 1179\n",
      "predict: 1180\n",
      "predict: 1181\n",
      "predict: 1182\n",
      "predict: 1183\n",
      "predict: 1184\n",
      "predict: 1185\n",
      "predict: 1186\n",
      "predict: 1187\n",
      "predict: 1188\n",
      "predict: 1189\n",
      "predict: 1190\n",
      "predict: 1191\n",
      "predict: 1192\n",
      "predict: 1193\n",
      "predict: 1194\n",
      "predict: 1195\n",
      "predict: 1196\n",
      "predict: 1197\n",
      "predict: 1198\n",
      "predict: 1199\n",
      "predict: 1200\n",
      "predict: 1201\n",
      "predict: 1202\n",
      "predict: 1203\n",
      "predict: 1204\n",
      "predict: 1205\n",
      "predict: 1206\n",
      "predict: 1207\n",
      "predict: 1208\n",
      "predict: 1209\n",
      "predict: 1210\n",
      "predict: 1211\n",
      "predict: 1212\n",
      "predict: 1213\n",
      "predict: 1214\n",
      "predict: 1215\n",
      "predict: 1216\n",
      "predict: 1217\n",
      "predict: 1218\n",
      "predict: 1219\n",
      "predict: 1220\n",
      "predict: 1221\n",
      "predict: 1222\n",
      "predict: 1223\n",
      "predict: 1224\n",
      "predict: 1225\n",
      "predict: 1226\n",
      "predict: 1227\n",
      "predict: 1228\n",
      "predict: 1229\n",
      "predict: 1230\n",
      "predict: 1231\n",
      "predict: 1232\n",
      "predict: 1233\n",
      "predict: 1234\n",
      "predict: 1235\n",
      "predict: 1236\n",
      "predict: 1237\n",
      "predict: 1238\n",
      "predict: 1239\n",
      "predict: 1240\n",
      "predict: 1241\n",
      "predict: 1242\n",
      "predict: 1243\n",
      "predict: 1244\n",
      "predict: 1245\n",
      "predict: 1246\n",
      "predict: 1247\n",
      "predict: 1248\n",
      "predict: 1249\n",
      "predict: 1250\n",
      "predict: 1251\n",
      "predict: 1252\n",
      "predict: 1253\n",
      "predict: 1254\n",
      "predict: 1255\n",
      "predict: 1256\n",
      "predict: 1257\n",
      "predict: 1258\n",
      "predict: 1259\n",
      "predict: 1260\n",
      "predict: 1261\n",
      "predict: 1262\n",
      "predict: 1263\n",
      "predict: 1264\n",
      "predict: 1265\n",
      "predict: 1266\n",
      "predict: 1267\n",
      "predict: 1268\n",
      "predict: 1269\n",
      "predict: 1270\n",
      "predict: 1271\n",
      "predict: 1272\n",
      "predict: 1273\n",
      "predict: 1274\n",
      "predict: 1275\n",
      "predict: 1276\n",
      "predict: 1277\n",
      "predict: 1278\n",
      "predict: 1279\n",
      "predict: 1280\n",
      "predict: 1281\n",
      "predict: 1282\n",
      "predict: 1283\n",
      "predict: 1284\n",
      "predict: 1285\n",
      "predict: 1286\n",
      "predict: 1287\n",
      "predict: 1288\n",
      "predict: 1289\n",
      "predict: 1290\n",
      "predict: 1291\n",
      "predict: 1292\n",
      "predict: 1293\n",
      "predict: 1294\n",
      "predict: 1295\n",
      "predict: 1296\n",
      "predict: 1297\n",
      "predict: 1298\n",
      "predict: 1299\n",
      "predict: 1300\n",
      "predict: 1301\n",
      "predict: 1302\n",
      "predict: 1303\n",
      "predict: 1304\n",
      "predict: 1305\n",
      "predict: 1306\n",
      "predict: 1307\n",
      "predict: 1308\n",
      "predict: 1309\n",
      "predict: 1310\n",
      "predict: 1311\n",
      "predict: 1312\n",
      "predict: 1313\n",
      "predict: 1314\n",
      "predict: 1315\n",
      "predict: 1316\n",
      "predict: 1317\n",
      "predict: 1318\n",
      "predict: 1319\n",
      "predict: 1320\n",
      "predict: 1321\n",
      "predict: 1322\n",
      "predict: 1323\n",
      "predict: 1324\n",
      "predict: 1325\n",
      "predict: 1326\n",
      "predict: 1327\n",
      "predict: 1328\n",
      "predict: 1329\n",
      "predict: 1330\n",
      "predict: 1331\n",
      "predict: 1332\n",
      "predict: 1333\n",
      "predict: 1334\n",
      "predict: 1335\n",
      "predict: 1336\n",
      "predict: 1337\n",
      "predict: 1338\n",
      "predict: 1339\n",
      "predict: 1340\n",
      "predict: 1341\n",
      "predict: 1342\n",
      "predict: 1343\n",
      "predict: 1344\n",
      "predict: 1345\n",
      "predict: 1346\n",
      "predict: 1347\n",
      "predict: 1348\n",
      "predict: 1349\n",
      "predict: 1350\n",
      "predict: 1351\n",
      "predict: 1352\n",
      "predict: 1353\n",
      "predict: 1354\n",
      "predict: 1355\n",
      "predict: 1356\n",
      "predict: 1357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 1358\n",
      "predict: 1359\n",
      "predict: 1360\n",
      "predict: 1361\n",
      "predict: 1362\n",
      "predict: 1363\n",
      "predict: 1364\n",
      "predict: 1365\n",
      "predict: 1366\n",
      "predict: 1367\n",
      "predict: 1368\n",
      "predict: 1369\n",
      "predict: 1370\n",
      "predict: 1371\n",
      "predict: 1372\n",
      "predict: 1373\n",
      "predict: 1374\n",
      "predict: 1375\n",
      "predict: 1376\n",
      "predict: 1377\n",
      "predict: 1378\n",
      "predict: 1379\n",
      "predict: 1380\n",
      "predict: 1381\n",
      "predict: 1382\n",
      "predict: 1383\n",
      "predict: 1384\n",
      "predict: 1385\n",
      "predict: 1386\n",
      "predict: 1387\n",
      "predict: 1388\n",
      "predict: 1389\n",
      "predict: 1390\n",
      "predict: 1391\n",
      "predict: 1392\n",
      "predict: 1393\n",
      "predict: 1394\n",
      "predict: 1395\n",
      "predict: 1396\n",
      "predict: 1397\n",
      "predict: 1398\n",
      "predict: 1399\n",
      "predict: 1400\n",
      "predict: 1401\n",
      "predict: 1402\n",
      "predict: 1403\n",
      "predict: 1404\n",
      "predict: 1405\n",
      "predict: 1406\n",
      "predict: 1407\n",
      "predict: 1408\n",
      "predict: 1409\n",
      "predict: 1410\n",
      "predict: 1411\n",
      "predict: 1412\n",
      "predict: 1413\n",
      "predict: 1414\n",
      "predict: 1415\n",
      "predict: 1416\n",
      "predict: 1417\n",
      "predict: 1418\n",
      "predict: 1419\n",
      "predict: 1420\n",
      "predict: 1421\n",
      "predict: 1422\n",
      "predict: 1423\n",
      "predict: 1424\n",
      "predict: 1425\n",
      "predict: 1426\n",
      "predict: 1427\n",
      "predict: 1428\n",
      "predict: 1429\n",
      "predict: 1430\n",
      "predict: 1431\n",
      "predict: 1432\n",
      "predict: 1433\n",
      "predict: 1434\n",
      "predict: 1435\n",
      "predict: 1436\n",
      "predict: 1437\n",
      "predict: 1438\n",
      "predict: 1439\n",
      "predict: 1440\n",
      "predict: 1441\n",
      "predict: 1442\n",
      "predict: 1443\n",
      "predict: 1444\n",
      "predict: 1445\n",
      "predict: 1446\n",
      "predict: 1447\n",
      "predict: 1448\n",
      "predict: 1449\n",
      "predict: 1450\n",
      "predict: 1451\n",
      "predict: 1452\n",
      "predict: 1453\n",
      "predict: 1454\n",
      "predict: 1455\n",
      "predict: 1456\n",
      "predict: 1457\n",
      "predict: 1458\n",
      "predict: 1459\n",
      "predict: 1460\n",
      "predict: 1461\n",
      "predict: 1462\n",
      "predict: 1463\n",
      "predict: 1464\n",
      "predict: 1465\n",
      "predict: 1466\n",
      "predict: 1467\n",
      "predict: 1468\n",
      "predict: 1469\n",
      "predict: 1470\n",
      "predict: 1471\n",
      "predict: 1472\n",
      "predict: 1473\n",
      "predict: 1474\n",
      "predict: 1475\n",
      "predict: 1476\n",
      "predict: 1477\n",
      "predict: 1478\n",
      "predict: 1479\n",
      "predict: 1480\n",
      "predict: 1481\n",
      "predict: 1482\n",
      "predict: 1483\n",
      "predict: 1484\n",
      "predict: 1485\n",
      "predict: 1486\n",
      "predict: 1487\n",
      "predict: 1488\n",
      "predict: 1489\n",
      "predict: 1490\n",
      "predict: 1491\n",
      "predict: 1492\n",
      "predict: 1493\n",
      "predict: 1494\n",
      "predict: 1495\n",
      "predict: 1496\n",
      "predict: 1497\n",
      "predict: 1498\n",
      "predict: 1499\n",
      "predict: 1500\n",
      "predict: 1501\n",
      "predict: 1502\n",
      "predict: 1503\n",
      "predict: 1504\n",
      "predict: 1505\n",
      "predict: 1506\n",
      "predict: 1507\n",
      "predict: 1508\n",
      "predict: 1509\n",
      "predict: 1510\n",
      "predict: 1511\n",
      "predict: 1512\n",
      "predict: 1513\n",
      "predict: 1514\n",
      "predict: 1515\n",
      "predict: 1516\n",
      "predict: 1517\n",
      "predict: 1518\n",
      "predict: 1519\n",
      "predict: 1520\n",
      "predict: 1521\n",
      "predict: 1522\n",
      "predict: 1523\n",
      "predict: 1524\n",
      "predict: 1525\n",
      "predict: 1526\n",
      "predict: 1527\n",
      "predict: 1528\n",
      "predict: 1529\n",
      "predict: 1530\n",
      "predict: 1531\n",
      "predict: 1532\n",
      "predict: 1533\n",
      "predict: 1534\n",
      "predict: 1535\n",
      "predict: 1536\n",
      "predict: 1537\n",
      "predict: 1538\n",
      "predict: 1539\n",
      "predict: 1540\n",
      "predict: 1541\n",
      "predict: 1542\n",
      "predict: 1543\n",
      "predict: 1544\n",
      "predict: 1545\n",
      "predict: 1546\n",
      "predict: 1547\n",
      "predict: 1548\n",
      "predict: 1549\n",
      "predict: 1550\n",
      "predict: 1551\n",
      "predict: 1552\n",
      "predict: 1553\n",
      "predict: 1554\n",
      "predict: 1555\n",
      "predict: 1556\n",
      "predict: 1557\n",
      "predict: 1558\n",
      "predict: 1559\n",
      "predict: 1560\n",
      "predict: 1561\n",
      "predict: 1562\n",
      "predict: 1563\n",
      "predict: 1564\n",
      "predict: 1565\n",
      "predict: 1566\n",
      "predict: 1567\n",
      "predict: 1568\n",
      "predict: 1569\n",
      "predict: 1570\n",
      "predict: 1571\n",
      "predict: 1572\n",
      "predict: 1573\n",
      "predict: 1574\n",
      "predict: 1575\n",
      "predict: 1576\n",
      "predict: 1577\n",
      "predict: 1578\n",
      "predict: 1579\n",
      "predict: 1580\n",
      "predict: 1581\n",
      "predict: 1582\n",
      "predict: 1583\n",
      "predict: 1584\n",
      "predict: 1585\n",
      "predict: 1586\n",
      "predict: 1587\n",
      "predict: 1588\n",
      "predict: 1589\n",
      "predict: 1590\n",
      "predict: 1591\n",
      "predict: 1592\n",
      "predict: 1593\n",
      "predict: 1594\n",
      "predict: 1595\n",
      "predict: 1596\n",
      "predict: 1597\n",
      "predict: 1598\n",
      "predict: 1599\n",
      "predict: 1600\n",
      "predict: 1601\n",
      "predict: 1602\n",
      "predict: 1603\n",
      "predict: 1604\n",
      "predict: 1605\n",
      "predict: 1606\n",
      "predict: 1607\n",
      "predict: 1608\n",
      "predict: 1609\n",
      "predict: 1610\n",
      "predict: 1611\n",
      "predict: 1612\n",
      "predict: 1613\n",
      "predict: 1614\n",
      "predict: 1615\n",
      "predict: 1616\n",
      "predict: 1617\n",
      "predict: 1618\n",
      "predict: 1619\n",
      "predict: 1620\n",
      "predict: 1621\n",
      "predict: 1622\n",
      "predict: 1623\n",
      "predict: 1624\n",
      "predict: 1625\n",
      "predict: 1626\n",
      "predict: 1627\n",
      "predict: 1628\n",
      "predict: 1629\n",
      "predict: 1630\n",
      "predict: 1631\n",
      "predict: 1632\n",
      "predict: 1633\n",
      "predict: 1634\n",
      "predict: 1635\n",
      "predict: 1636\n",
      "predict: 1637\n",
      "predict: 1638\n",
      "predict: 1639\n",
      "predict: 1640\n",
      "predict: 1641\n",
      "predict: 1642\n",
      "predict: 1643\n",
      "predict: 1644\n",
      "predict: 1645\n",
      "predict: 1646\n",
      "predict: 1647\n",
      "predict: 1648\n",
      "predict: 1649\n",
      "predict: 1650\n",
      "predict: 1651\n",
      "predict: 1652\n",
      "predict: 1653\n",
      "predict: 1654\n",
      "predict: 1655\n",
      "predict: 1656\n",
      "predict: 1657\n",
      "predict: 1658\n",
      "predict: 1659\n",
      "predict: 1660\n",
      "predict: 1661\n",
      "predict: 1662\n",
      "predict: 1663\n",
      "predict: 1664\n",
      "predict: 1665\n",
      "predict: 1666\n",
      "predict: 1667\n",
      "predict: 1668\n",
      "predict: 1669\n",
      "predict: 1670\n",
      "predict: 1671\n",
      "predict: 1672\n",
      "predict: 1673\n",
      "predict: 1674\n",
      "predict: 1675\n",
      "predict: 1676\n",
      "predict: 1677\n",
      "predict: 1678\n",
      "predict: 1679\n",
      "predict: 1680\n",
      "predict: 1681\n",
      "predict: 1682\n",
      "predict: 1683\n",
      "predict: 1684\n",
      "predict: 1685\n",
      "predict: 1686\n",
      "predict: 1687\n",
      "predict: 1688\n",
      "predict: 1689\n",
      "predict: 1690\n",
      "predict: 1691\n",
      "predict: 1692\n",
      "predict: 1693\n",
      "predict: 1694\n",
      "predict: 1695\n",
      "predict: 1696\n",
      "predict: 1697\n",
      "predict: 1698\n",
      "predict: 1699\n",
      "predict: 1700\n",
      "predict: 1701\n",
      "predict: 1702\n",
      "predict: 1703\n",
      "predict: 1704\n",
      "predict: 1705\n",
      "predict: 1706\n",
      "predict: 1707\n",
      "predict: 1708\n",
      "predict: 1709\n",
      "predict: 1710\n",
      "predict: 1711\n",
      "predict: 1712\n",
      "predict: 1713\n",
      "predict: 1714\n",
      "predict: 1715\n",
      "predict: 1716\n",
      "predict: 1717\n",
      "predict: 1718\n",
      "predict: 1719\n",
      "predict: 1720\n",
      "predict: 1721\n",
      "predict: 1722\n",
      "predict: 1723\n",
      "predict: 1724\n",
      "predict: 1725\n",
      "predict: 1726\n",
      "predict: 1727\n",
      "predict: 1728\n",
      "predict: 1729\n",
      "predict: 1730\n",
      "predict: 1731\n",
      "predict: 1732\n",
      "predict: 1733\n",
      "predict: 1734\n",
      "predict: 1735\n",
      "predict: 1736\n",
      "predict: 1737\n",
      "predict: 1738\n",
      "predict: 1739\n",
      "predict: 1740\n",
      "predict: 1741\n",
      "predict: 1742\n",
      "predict: 1743\n",
      "predict: 1744\n",
      "predict: 1745\n",
      "predict: 1746\n",
      "predict: 1747\n",
      "predict: 1748\n",
      "predict: 1749\n",
      "predict: 1750\n",
      "predict: 1751\n",
      "predict: 1752\n",
      "predict: 1753\n",
      "predict: 1754\n",
      "predict: 1755\n",
      "predict: 1756\n",
      "predict: 1757\n",
      "predict: 1758\n",
      "predict: 1759\n",
      "predict: 1760\n",
      "predict: 1761\n",
      "predict: 1762\n",
      "predict: 1763\n",
      "predict: 1764\n",
      "predict: 1765\n",
      "predict: 1766\n",
      "predict: 1767\n",
      "predict: 1768\n",
      "predict: 1769\n",
      "predict: 1770\n",
      "predict: 1771\n",
      "predict: 1772\n",
      "predict: 1773\n",
      "predict: 1774\n",
      "predict: 1775\n",
      "predict: 1776\n",
      "predict: 1777\n",
      "predict: 1778\n",
      "predict: 1779\n",
      "predict: 1780\n",
      "predict: 1781\n",
      "predict: 1782\n",
      "predict: 1783\n",
      "predict: 1784\n",
      "predict: 1785\n",
      "predict: 1786\n",
      "predict: 1787\n",
      "predict: 1788\n",
      "predict: 1789\n",
      "predict: 1790\n",
      "predict: 1791\n",
      "predict: 1792\n",
      "predict: 1793\n",
      "predict: 1794\n",
      "predict: 1795\n",
      "predict: 1796\n",
      "predict: 1797\n",
      "predict: 1798\n",
      "predict: 1799\n",
      "predict: 1800\n",
      "predict: 1801\n",
      "predict: 1802\n",
      "predict: 1803\n",
      "predict: 1804\n",
      "predict: 1805\n",
      "predict: 1806\n",
      "predict: 1807\n",
      "predict: 1808\n",
      "predict: 1809\n",
      "predict: 1810\n",
      "predict: 1811\n",
      "predict: 1812\n",
      "predict: 1813\n",
      "predict: 1814\n",
      "predict: 1815\n",
      "predict: 1816\n",
      "predict: 1817\n",
      "predict: 1818\n",
      "predict: 1819\n",
      "predict: 1820\n",
      "predict: 1821\n",
      "predict: 1822\n",
      "predict: 1823\n",
      "predict: 1824\n",
      "predict: 1825\n",
      "predict: 1826\n",
      "predict: 1827\n",
      "predict: 1828\n",
      "predict: 1829\n",
      "predict: 1830\n",
      "predict: 1831\n",
      "predict: 1832\n",
      "predict: 1833\n",
      "predict: 1834\n",
      "predict: 1835\n",
      "predict: 1836\n",
      "predict: 1837\n",
      "predict: 1838\n",
      "predict: 1839\n",
      "predict: 1840\n",
      "predict: 1841\n",
      "predict: 1842\n",
      "predict: 1843\n",
      "predict: 1844\n",
      "predict: 1845\n",
      "predict: 1846\n",
      "predict: 1847\n",
      "predict: 1848\n",
      "predict: 1849\n",
      "predict: 1850\n",
      "predict: 1851\n",
      "predict: 1852\n",
      "predict: 1853\n",
      "predict: 1854\n",
      "predict: 1855\n",
      "predict: 1856\n",
      "predict: 1857\n",
      "predict: 1858\n",
      "predict: 1859\n",
      "predict: 1860\n",
      "predict: 1861\n",
      "predict: 1862\n",
      "predict: 1863\n",
      "predict: 1864\n",
      "predict: 1865\n",
      "predict: 1866\n",
      "predict: 1867\n",
      "predict: 1868\n",
      "predict: 1869\n",
      "predict: 1870\n",
      "predict: 1871\n",
      "predict: 1872\n",
      "predict: 1873\n",
      "predict: 1874\n",
      "predict: 1875\n",
      "predict: 1876\n",
      "predict: 1877\n",
      "predict: 1878\n",
      "predict: 1879\n",
      "predict: 1880\n",
      "predict: 1881\n",
      "predict: 1882\n",
      "predict: 1883\n",
      "predict: 1884\n",
      "predict: 1885\n",
      "predict: 1886\n",
      "predict: 1887\n",
      "predict: 1888\n",
      "predict: 1889\n",
      "predict: 1890\n",
      "predict: 1891\n",
      "predict: 1892\n",
      "predict: 1893\n",
      "predict: 1894\n",
      "predict: 1895\n",
      "predict: 1896\n",
      "predict: 1897\n",
      "predict: 1898\n",
      "predict: 1899\n",
      "predict: 1900\n",
      "predict: 1901\n",
      "predict: 1902\n",
      "predict: 1903\n",
      "predict: 1904\n",
      "predict: 1905\n",
      "predict: 1906\n",
      "predict: 1907\n",
      "predict: 1908\n",
      "predict: 1909\n",
      "predict: 1910\n",
      "predict: 1911\n",
      "predict: 1912\n",
      "predict: 1913\n",
      "predict: 1914\n",
      "predict: 1915\n",
      "predict: 1916\n",
      "predict: 1917\n",
      "predict: 1918\n",
      "predict: 1919\n",
      "predict: 1920\n",
      "predict: 1921\n",
      "predict: 1922\n",
      "predict: 1923\n",
      "predict: 1924\n",
      "predict: 1925\n",
      "predict: 1926\n",
      "predict: 1927\n",
      "predict: 1928\n",
      "predict: 1929\n",
      "predict: 1930\n",
      "predict: 1931\n",
      "predict: 1932\n",
      "predict: 1933\n",
      "predict: 1934\n",
      "predict: 1935\n",
      "predict: 1936\n",
      "predict: 1937\n",
      "predict: 1938\n",
      "predict: 1939\n",
      "predict: 1940\n",
      "predict: 1941\n",
      "predict: 1942\n",
      "predict: 1943\n",
      "predict: 1944\n",
      "predict: 1945\n",
      "predict: 1946\n",
      "predict: 1947\n",
      "predict: 1948\n",
      "predict: 1949\n",
      "predict: 1950\n",
      "predict: 1951\n",
      "predict: 1952\n",
      "predict: 1953\n",
      "predict: 1954\n",
      "predict: 1955\n",
      "predict: 1956\n",
      "predict: 1957\n",
      "predict: 1958\n",
      "predict: 1959\n",
      "predict: 1960\n",
      "predict: 1961\n",
      "predict: 1962\n",
      "predict: 1963\n",
      "predict: 1964\n",
      "predict: 1965\n",
      "predict: 1966\n",
      "predict: 1967\n",
      "predict: 1968\n",
      "predict: 1969\n",
      "predict: 1970\n",
      "predict: 1971\n",
      "predict: 1972\n",
      "predict: 1973\n",
      "predict: 1974\n",
      "predict: 1975\n",
      "predict: 1976\n",
      "predict: 1977\n",
      "predict: 1978\n",
      "predict: 1979\n",
      "predict: 1980\n",
      "predict: 1981\n",
      "predict: 1982\n",
      "predict: 1983\n",
      "predict: 1984\n",
      "predict: 1985\n",
      "predict: 1986\n",
      "predict: 1987\n",
      "predict: 1988\n",
      "predict: 1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 1990\n",
      "predict: 1991\n",
      "predict: 1992\n",
      "predict: 1993\n",
      "predict: 1994\n",
      "predict: 1995\n",
      "predict: 1996\n",
      "predict: 1997\n",
      "predict: 1998\n",
      "predict: 1999\n",
      "predict: 2000\n",
      "predict: 2001\n",
      "predict: 2002\n",
      "predict: 2003\n",
      "predict: 2004\n",
      "predict: 2005\n",
      "predict: 2006\n",
      "predict: 2007\n",
      "predict: 2008\n",
      "predict: 2009\n",
      "predict: 2010\n",
      "predict: 2011\n",
      "predict: 2012\n",
      "predict: 2013\n",
      "predict: 2014\n",
      "predict: 2015\n",
      "predict: 2016\n",
      "predict: 2017\n",
      "predict: 2018\n",
      "predict: 2019\n",
      "predict: 2020\n",
      "predict: 2021\n",
      "predict: 2022\n",
      "predict: 2023\n",
      "predict: 2024\n",
      "predict: 2025\n",
      "predict: 2026\n",
      "predict: 2027\n",
      "predict: 2028\n",
      "predict: 2029\n",
      "predict: 2030\n",
      "predict: 2031\n",
      "predict: 2032\n",
      "predict: 2033\n",
      "predict: 2034\n",
      "predict: 2035\n",
      "predict: 2036\n",
      "predict: 2037\n",
      "predict: 2038\n",
      "predict: 2039\n",
      "predict: 2040\n",
      "predict: 2041\n",
      "predict: 2042\n",
      "predict: 2043\n",
      "predict: 2044\n",
      "predict: 2045\n",
      "predict: 2046\n",
      "predict: 2047\n",
      "predict: 2048\n",
      "predict: 2049\n",
      "predict: 2050\n",
      "predict: 2051\n",
      "predict: 2052\n",
      "predict: 2053\n",
      "predict: 2054\n",
      "predict: 2055\n",
      "predict: 2056\n",
      "predict: 2057\n",
      "predict: 2058\n",
      "predict: 2059\n",
      "predict: 2060\n",
      "predict: 2061\n",
      "predict: 2062\n",
      "predict: 2063\n",
      "predict: 2064\n",
      "predict: 2065\n",
      "predict: 2066\n",
      "predict: 2067\n",
      "predict: 2068\n",
      "predict: 2069\n",
      "predict: 2070\n",
      "predict: 2071\n",
      "predict: 2072\n",
      "predict: 2073\n",
      "predict: 2074\n",
      "predict: 2075\n",
      "predict: 2076\n",
      "predict: 2077\n",
      "predict: 2078\n",
      "predict: 2079\n",
      "predict: 2080\n",
      "predict: 2081\n",
      "predict: 2082\n",
      "predict: 2083\n",
      "predict: 2084\n",
      "predict: 2085\n",
      "predict: 2086\n",
      "predict: 2087\n",
      "predict: 2088\n",
      "predict: 2089\n",
      "predict: 2090\n",
      "predict: 2091\n",
      "predict: 2092\n",
      "predict: 2093\n",
      "predict: 2094\n",
      "predict: 2095\n",
      "predict: 2096\n",
      "predict: 2097\n",
      "predict: 2098\n",
      "predict: 2099\n",
      "predict: 2100\n",
      "predict: 2101\n",
      "predict: 2102\n",
      "predict: 2103\n",
      "predict: 2104\n",
      "predict: 2105\n",
      "predict: 2106\n",
      "predict: 2107\n",
      "predict: 2108\n",
      "predict: 2109\n",
      "predict: 2110\n",
      "predict: 2111\n",
      "predict: 2112\n",
      "predict: 2113\n",
      "predict: 2114\n",
      "predict: 2115\n",
      "predict: 2116\n",
      "predict: 2117\n",
      "predict: 2118\n",
      "predict: 2119\n",
      "predict: 2120\n",
      "predict: 2121\n",
      "predict: 2122\n",
      "predict: 2123\n",
      "predict: 2124\n",
      "predict: 2125\n",
      "predict: 2126\n",
      "predict: 2127\n",
      "predict: 2128\n",
      "predict: 2129\n",
      "predict: 2130\n",
      "predict: 2131\n",
      "predict: 2132\n",
      "predict: 2133\n",
      "predict: 2134\n",
      "predict: 2135\n",
      "predict: 2136\n",
      "predict: 2137\n",
      "predict: 2138\n",
      "predict: 2139\n",
      "predict: 2140\n",
      "predict: 2141\n",
      "predict: 2142\n",
      "predict: 2143\n",
      "predict: 2144\n",
      "predict: 2145\n",
      "predict: 2146\n",
      "predict: 2147\n",
      "predict: 2148\n",
      "predict: 2149\n",
      "predict: 2150\n",
      "predict: 2151\n",
      "predict: 2152\n",
      "predict: 2153\n",
      "predict: 2154\n",
      "predict: 2155\n",
      "predict: 2156\n",
      "predict: 2157\n",
      "predict: 2158\n",
      "predict: 2159\n",
      "predict: 2160\n",
      "predict: 2161\n",
      "predict: 2162\n",
      "predict: 2163\n",
      "predict: 2164\n",
      "predict: 2165\n",
      "predict: 2166\n",
      "predict: 2167\n",
      "predict: 2168\n",
      "predict: 2169\n",
      "predict: 2170\n",
      "predict: 2171\n",
      "predict: 2172\n",
      "predict: 2173\n",
      "predict: 2174\n",
      "predict: 2175\n",
      "predict: 2176\n",
      "predict: 2177\n",
      "predict: 2178\n",
      "predict: 2179\n",
      "predict: 2180\n",
      "predict: 2181\n",
      "predict: 2182\n",
      "predict: 2183\n",
      "predict: 2184\n",
      "predict: 2185\n",
      "predict: 2186\n",
      "predict: 2187\n",
      "predict: 2188\n",
      "predict: 2189\n",
      "predict: 2190\n",
      "predict: 2191\n",
      "predict: 2192\n",
      "predict: 2193\n",
      "predict: 2194\n",
      "predict: 2195\n",
      "predict: 2196\n",
      "predict: 2197\n",
      "predict: 2198\n",
      "predict: 2199\n",
      "predict: 2200\n",
      "predict: 2201\n",
      "predict: 2202\n",
      "predict: 2203\n",
      "predict: 2204\n",
      "predict: 2205\n",
      "predict: 2206\n",
      "predict: 2207\n",
      "predict: 2208\n",
      "predict: 2209\n",
      "predict: 2210\n",
      "predict: 2211\n",
      "predict: 2212\n",
      "predict: 2213\n",
      "predict: 2214\n",
      "predict: 2215\n",
      "predict: 2216\n",
      "predict: 2217\n",
      "predict: 2218\n",
      "predict: 2219\n",
      "predict: 2220\n",
      "predict: 2221\n",
      "predict: 2222\n",
      "predict: 2223\n",
      "predict: 2224\n",
      "predict: 2225\n",
      "predict: 2226\n",
      "predict: 2227\n",
      "predict: 2228\n",
      "predict: 2229\n",
      "predict: 2230\n",
      "predict: 2231\n",
      "predict: 2232\n",
      "predict: 2233\n",
      "predict: 2234\n",
      "predict: 2235\n",
      "predict: 2236\n",
      "predict: 2237\n",
      "predict: 2238\n",
      "predict: 2239\n",
      "predict: 2240\n",
      "predict: 2241\n",
      "predict: 2242\n",
      "predict: 2243\n",
      "predict: 2244\n",
      "predict: 2245\n",
      "predict: 2246\n",
      "predict: 2247\n",
      "predict: 2248\n",
      "predict: 2249\n",
      "predict: 2250\n",
      "predict: 2251\n",
      "predict: 2252\n",
      "predict: 2253\n",
      "predict: 2254\n",
      "predict: 2255\n",
      "predict: 2256\n",
      "predict: 2257\n",
      "predict: 2258\n",
      "predict: 2259\n",
      "predict: 2260\n",
      "predict: 2261\n",
      "predict: 2262\n",
      "predict: 2263\n",
      "predict: 2264\n",
      "predict: 2265\n",
      "predict: 2266\n",
      "predict: 2267\n",
      "predict: 2268\n",
      "predict: 2269\n",
      "predict: 2270\n",
      "predict: 2271\n",
      "predict: 2272\n",
      "predict: 2273\n",
      "predict: 2274\n",
      "predict: 2275\n",
      "predict: 2276\n",
      "predict: 2277\n",
      "predict: 2278\n",
      "predict: 2279\n",
      "predict: 2280\n",
      "predict: 2281\n",
      "predict: 2282\n",
      "predict: 2283\n",
      "predict: 2284\n",
      "predict: 2285\n",
      "predict: 2286\n",
      "predict: 2287\n",
      "predict: 2288\n",
      "predict: 2289\n",
      "predict: 2290\n",
      "predict: 2291\n",
      "predict: 2292\n",
      "predict: 2293\n",
      "predict: 2294\n",
      "predict: 2295\n",
      "predict: 2296\n",
      "predict: 2297\n",
      "predict: 2298\n",
      "predict: 2299\n",
      "predict: 2300\n",
      "predict: 2301\n",
      "predict: 2302\n",
      "predict: 2303\n",
      "predict: 2304\n",
      "predict: 2305\n",
      "predict: 2306\n",
      "predict: 2307\n",
      "predict: 2308\n",
      "predict: 2309\n",
      "predict: 2310\n",
      "predict: 2311\n",
      "predict: 2312\n",
      "predict: 2313\n",
      "predict: 2314\n",
      "predict: 2315\n",
      "predict: 2316\n",
      "predict: 2317\n",
      "predict: 2318\n",
      "predict: 2319\n",
      "predict: 2320\n",
      "predict: 2321\n",
      "predict: 2322\n",
      "predict: 2323\n",
      "predict: 2324\n",
      "predict: 2325\n",
      "predict: 2326\n",
      "predict: 2327\n",
      "predict: 2328\n",
      "predict: 2329\n",
      "predict: 2330\n",
      "predict: 2331\n",
      "predict: 2332\n",
      "predict: 2333\n",
      "predict: 2334\n",
      "predict: 2335\n",
      "predict: 2336\n",
      "predict: 2337\n",
      "predict: 2338\n",
      "predict: 2339\n",
      "predict: 2340\n",
      "predict: 2341\n",
      "predict: 2342\n",
      "predict: 2343\n",
      "predict: 2344\n",
      "predict: 2345\n",
      "predict: 2346\n",
      "predict: 2347\n",
      "predict: 2348\n",
      "predict: 2349\n",
      "predict: 2350\n",
      "predict: 2351\n",
      "predict: 2352\n",
      "predict: 2353\n",
      "predict: 2354\n",
      "predict: 2355\n",
      "predict: 2356\n",
      "predict: 2357\n",
      "predict: 2358\n",
      "predict: 2359\n",
      "predict: 2360\n",
      "predict: 2361\n",
      "predict: 2362\n",
      "predict: 2363\n",
      "predict: 2364\n",
      "predict: 2365\n",
      "predict: 2366\n",
      "predict: 2367\n",
      "predict: 2368\n",
      "predict: 2369\n",
      "predict: 2370\n",
      "predict: 2371\n",
      "predict: 2372\n",
      "predict: 2373\n",
      "predict: 2374\n",
      "predict: 2375\n",
      "predict: 2376\n",
      "predict: 2377\n",
      "predict: 2378\n",
      "predict: 2379\n",
      "predict: 2380\n",
      "predict: 2381\n",
      "predict: 2382\n",
      "predict: 2383\n",
      "predict: 2384\n",
      "predict: 2385\n",
      "predict: 2386\n",
      "predict: 2387\n",
      "predict: 2388\n",
      "predict: 2389\n",
      "predict: 2390\n",
      "predict: 2391\n",
      "predict: 2392\n",
      "predict: 2393\n",
      "predict: 2394\n",
      "predict: 2395\n",
      "predict: 2396\n",
      "predict: 2397\n",
      "predict: 2398\n",
      "predict: 2399\n",
      "predict: 2400\n",
      "predict: 2401\n",
      "predict: 2402\n",
      "predict: 2403\n",
      "predict: 2404\n",
      "predict: 2405\n",
      "predict: 2406\n",
      "predict: 2407\n",
      "predict: 2408\n",
      "predict: 2409\n",
      "predict: 2410\n",
      "predict: 2411\n",
      "predict: 2412\n",
      "predict: 2413\n",
      "predict: 2414\n",
      "predict: 2415\n",
      "predict: 2416\n",
      "predict: 2417\n",
      "predict: 2418\n",
      "predict: 2419\n",
      "predict: 2420\n",
      "predict: 2421\n",
      "predict: 2422\n",
      "predict: 2423\n",
      "predict: 2424\n",
      "predict: 2425\n",
      "predict: 2426\n",
      "predict: 2427\n",
      "predict: 2428\n",
      "predict: 2429\n",
      "predict: 2430\n",
      "predict: 2431\n",
      "predict: 2432\n",
      "predict: 2433\n",
      "predict: 2434\n",
      "predict: 2435\n",
      "predict: 2436\n",
      "predict: 2437\n",
      "predict: 2438\n",
      "predict: 2439\n",
      "predict: 2440\n",
      "predict: 2441\n",
      "predict: 2442\n",
      "predict: 2443\n",
      "predict: 2444\n",
      "predict: 2445\n",
      "predict: 2446\n",
      "predict: 2447\n",
      "predict: 2448\n",
      "predict: 2449\n",
      "predict: 2450\n",
      "predict: 2451\n",
      "predict: 2452\n",
      "predict: 2453\n",
      "predict: 2454\n",
      "predict: 2455\n",
      "predict: 2456\n",
      "predict: 2457\n",
      "predict: 2458\n",
      "predict: 2459\n",
      "predict: 2460\n",
      "predict: 2461\n",
      "predict: 2462\n",
      "predict: 2463\n",
      "predict: 2464\n",
      "predict: 2465\n",
      "predict: 2466\n",
      "predict: 2467\n",
      "predict: 2468\n",
      "predict: 2469\n",
      "predict: 2470\n",
      "predict: 2471\n",
      "predict: 2472\n",
      "predict: 2473\n",
      "predict: 2474\n",
      "predict: 2475\n",
      "predict: 2476\n",
      "predict: 2477\n",
      "predict: 2478\n",
      "predict: 2479\n",
      "predict: 2480\n",
      "predict: 2481\n",
      "predict: 2482\n",
      "predict: 2483\n",
      "predict: 2484\n",
      "predict: 2485\n",
      "predict: 2486\n",
      "predict: 2487\n",
      "predict: 2488\n",
      "predict: 2489\n",
      "predict: 2490\n",
      "predict: 2491\n",
      "predict: 2492\n",
      "predict: 2493\n",
      "predict: 2494\n",
      "predict: 2495\n",
      "predict: 2496\n",
      "predict: 2497\n",
      "predict: 2498\n",
      "predict: 2499\n",
      "predict: 2500\n",
      "predict: 2501\n",
      "predict: 2502\n",
      "predict: 2503\n",
      "predict: 2504\n",
      "predict: 2505\n",
      "predict: 2506\n",
      "predict: 2507\n",
      "predict: 2508\n",
      "predict: 2509\n",
      "predict: 2510\n",
      "predict: 2511\n",
      "predict: 2512\n",
      "predict: 2513\n",
      "predict: 2514\n",
      "predict: 2515\n",
      "predict: 2516\n",
      "predict: 2517\n",
      "predict: 2518\n",
      "predict: 2519\n",
      "predict: 2520\n",
      "predict: 2521\n",
      "predict: 2522\n",
      "predict: 2523\n",
      "predict: 2524\n",
      "predict: 2525\n",
      "predict: 2526\n",
      "predict: 2527\n",
      "predict: 2528\n",
      "predict: 2529\n",
      "predict: 2530\n",
      "predict: 2531\n",
      "predict: 2532\n",
      "predict: 2533\n",
      "predict: 2534\n",
      "predict: 2535\n",
      "predict: 2536\n",
      "predict: 2537\n",
      "predict: 2538\n",
      "predict: 2539\n",
      "predict: 2540\n",
      "predict: 2541\n",
      "predict: 2542\n",
      "predict: 2543\n",
      "predict: 2544\n",
      "predict: 2545\n",
      "predict: 2546\n",
      "predict: 2547\n",
      "predict: 2548\n",
      "predict: 2549\n",
      "predict: 2550\n",
      "predict: 2551\n",
      "predict: 2552\n",
      "predict: 2553\n",
      "predict: 2554\n",
      "predict: 2555\n",
      "predict: 2556\n",
      "predict: 2557\n",
      "predict: 2558\n",
      "predict: 2559\n",
      "predict: 2560\n",
      "predict: 2561\n",
      "predict: 2562\n",
      "predict: 2563\n",
      "predict: 2564\n",
      "predict: 2565\n",
      "predict: 2566\n",
      "predict: 2567\n",
      "predict: 2568\n",
      "predict: 2569\n",
      "predict: 2570\n",
      "predict: 2571\n",
      "predict: 2572\n",
      "predict: 2573\n",
      "predict: 2574\n",
      "predict: 2575\n",
      "predict: 2576\n",
      "predict: 2577\n",
      "predict: 2578\n",
      "predict: 2579\n",
      "predict: 2580\n",
      "predict: 2581\n",
      "predict: 2582\n",
      "predict: 2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 2584\n",
      "predict: 2585\n",
      "predict: 2586\n",
      "predict: 2587\n",
      "predict: 2588\n",
      "predict: 2589\n",
      "predict: 2590\n",
      "predict: 2591\n",
      "predict: 2592\n",
      "predict: 2593\n",
      "predict: 2594\n",
      "predict: 2595\n",
      "predict: 2596\n",
      "predict: 2597\n",
      "predict: 2598\n",
      "predict: 2599\n",
      "predict: 2600\n",
      "predict: 2601\n",
      "predict: 2602\n",
      "predict: 2603\n",
      "predict: 2604\n",
      "predict: 2605\n",
      "predict: 2606\n",
      "predict: 2607\n",
      "predict: 2608\n",
      "predict: 2609\n",
      "predict: 2610\n",
      "predict: 2611\n",
      "predict: 2612\n",
      "predict: 2613\n",
      "predict: 2614\n",
      "predict: 2615\n",
      "predict: 2616\n",
      "predict: 2617\n",
      "predict: 2618\n",
      "predict: 2619\n",
      "predict: 2620\n",
      "predict: 2621\n",
      "predict: 2622\n",
      "predict: 2623\n",
      "predict: 2624\n",
      "predict: 2625\n",
      "predict: 2626\n",
      "predict: 2627\n",
      "predict: 2628\n",
      "predict: 2629\n",
      "predict: 2630\n",
      "predict: 2631\n",
      "predict: 2632\n",
      "predict: 2633\n",
      "predict: 2634\n",
      "predict: 2635\n",
      "predict: 2636\n",
      "predict: 2637\n",
      "predict: 2638\n",
      "predict: 2639\n",
      "predict: 2640\n",
      "predict: 2641\n",
      "predict: 2642\n",
      "predict: 2643\n",
      "predict: 2644\n",
      "predict: 2645\n",
      "predict: 2646\n",
      "predict: 2647\n",
      "predict: 2648\n",
      "predict: 2649\n",
      "predict: 2650\n",
      "predict: 2651\n",
      "predict: 2652\n",
      "predict: 2653\n",
      "predict: 2654\n",
      "predict: 2655\n",
      "predict: 2656\n",
      "predict: 2657\n",
      "predict: 2658\n",
      "predict: 2659\n",
      "predict: 2660\n",
      "predict: 2661\n",
      "predict: 2662\n",
      "predict: 2663\n",
      "predict: 2664\n",
      "predict: 2665\n",
      "predict: 2666\n",
      "predict: 2667\n",
      "predict: 2668\n",
      "predict: 2669\n",
      "predict: 2670\n",
      "predict: 2671\n",
      "predict: 2672\n",
      "predict: 2673\n",
      "predict: 2674\n",
      "predict: 2675\n",
      "predict: 2676\n",
      "predict: 2677\n",
      "predict: 2678\n",
      "predict: 2679\n",
      "predict: 2680\n",
      "predict: 2681\n",
      "predict: 2682\n",
      "predict: 2683\n",
      "predict: 2684\n",
      "predict: 2685\n",
      "predict: 2686\n",
      "predict: 2687\n",
      "predict: 2688\n",
      "predict: 2689\n",
      "predict: 2690\n",
      "predict: 2691\n",
      "predict: 2692\n",
      "predict: 2693\n",
      "predict: 2694\n",
      "predict: 2695\n",
      "predict: 2696\n",
      "predict: 2697\n",
      "predict: 2698\n",
      "predict: 2699\n",
      "predict: 2700\n",
      "predict: 2701\n",
      "predict: 2702\n",
      "predict: 2703\n",
      "predict: 2704\n",
      "predict: 2705\n",
      "predict: 2706\n",
      "predict: 2707\n",
      "predict: 2708\n",
      "predict: 2709\n",
      "predict: 2710\n",
      "predict: 2711\n",
      "predict: 2712\n",
      "predict: 2713\n",
      "predict: 2714\n",
      "predict: 2715\n",
      "predict: 2716\n",
      "predict: 2717\n",
      "predict: 2718\n",
      "predict: 2719\n",
      "predict: 2720\n",
      "predict: 2721\n",
      "predict: 2722\n",
      "predict: 2723\n",
      "predict: 2724\n",
      "predict: 2725\n",
      "predict: 2726\n",
      "predict: 2727\n",
      "predict: 2728\n",
      "predict: 2729\n",
      "predict: 2730\n",
      "predict: 2731\n",
      "predict: 2732\n",
      "predict: 2733\n",
      "predict: 2734\n",
      "predict: 2735\n",
      "predict: 2736\n",
      "predict: 2737\n",
      "predict: 2738\n",
      "predict: 2739\n",
      "predict: 2740\n",
      "predict: 2741\n",
      "predict: 2742\n",
      "predict: 2743\n",
      "predict: 2744\n",
      "predict: 2745\n",
      "predict: 2746\n",
      "predict: 2747\n",
      "predict: 2748\n",
      "predict: 2749\n",
      "predict: 2750\n",
      "predict: 2751\n",
      "predict: 2752\n",
      "predict: 2753\n",
      "predict: 2754\n",
      "predict: 2755\n",
      "predict: 2756\n",
      "predict: 2757\n",
      "predict: 2758\n",
      "predict: 2759\n",
      "predict: 2760\n",
      "predict: 2761\n",
      "predict: 2762\n",
      "predict: 2763\n",
      "predict: 2764\n",
      "predict: 2765\n",
      "predict: 2766\n",
      "predict: 2767\n",
      "predict: 2768\n",
      "predict: 2769\n",
      "predict: 2770\n",
      "predict: 2771\n",
      "predict: 2772\n",
      "predict: 2773\n",
      "predict: 2774\n",
      "predict: 2775\n",
      "predict: 2776\n",
      "predict: 2777\n",
      "predict: 2778\n",
      "predict: 2779\n",
      "predict: 2780\n",
      "predict: 2781\n",
      "predict: 2782\n",
      "predict: 2783\n",
      "predict: 2784\n",
      "predict: 2785\n",
      "predict: 2786\n",
      "predict: 2787\n",
      "predict: 2788\n",
      "predict: 2789\n",
      "predict: 2790\n",
      "predict: 2791\n",
      "predict: 2792\n",
      "predict: 2793\n",
      "predict: 2794\n",
      "predict: 2795\n",
      "predict: 2796\n",
      "predict: 2797\n",
      "predict: 2798\n",
      "predict: 2799\n",
      "predict: 2800\n",
      "predict: 2801\n",
      "predict: 2802\n",
      "predict: 2803\n",
      "predict: 2804\n",
      "predict: 2805\n",
      "predict: 2806\n",
      "predict: 2807\n",
      "predict: 2808\n",
      "predict: 2809\n",
      "predict: 2810\n",
      "predict: 2811\n",
      "predict: 2812\n",
      "predict: 2813\n",
      "predict: 2814\n",
      "predict: 2815\n",
      "predict: 2816\n",
      "predict: 2817\n",
      "predict: 2818\n",
      "predict: 2819\n",
      "predict: 2820\n",
      "predict: 2821\n",
      "predict: 2822\n",
      "predict: 2823\n",
      "predict: 2824\n",
      "predict: 2825\n",
      "predict: 2826\n",
      "predict: 2827\n",
      "predict: 2828\n",
      "predict: 2829\n",
      "predict: 2830\n",
      "predict: 2831\n",
      "predict: 2832\n",
      "predict: 2833\n",
      "predict: 2834\n",
      "predict: 2835\n",
      "predict: 2836\n",
      "predict: 2837\n",
      "predict: 2838\n",
      "predict: 2839\n",
      "predict: 2840\n",
      "predict: 2841\n",
      "predict: 2842\n",
      "predict: 2843\n",
      "predict: 2844\n",
      "predict: 2845\n",
      "predict: 2846\n",
      "predict: 2847\n",
      "predict: 2848\n",
      "predict: 2849\n",
      "predict: 2850\n",
      "predict: 2851\n",
      "predict: 2852\n",
      "predict: 2853\n",
      "predict: 2854\n",
      "predict: 2855\n",
      "predict: 2856\n",
      "predict: 2857\n",
      "predict: 2858\n",
      "predict: 2859\n",
      "predict: 2860\n",
      "predict: 2861\n",
      "predict: 2862\n",
      "predict: 2863\n",
      "predict: 2864\n",
      "predict: 2865\n",
      "predict: 2866\n",
      "predict: 2867\n",
      "predict: 2868\n",
      "predict: 2869\n",
      "predict: 2870\n",
      "predict: 2871\n",
      "predict: 2872\n",
      "predict: 2873\n",
      "predict: 2874\n",
      "predict: 2875\n",
      "predict: 2876\n",
      "predict: 2877\n",
      "predict: 2878\n",
      "predict: 2879\n",
      "predict: 2880\n",
      "predict: 2881\n",
      "predict: 2882\n",
      "predict: 2883\n",
      "predict: 2884\n",
      "predict: 2885\n",
      "predict: 2886\n",
      "predict: 2887\n",
      "predict: 2888\n",
      "predict: 2889\n",
      "predict: 2890\n",
      "predict: 2891\n",
      "predict: 2892\n",
      "predict: 2893\n",
      "predict: 2894\n",
      "predict: 2895\n",
      "predict: 2896\n",
      "predict: 2897\n",
      "predict: 2898\n",
      "predict: 2899\n",
      "predict: 2900\n",
      "predict: 2901\n",
      "predict: 2902\n",
      "predict: 2903\n",
      "predict: 2904\n",
      "predict: 2905\n",
      "predict: 2906\n",
      "predict: 2907\n",
      "predict: 2908\n",
      "predict: 2909\n",
      "predict: 2910\n",
      "predict: 2911\n",
      "predict: 2912\n",
      "predict: 2913\n",
      "predict: 2914\n",
      "predict: 2915\n",
      "predict: 2916\n",
      "predict: 2917\n",
      "predict: 2918\n",
      "predict: 2919\n",
      "predict: 2920\n",
      "predict: 2921\n",
      "predict: 2922\n",
      "predict: 2923\n",
      "predict: 2924\n",
      "predict: 2925\n",
      "predict: 2926\n",
      "predict: 2927\n",
      "predict: 2928\n",
      "predict: 2929\n",
      "predict: 2930\n",
      "predict: 2931\n",
      "predict: 2932\n",
      "predict: 2933\n",
      "predict: 2934\n",
      "predict: 2935\n",
      "predict: 2936\n",
      "predict: 2937\n",
      "predict: 2938\n",
      "predict: 2939\n",
      "predict: 2940\n",
      "predict: 2941\n",
      "predict: 2942\n",
      "predict: 2943\n",
      "predict: 2944\n",
      "predict: 2945\n",
      "predict: 2946\n",
      "predict: 2947\n",
      "predict: 2948\n",
      "predict: 2949\n",
      "predict: 2950\n",
      "predict: 2951\n",
      "predict: 2952\n",
      "predict: 2953\n",
      "predict: 2954\n",
      "predict: 2955\n",
      "predict: 2956\n",
      "predict: 2957\n",
      "predict: 2958\n",
      "predict: 2959\n",
      "predict: 2960\n",
      "predict: 2961\n",
      "predict: 2962\n",
      "predict: 2963\n",
      "predict: 2964\n",
      "predict: 2965\n",
      "predict: 2966\n",
      "predict: 2967\n",
      "predict: 2968\n",
      "predict: 2969\n",
      "predict: 2970\n",
      "predict: 2971\n",
      "predict: 2972\n",
      "predict: 2973\n",
      "predict: 2974\n",
      "predict: 2975\n",
      "predict: 2976\n",
      "predict: 2977\n",
      "predict: 2978\n",
      "predict: 2979\n",
      "predict: 2980\n",
      "predict: 2981\n",
      "predict: 2982\n",
      "predict: 2983\n",
      "predict: 2984\n",
      "predict: 2985\n",
      "predict: 2986\n",
      "predict: 2987\n",
      "predict: 2988\n",
      "predict: 2989\n",
      "predict: 2990\n",
      "predict: 2991\n",
      "predict: 2992\n",
      "predict: 2993\n",
      "predict: 2994\n",
      "predict: 2995\n",
      "predict: 2996\n",
      "predict: 2997\n",
      "predict: 2998\n",
      "predict: 2999\n",
      "predict: 3000\n"
     ]
    }
   ],
   "source": [
    "X_Train_sdf[\"Grade\"] = train_grade_values\n",
    "ensamble_dframe = pd.DataFrame()\n",
    "for i in range(0,3001):\n",
    "    print(\"predict:\",i)\n",
    "    ssample = X_Train_sdf.sample(frac=0.65, replace=True)\n",
    "    sampleX = ssample.drop([\"Grade\"], axis=1)\n",
    "    sampleY = ssample[\"Grade\"]\n",
    "    #print(sampleX.shape)\n",
    "    bregr = Ridge(fit_intercept=True, normalize=True,max_iter=1500 ,alpha=0.3, tol=0.001)\n",
    "    bregr.fit(sampleX, sampleY)\n",
    "    \n",
    "    y_pred2 = bregr.predict(X_Test_sdf)\n",
    "    ensamble_dframe[i] = y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "# round up gives even better result\n",
    "ensamble_predict = np.ceil(ensamble_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>376</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>375</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>367</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>126</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>332</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>127</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>237</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>305</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>306</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>110</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>208</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>326</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>167</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>192</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>337</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>363</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>290</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>41</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>271</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>251</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>61</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>269</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>244</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>36</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>364</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>198</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>235</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>157</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>386</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>247</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>81</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>314</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>31</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>346</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>46</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>76</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>107</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>146</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>248</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>105</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>73</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>279</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>340</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>213</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>316</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>342</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Grade\n",
       "0    312   12.0\n",
       "1    164   12.0\n",
       "2    245   11.0\n",
       "3    303   11.0\n",
       "4    260   16.0\n",
       "5    376   11.0\n",
       "6     86    9.0\n",
       "7    375   10.0\n",
       "8    367   14.0\n",
       "9      6   13.0\n",
       "10    84   13.0\n",
       "11    35   11.0\n",
       "12   114   13.0\n",
       "13    69   10.0\n",
       "14   126   11.0\n",
       "15   332   10.0\n",
       "16   127   11.0\n",
       "17    77   14.0\n",
       "18   237   13.0\n",
       "19   305   10.0\n",
       "20   306   11.0\n",
       "21   110   14.0\n",
       "22   208   12.0\n",
       "23    21   14.0\n",
       "24    24   11.0\n",
       "25   326   13.0\n",
       "26   167   10.0\n",
       "27   192   12.0\n",
       "28   337   10.0\n",
       "29   363   12.0\n",
       "..   ...    ...\n",
       "167  290   13.0\n",
       "168   41    9.0\n",
       "169  271    7.0\n",
       "170  251    8.0\n",
       "171   61   12.0\n",
       "172  269   12.0\n",
       "173  244   13.0\n",
       "174   36   12.0\n",
       "175  364   11.0\n",
       "176  198    8.0\n",
       "177  235   13.0\n",
       "178  157   10.0\n",
       "179  386   10.0\n",
       "180  247   11.0\n",
       "181   81   11.0\n",
       "182  314    9.0\n",
       "183   31   16.0\n",
       "184  346   13.0\n",
       "185   46   10.0\n",
       "186   76   11.0\n",
       "187  107   11.0\n",
       "188  146   13.0\n",
       "189  248    6.0\n",
       "190  105   11.0\n",
       "191   73    4.0\n",
       "192  279   11.0\n",
       "193  340    9.0\n",
       "194  213    9.0\n",
       "195  316    8.0\n",
       "196  342   11.0\n",
       "\n",
       "[197 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df = pd.DataFrame({'id': test_ID_rows, 'Grade': ensamble_predict})\n",
    "submit_df = submit_df[['id','Grade']]\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "submit_df.to_csv(\"student_grade_prediction.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
