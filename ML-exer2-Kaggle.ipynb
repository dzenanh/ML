{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dzenan Hamzic BSc, TU Wien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data : 184.702 TU ML WS 18 - Student Performance (Kaggle)\n",
    "#### Group 15\n",
    "https://www.kaggle.com/c/184702-tu-ml-ws-18-student-performance/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statistics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Performance:\n",
    "    @staticmethod\n",
    "    def get_perf(y, y_pred):\n",
    "        ''' This method outputs several performance metrics for classification. '''\n",
    "\n",
    "        # ‘explained_variance’\n",
    "        explained_variance = metrics.explained_variance_score(y_true = y, y_pred = y_pred)\n",
    "\n",
    "        # R2\n",
    "        r2 = metrics.r2_score(y_true = y, y_pred = y_pred)\n",
    "\n",
    "        # ‘neg_mean_absolute_error’\n",
    "        mean_abs_err = metrics.mean_absolute_error(y_true = y, y_pred = y_pred)\n",
    "        \n",
    "        # ‘neg_mean_squared_error’\n",
    "        mean_sq_err = np.sqrt(metrics.mean_squared_error(y_true = y, y_pred = y_pred))\n",
    "        \n",
    "        return {'expl_variance': np.round(explained_variance, 6), 'R2': np.round(r2, 6),\n",
    "                'mean_abs_err': np.round(mean_abs_err,6), 'r_mean_sq_err': np.round(mean_sq_err,6)}\n",
    "   \n",
    "\n",
    "''' Run tests with default algorithm settings '''\n",
    "def run_tests(test_size, setting, X_train, X_test, y_train, y_test, log = False):\n",
    "    rstate = 1234\n",
    "    # algorithms\n",
    "    algo_names = [\"BaggingRegressor\",\n",
    "                   \"RandomForestRegressor\",\n",
    "                  \"BayesianRidge-Score\",\n",
    "                  \"GradientBoosting\",\n",
    "                  \"LinearRegression-NonNorm\",\n",
    "                  \"LinearRegression-Norm-NoInterc\",\n",
    "                  \"DecisionTreeRegressor-\",\n",
    "                  \"Ridge\",\n",
    "                  \"Ridge-Alpha001\",\n",
    "                  \"KNRegressor-distanceN5\",\n",
    "                  \"KNRegressor-distanceN50\",\n",
    "                 \"KNRegressor-uniformN5\"]\n",
    "    \n",
    "    algo_list = [BaggingRegressor(),\n",
    "                RandomForestRegressor(criterion='mse', oob_score=True),\n",
    "                BayesianRidge(compute_score=True),\n",
    "                 GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=5, random_state=1607, loss='ls'),\n",
    "                 LinearRegression(normalize=False),\n",
    "                 LinearRegression(normalize=True, fit_intercept=False),\n",
    "                tree.DecisionTreeRegressor(),\n",
    "                linear_model.Ridge(),\n",
    "                 linear_model.Ridge(alpha=.001),\n",
    "                neighbors.KNeighborsRegressor(5, weights=\"distance\"),\n",
    "                neighbors.KNeighborsRegressor(50, weights=\"distance\"),\n",
    "                neighbors.KNeighborsRegressor(5, weights=\"uniform\")]\n",
    "    \n",
    "    assert len(algo_names) == len(algo_list)\n",
    "    \n",
    "    # save prediction values\n",
    "    predictionDf = pd.DataFrame()\n",
    "    predictionDf[\"actual\"] = y_test.cumsum()\n",
    "    \n",
    "    performanceDf = pd.DataFrame()\n",
    "    for algo,name in zip(algo_list, algo_names):\n",
    "        if log: print(algo,setting)\n",
    "        algo_instance = algo\n",
    "\n",
    "        t0=time.time()\n",
    "        if log: print(name,\"...fit\")\n",
    "        algo.fit(X_train, y_train)\n",
    "        fit_time = round(time.time()-t0, 3)\n",
    "\n",
    "        ## And score it on your testing data.\n",
    "        if log: print(name,\"...score\")\n",
    "        score = algo.score(X_test, y_test)\n",
    "\n",
    "        ## prediction quality measures\n",
    "        t0=time.time()\n",
    "        if log: print(name,\"...predict\")\n",
    "        y_predict = algo.predict(X_test)\n",
    "        #y_predict = np.ceil(y_predict)\n",
    "        pred_time = round(time.time()-t0, 3)\n",
    "        \n",
    "        if log: \n",
    "            print(name,\"...performance\")\n",
    "            print(Performance.get_perf(y_test, y_predict))\n",
    "            \n",
    "        predictionDf[name] = y_predict.cumsum()    \n",
    "            \n",
    "        pp = Performance.get_perf(y_test, y_predict)\n",
    "        pp[\"Algorithm\"] = name\n",
    "        pp[\"Setting\"] = setting\n",
    "        pp[\"fit_time\"] = fit_time\n",
    "        pp[\"pred_time\"] = pred_time\n",
    "        pp[\"score\"] = score\n",
    "        pp[\"testSize\"] = test_size\n",
    "        pp[\"Xsize\"] = str(X_train.shape)\n",
    "        \n",
    "        pd2 = pd.DataFrame(pp, index=[0])\n",
    "        performanceDf = pd.concat([performanceDf ,pd2])\n",
    "        performanceDf.sort_values(by=['R2'], ascending=False, inplace=True)\n",
    "        \n",
    "        \n",
    "    return [performanceDf, predictionDf]\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/xxx/MScBI/S4/ML/exer2/data/all/StudentPerformance.shuf.train.csv', sep = ',')\n",
    "# shuffle\n",
    "df = df.sample(frac=1, random_state=1607).reset_index(drop=True)\n",
    "df_test = pd.read_csv('/home/xxx/MScBI/S4/ML/exer2/data/all/StudentPerformance.shuf.test.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Grade</th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>16</td>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>10</td>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>19</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Grade school sex  age address famsize Pstatus  Medu  Fedu   ...     \\\n",
       "0  360     16     MS   F   18       U     LE3       T     1     1   ...      \n",
       "1  394     10     MS   M   18       R     LE3       T     3     2   ...      \n",
       "2  287     19     GP   F   18       U     GT3       T     2     2   ...      \n",
       "3   20     10     GP   M   16       U     LE3       T     4     3   ...      \n",
       "4  317      0     GP   F   18       U     GT3       T     2     1   ...      \n",
       "\n",
       "  higher internet romantic famrel  freetime  goout  Dalc Walc health absences  \n",
       "0    yes      yes       no      5         3      2     1    1      4        0  \n",
       "1    yes      yes       no      4         4      1     3    4      5        0  \n",
       "2    yes      yes       no      4         3      3     1    2      2        5  \n",
       "3    yes      yes       no      3         1      3     1    3      5        4  \n",
       "4    yes      yes       no      5         3      3     1    2      1        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 31) (197, 31)\n",
      "(395, 31)\n"
     ]
    }
   ],
   "source": [
    "train_grade_values = df[\"Grade\"]\n",
    "df_copy = df.drop([\"Grade\"], axis=1)\n",
    "\n",
    "# align with test frame\n",
    "print(df_copy.shape, df_test.shape)\n",
    "dfb = pd.concat([df_copy,df_test], axis=0)\n",
    "print(dfb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_no</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0  360   18     1     1           2          3         0       5         3   \n",
       "1  394   18     3     2           3          1         0       4         4   \n",
       "2  287   18     2     2           1          3         0       4         3   \n",
       "3   20   16     4     3           1          1         0       3         1   \n",
       "4  317   18     2     1           2          2         0       5         3   \n",
       "\n",
       "   goout      ...       activities_no  activities_yes  nursery_no  \\\n",
       "0      2      ...                   1               0           0   \n",
       "1      1      ...                   1               0           1   \n",
       "2      3      ...                   1               0           0   \n",
       "3      3      ...                   0               1           0   \n",
       "4      3      ...                   0               1           0   \n",
       "\n",
       "   nursery_yes  higher_no  higher_yes  internet_no  internet_yes  romantic_no  \\\n",
       "0            1          0           1            0             1            1   \n",
       "1            0          0           1            0             1            1   \n",
       "2            1          0           1            0             1            1   \n",
       "3            1          0           1            0             1            1   \n",
       "4            1          0           1            0             1            1   \n",
       "\n",
       "   romantic_yes  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy = pd.get_dummies(dfb)\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 58) : (197, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## split df_dummy back to train and test\n",
    "df_dummy_train = df_dummy.iloc[0:198]\n",
    "df_dummy_test = df_dummy.iloc[198:395]\n",
    "# return y to train dummy df\n",
    "df_dummy_train[\"Grade\"] = train_grade_values\n",
    "print(df_dummy_train.shape,\":\",df_dummy_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0  360   18     1     1           2          3         0       5         3   \n",
       "1  394   18     3     2           3          1         0       4         4   \n",
       "2  287   18     2     2           1          3         0       4         3   \n",
       "3   20   16     4     3           1          1         0       3         1   \n",
       "4  317   18     2     1           2          2         0       5         3   \n",
       "\n",
       "   goout  ...    activities_yes  nursery_no  nursery_yes  higher_no  \\\n",
       "0      2  ...                 0           0            1          0   \n",
       "1      1  ...                 0           1            0          0   \n",
       "2      3  ...                 0           0            1          0   \n",
       "3      3  ...                 1           0            1          0   \n",
       "4      3  ...                 1           0            1          0   \n",
       "\n",
       "   higher_yes  internet_no  internet_yes  romantic_no  romantic_yes  Grade  \n",
       "0           1            0             1            1             0     16  \n",
       "1           1            0             1            1             0     10  \n",
       "2           1            0             1            1             0     19  \n",
       "3           1            0             1            1             0     10  \n",
       "4           1            0             1            1             0      0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save dummified to scv for R stepwise algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy_train.to_csv(\"/home/xxx/MScBI/S4/ML/exer2/data/all/oversampled_student_data_train.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 58)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split to seen and unseen \n",
    "X_test_unseen = df_dummy_train.iloc[148:198]\n",
    "y_test_unseen = X_test_unseen[\"Grade\"]\n",
    "\n",
    "X_train_seen = df_dummy_train.iloc[0:148]\n",
    "y_train_seen = X_train_seen[\"Grade\"]\n",
    "\n",
    "X_train_seen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Predictor variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X_train_seen[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X_test_unseen[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n",
      "4 1\n",
      "5 1\n",
      "6 7\n",
      "7 5\n",
      "8 11\n",
      "9 11\n",
      "10 21\n",
      "11 19\n",
      "12 12\n",
      "13 7\n",
      "14 11\n",
      "15 14\n",
      "16 7\n",
      "17 1\n",
      "18 3\n",
      "19 2\n"
     ]
    }
   ],
   "source": [
    "number_grades = []\n",
    "for g in list(set(X_train_seen['Grade'])):\n",
    "    number_grades.insert(0,len(X_train_seen.loc[df['Grade'] == g]))\n",
    "    print(g, len(X_train_seen.loc[X_train_seen['Grade'] == g]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "most_frequent = 10\n",
    "most_frequent_len = len(X_train_seen.loc[X_train_seen['Grade'] == most_frequent])\n",
    "print(most_frequent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 58)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df = X_train_seen.loc[X_train_seen['Grade'] == most_frequent]\n",
    "for grade in set(X_train_seen['Grade']):\n",
    "    if grade != most_frequent:\n",
    "        #print(grade,len(X_train_seen.loc[X_train_seen['Grade'] == grade]))\n",
    "        subdf = X_train_seen.loc[X_train_seen['Grade'] == grade].sample(n=most_frequent_len, replace=True)\n",
    "        oversampled_df = oversampled_df.append(subdf)\n",
    "        \n",
    "# shuffle\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=1607).reset_index(drop=True)\n",
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21\n",
      "4 21\n",
      "5 21\n",
      "6 21\n",
      "7 21\n",
      "8 21\n",
      "9 21\n",
      "10 21\n",
      "11 21\n",
      "12 21\n",
      "13 21\n",
      "14 21\n",
      "15 21\n",
      "16 21\n",
      "17 21\n",
      "18 21\n",
      "19 21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for grade in set(oversampled_df['Grade']):\n",
    "    print(grade,len(oversampled_df.loc[oversampled_df['Grade'] == grade]))\n",
    "    \n",
    "sns.distplot(oversampled_df[\"Grade\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 58)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save oversampled to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>387</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>234</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>339</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>217</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "0  387   18     4     4           3          1         0       4         4   \n",
       "1  307   20     3     2           1          1         0       5         5   \n",
       "2  234   16     4     4           1          2         0       4         2   \n",
       "3  339   18     3     3           1          4         0       5         3   \n",
       "4  217   17     4     3           1          2         2       3         4   \n",
       "\n",
       "   goout  ...    activities_yes  nursery_no  nursery_yes  higher_no  \\\n",
       "0      3  ...                 1           0            1          0   \n",
       "1      3  ...                 1           0            1          0   \n",
       "2      4  ...                 1           0            1          0   \n",
       "3      3  ...                 0           0            1          0   \n",
       "4      5  ...                 0           0            1          0   \n",
       "\n",
       "   higher_yes  internet_no  internet_yes  romantic_no  romantic_yes  Grade  \n",
       "0           1            0             1            0             1      6  \n",
       "1           1            1             0            1             0     18  \n",
       "2           1            0             1            1             0     13  \n",
       "3           1            0             1            1             0     17  \n",
       "4           1            0             1            0             1      4  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampled_df[\"Grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# df = balanced_df\n",
    "y_train = oversampled_df[\"Grade\"]\n",
    "oversampled_df.drop([\"Grade\",\"id\"], axis=1, inplace=True)\n",
    "X_test_unseen.drop([\"Grade\",\"id\"], axis=1, inplace=True)\n",
    "X_train = oversampled_df\n",
    "X_test = X_test_unseen\n",
    "y_test = y_test_unseen\n",
    "test_ID_rows = df_test[\"id\"]\n",
    "df_test.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 56) (357,) (50, 56) (50,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "Medu                 0\n",
       "Fedu                 0\n",
       "traveltime           0\n",
       "studytime            0\n",
       "failures             0\n",
       "famrel               0\n",
       "freetime             0\n",
       "goout                0\n",
       "Dalc                 0\n",
       "Walc                 0\n",
       "health               0\n",
       "absences             0\n",
       "school_GP            0\n",
       "school_MS            0\n",
       "sex_F                0\n",
       "sex_M                0\n",
       "address_R            0\n",
       "address_U            0\n",
       "famsize_GT3          0\n",
       "famsize_LE3          0\n",
       "Pstatus_A            0\n",
       "Pstatus_T            0\n",
       "Mjob_at_home         0\n",
       "Mjob_health          0\n",
       "Mjob_other           0\n",
       "Mjob_services        0\n",
       "Mjob_teacher         0\n",
       "Fjob_at_home         0\n",
       "Fjob_health          0\n",
       "Fjob_other           0\n",
       "Fjob_services        0\n",
       "Fjob_teacher         0\n",
       "reason_course        0\n",
       "reason_home          0\n",
       "reason_other         0\n",
       "reason_reputation    0\n",
       "guardian_father      0\n",
       "guardian_mother      0\n",
       "guardian_other       0\n",
       "schoolsup_no         0\n",
       "schoolsup_yes        0\n",
       "famsup_no            0\n",
       "famsup_yes           0\n",
       "paid_no              0\n",
       "paid_yes             0\n",
       "activities_no        0\n",
       "activities_yes       0\n",
       "nursery_no           0\n",
       "nursery_yes          0\n",
       "higher_no            0\n",
       "higher_yes           0\n",
       "internet_no          0\n",
       "internet_yes         0\n",
       "romantic_no          0\n",
       "romantic_yes         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Regression summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Grade</td>      <th>  R-squared:         </th> <td>   0.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   16.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 07 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>2.51e-55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:43:22</td>     <th>  Log-Likelihood:    </th> <td> -899.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   357</td>      <th>  AIC:               </th> <td>   1879.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   317</td>      <th>  BIC:               </th> <td>   2034.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>               <td>   -0.2650</td> <td>    0.220</td> <td>   -1.203</td> <td> 0.230</td> <td>   -0.699</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu</th>              <td>    0.3486</td> <td>    0.361</td> <td>    0.966</td> <td> 0.335</td> <td>   -0.361</td> <td>    1.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu</th>              <td>   -0.7212</td> <td>    0.270</td> <td>   -2.673</td> <td> 0.008</td> <td>   -1.252</td> <td>   -0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime</th>        <td>   -0.2343</td> <td>    0.348</td> <td>   -0.674</td> <td> 0.501</td> <td>   -0.918</td> <td>    0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime</th>         <td>    1.3552</td> <td>    0.266</td> <td>    5.099</td> <td> 0.000</td> <td>    0.832</td> <td>    1.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>failures</th>          <td>   -1.8196</td> <td>    0.282</td> <td>   -6.443</td> <td> 0.000</td> <td>   -2.375</td> <td>   -1.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel</th>            <td>    0.3504</td> <td>    0.249</td> <td>    1.406</td> <td> 0.161</td> <td>   -0.140</td> <td>    0.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime</th>          <td>    0.2811</td> <td>    0.264</td> <td>    1.066</td> <td> 0.287</td> <td>   -0.238</td> <td>    0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout</th>             <td>   -0.9504</td> <td>    0.275</td> <td>   -3.458</td> <td> 0.001</td> <td>   -1.491</td> <td>   -0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dalc</th>              <td>   -1.1650</td> <td>    0.364</td> <td>   -3.203</td> <td> 0.001</td> <td>   -1.881</td> <td>   -0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Walc</th>              <td>    0.5215</td> <td>    0.250</td> <td>    2.085</td> <td> 0.038</td> <td>    0.029</td> <td>    1.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health</th>            <td>   -0.5529</td> <td>    0.167</td> <td>   -3.310</td> <td> 0.001</td> <td>   -0.882</td> <td>   -0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>          <td>    0.0026</td> <td>    0.030</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.057</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school_GP</th>         <td>    0.9850</td> <td>    0.410</td> <td>    2.403</td> <td> 0.017</td> <td>    0.179</td> <td>    1.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school_MS</th>         <td>    1.2371</td> <td>    0.569</td> <td>    2.175</td> <td> 0.030</td> <td>    0.118</td> <td>    2.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_F</th>             <td>   -0.0080</td> <td>    0.410</td> <td>   -0.020</td> <td> 0.984</td> <td>   -0.815</td> <td>    0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_M</th>             <td>    2.2302</td> <td>    0.362</td> <td>    6.167</td> <td> 0.000</td> <td>    1.519</td> <td>    2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address_R</th>         <td>    0.1964</td> <td>    0.492</td> <td>    0.399</td> <td> 0.690</td> <td>   -0.771</td> <td>    1.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address_U</th>         <td>    2.0257</td> <td>    0.369</td> <td>    5.487</td> <td> 0.000</td> <td>    1.299</td> <td>    2.752</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize_GT3</th>       <td>    1.0287</td> <td>    0.410</td> <td>    2.509</td> <td> 0.013</td> <td>    0.222</td> <td>    1.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize_LE3</th>       <td>    1.1934</td> <td>    0.354</td> <td>    3.367</td> <td> 0.001</td> <td>    0.496</td> <td>    1.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus_A</th>         <td>    1.8350</td> <td>    0.451</td> <td>    4.065</td> <td> 0.000</td> <td>    0.947</td> <td>    2.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus_T</th>         <td>    0.3871</td> <td>    0.488</td> <td>    0.793</td> <td> 0.429</td> <td>   -0.574</td> <td>    1.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_at_home</th>      <td>   -0.4595</td> <td>    0.629</td> <td>   -0.730</td> <td> 0.466</td> <td>   -1.697</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_health</th>       <td>    1.8065</td> <td>    0.805</td> <td>    2.243</td> <td> 0.026</td> <td>    0.222</td> <td>    3.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_other</th>        <td>    0.2672</td> <td>    0.535</td> <td>    0.499</td> <td> 0.618</td> <td>   -0.786</td> <td>    1.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_services</th>     <td>   -0.3102</td> <td>    0.493</td> <td>   -0.629</td> <td> 0.530</td> <td>   -1.280</td> <td>    0.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob_teacher</th>      <td>    0.9181</td> <td>    0.716</td> <td>    1.283</td> <td> 0.200</td> <td>   -0.490</td> <td>    2.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_at_home</th>      <td>    1.3136</td> <td>    0.839</td> <td>    1.566</td> <td> 0.118</td> <td>   -0.336</td> <td>    2.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_health</th>       <td>   -0.5260</td> <td>    0.812</td> <td>   -0.648</td> <td> 0.517</td> <td>   -2.123</td> <td>    1.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_other</th>        <td>   -1.2327</td> <td>    0.482</td> <td>   -2.555</td> <td> 0.011</td> <td>   -2.182</td> <td>   -0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_services</th>     <td>    0.1899</td> <td>    0.512</td> <td>    0.371</td> <td> 0.711</td> <td>   -0.818</td> <td>    1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob_teacher</th>      <td>    2.4774</td> <td>    0.887</td> <td>    2.794</td> <td> 0.006</td> <td>    0.733</td> <td>    4.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_course</th>     <td>    1.4260</td> <td>    0.432</td> <td>    3.304</td> <td> 0.001</td> <td>    0.577</td> <td>    2.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_home</th>       <td>    0.4173</td> <td>    0.442</td> <td>    0.945</td> <td> 0.345</td> <td>   -0.452</td> <td>    1.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_other</th>      <td>    1.9091</td> <td>    0.654</td> <td>    2.917</td> <td> 0.004</td> <td>    0.621</td> <td>    3.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_reputation</th> <td>   -1.5303</td> <td>    0.430</td> <td>   -3.559</td> <td> 0.000</td> <td>   -2.376</td> <td>   -0.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_father</th>   <td>    0.3815</td> <td>    0.520</td> <td>    0.733</td> <td> 0.464</td> <td>   -0.642</td> <td>    1.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_mother</th>   <td>   -0.4491</td> <td>    0.372</td> <td>   -1.207</td> <td> 0.228</td> <td>   -1.181</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian_other</th>    <td>    2.2898</td> <td>    0.778</td> <td>    2.942</td> <td> 0.003</td> <td>    0.759</td> <td>    3.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup_no</th>      <td>    3.0818</td> <td>    0.461</td> <td>    6.678</td> <td> 0.000</td> <td>    2.174</td> <td>    3.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup_yes</th>     <td>   -0.8597</td> <td>    0.482</td> <td>   -1.783</td> <td> 0.076</td> <td>   -1.808</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup_no</th>         <td>    1.3096</td> <td>    0.403</td> <td>    3.247</td> <td> 0.001</td> <td>    0.516</td> <td>    2.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup_yes</th>        <td>    0.9125</td> <td>    0.342</td> <td>    2.670</td> <td> 0.008</td> <td>    0.240</td> <td>    1.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid_no</th>           <td>    0.8913</td> <td>    0.397</td> <td>    2.244</td> <td> 0.026</td> <td>    0.110</td> <td>    1.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid_yes</th>          <td>    1.3308</td> <td>    0.374</td> <td>    3.561</td> <td> 0.000</td> <td>    0.596</td> <td>    2.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities_no</th>     <td>    1.1294</td> <td>    0.368</td> <td>    3.069</td> <td> 0.002</td> <td>    0.405</td> <td>    1.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities_yes</th>    <td>    1.0927</td> <td>    0.379</td> <td>    2.884</td> <td> 0.004</td> <td>    0.347</td> <td>    1.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery_no</th>        <td>   -0.1661</td> <td>    0.401</td> <td>   -0.414</td> <td> 0.679</td> <td>   -0.955</td> <td>    0.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery_yes</th>       <td>    2.3882</td> <td>    0.473</td> <td>    5.045</td> <td> 0.000</td> <td>    1.457</td> <td>    3.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher_no</th>         <td>    0.5901</td> <td>    0.671</td> <td>    0.880</td> <td> 0.379</td> <td>   -0.729</td> <td>    1.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher_yes</th>        <td>    1.6320</td> <td>    0.584</td> <td>    2.793</td> <td> 0.006</td> <td>    0.482</td> <td>    2.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet_no</th>       <td>    0.9075</td> <td>    0.445</td> <td>    2.041</td> <td> 0.042</td> <td>    0.033</td> <td>    1.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet_yes</th>      <td>    1.3146</td> <td>    0.393</td> <td>    3.347</td> <td> 0.001</td> <td>    0.542</td> <td>    2.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic_no</th>       <td>    1.8251</td> <td>    0.389</td> <td>    4.687</td> <td> 0.000</td> <td>    1.059</td> <td>    2.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic_yes</th>      <td>    0.3970</td> <td>    0.424</td> <td>    0.936</td> <td> 0.350</td> <td>   -0.437</td> <td>    1.231</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>53.489</td> <th>  Durbin-Watson:     </th> <td>   2.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 115.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.781</td> <th>  Prob(JB):          </th> <td>7.90e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.310</td> <th>  Cond. No.          </th> <td>1.24e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is  1e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Grade   R-squared:                       0.669\n",
       "Model:                            OLS   Adj. R-squared:                  0.628\n",
       "Method:                 Least Squares   F-statistic:                     16.44\n",
       "Date:                Mon, 07 Jan 2019   Prob (F-statistic):           2.51e-55\n",
       "Time:                        21:43:22   Log-Likelihood:                -899.57\n",
       "No. Observations:                 357   AIC:                             1879.\n",
       "Df Residuals:                     317   BIC:                             2034.\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "age                  -0.2650      0.220     -1.203      0.230      -0.699       0.168\n",
       "Medu                  0.3486      0.361      0.966      0.335      -0.361       1.059\n",
       "Fedu                 -0.7212      0.270     -2.673      0.008      -1.252      -0.190\n",
       "traveltime           -0.2343      0.348     -0.674      0.501      -0.918       0.450\n",
       "studytime             1.3552      0.266      5.099      0.000       0.832       1.878\n",
       "failures             -1.8196      0.282     -6.443      0.000      -2.375      -1.264\n",
       "famrel                0.3504      0.249      1.406      0.161      -0.140       0.841\n",
       "freetime              0.2811      0.264      1.066      0.287      -0.238       0.800\n",
       "goout                -0.9504      0.275     -3.458      0.001      -1.491      -0.410\n",
       "Dalc                 -1.1650      0.364     -3.203      0.001      -1.881      -0.449\n",
       "Walc                  0.5215      0.250      2.085      0.038       0.029       1.014\n",
       "health               -0.5529      0.167     -3.310      0.001      -0.882      -0.224\n",
       "absences              0.0026      0.030      0.084      0.933      -0.057       0.063\n",
       "school_GP             0.9850      0.410      2.403      0.017       0.179       1.791\n",
       "school_MS             1.2371      0.569      2.175      0.030       0.118       2.356\n",
       "sex_F                -0.0080      0.410     -0.020      0.984      -0.815       0.799\n",
       "sex_M                 2.2302      0.362      6.167      0.000       1.519       2.942\n",
       "address_R             0.1964      0.492      0.399      0.690      -0.771       1.164\n",
       "address_U             2.0257      0.369      5.487      0.000       1.299       2.752\n",
       "famsize_GT3           1.0287      0.410      2.509      0.013       0.222       1.836\n",
       "famsize_LE3           1.1934      0.354      3.367      0.001       0.496       1.891\n",
       "Pstatus_A             1.8350      0.451      4.065      0.000       0.947       2.723\n",
       "Pstatus_T             0.3871      0.488      0.793      0.429      -0.574       1.348\n",
       "Mjob_at_home         -0.4595      0.629     -0.730      0.466      -1.697       0.778\n",
       "Mjob_health           1.8065      0.805      2.243      0.026       0.222       3.391\n",
       "Mjob_other            0.2672      0.535      0.499      0.618      -0.786       1.320\n",
       "Mjob_services        -0.3102      0.493     -0.629      0.530      -1.280       0.660\n",
       "Mjob_teacher          0.9181      0.716      1.283      0.200      -0.490       2.326\n",
       "Fjob_at_home          1.3136      0.839      1.566      0.118      -0.336       2.963\n",
       "Fjob_health          -0.5260      0.812     -0.648      0.517      -2.123       1.071\n",
       "Fjob_other           -1.2327      0.482     -2.555      0.011      -2.182      -0.284\n",
       "Fjob_services         0.1899      0.512      0.371      0.711      -0.818       1.198\n",
       "Fjob_teacher          2.4774      0.887      2.794      0.006       0.733       4.222\n",
       "reason_course         1.4260      0.432      3.304      0.001       0.577       2.275\n",
       "reason_home           0.4173      0.442      0.945      0.345      -0.452       1.286\n",
       "reason_other          1.9091      0.654      2.917      0.004       0.621       3.197\n",
       "reason_reputation    -1.5303      0.430     -3.559      0.000      -2.376      -0.684\n",
       "guardian_father       0.3815      0.520      0.733      0.464      -0.642       1.405\n",
       "guardian_mother      -0.4491      0.372     -1.207      0.228      -1.181       0.283\n",
       "guardian_other        2.2898      0.778      2.942      0.003       0.759       3.821\n",
       "schoolsup_no          3.0818      0.461      6.678      0.000       2.174       3.990\n",
       "schoolsup_yes        -0.8597      0.482     -1.783      0.076      -1.808       0.089\n",
       "famsup_no             1.3096      0.403      3.247      0.001       0.516       2.103\n",
       "famsup_yes            0.9125      0.342      2.670      0.008       0.240       1.585\n",
       "paid_no               0.8913      0.397      2.244      0.026       0.110       1.673\n",
       "paid_yes              1.3308      0.374      3.561      0.000       0.596       2.066\n",
       "activities_no         1.1294      0.368      3.069      0.002       0.405       1.853\n",
       "activities_yes        1.0927      0.379      2.884      0.004       0.347       1.838\n",
       "nursery_no           -0.1661      0.401     -0.414      0.679      -0.955       0.623\n",
       "nursery_yes           2.3882      0.473      5.045      0.000       1.457       3.320\n",
       "higher_no             0.5901      0.671      0.880      0.379      -0.729       1.909\n",
       "higher_yes            1.6320      0.584      2.793      0.006       0.482       2.782\n",
       "internet_no           0.9075      0.445      2.041      0.042       0.033       1.782\n",
       "internet_yes          1.3146      0.393      3.347      0.001       0.542       2.087\n",
       "romantic_no           1.8251      0.389      4.687      0.000       1.059       2.591\n",
       "romantic_yes          0.3970      0.424      0.936      0.350      -0.437       1.231\n",
       "==============================================================================\n",
       "Omnibus:                       53.489   Durbin-Watson:                   2.047\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              115.602\n",
       "Skew:                          -0.781   Prob(JB):                     7.90e-26\n",
       "Kurtosis:                       5.310   Cond. No.                     1.24e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is  1e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = sm.add_constant(X)\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute for no oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 56) (148,) (50, 56) (50,)\n"
     ]
    }
   ],
   "source": [
    "# no oversampling\n",
    "y_train = y_train_seen\n",
    "X_train = X_train_seen.drop([\"Grade\",\"id\"], axis=1)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented above\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=1607)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1:\n",
    " - test data is 25% of the complete dataframe\n",
    " - X has dimension of (148, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.323225</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.323618</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.255712</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.973012</td>\n",
       "      <td>0.323225</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.323012</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.323061</td>\n",
       "      <td>0.034</td>\n",
       "      <td>2.982000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.973638</td>\n",
       "      <td>0.323012</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.321786</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.322149</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.236000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.977235</td>\n",
       "      <td>0.321786</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.311805</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.312163</td>\n",
       "      <td>0.357</td>\n",
       "      <td>2.938091</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.006393</td>\n",
       "      <td>0.311805</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.311573</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.312254</td>\n",
       "      <td>0.044</td>\n",
       "      <td>3.066000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.007069</td>\n",
       "      <td>0.311573</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.227120</td>\n",
       "      <td>0.076</td>\n",
       "      <td>3.345408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.248402</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>0.010</td>\n",
       "      <td>3.536831</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.309439</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199069</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.563308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334599</td>\n",
       "      <td>0.194432</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199059</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.563337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334627</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.199059</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.563337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.334627</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.157112</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>0.158725</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.390357</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.433867</td>\n",
       "      <td>0.157112</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.091598</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 56)</td>\n",
       "      <td>-0.046991</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.045790</td>\n",
       "      <td>-0.091598</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2   Setting      Xsize  \\\n",
       "0          KNRegressor-distanceN5  0.323225  raw_data  (148, 56)   \n",
       "0           RandomForestRegressor  0.323012  raw_data  (148, 56)   \n",
       "0           KNRegressor-uniformN5  0.321786  raw_data  (148, 56)   \n",
       "0                GradientBoosting  0.311805  raw_data  (148, 56)   \n",
       "0                BaggingRegressor  0.311573  raw_data  (148, 56)   \n",
       "0             BayesianRidge-Score  0.226152  raw_data  (148, 56)   \n",
       "0                           Ridge  0.203756  raw_data  (148, 56)   \n",
       "0                  Ridge-Alpha001  0.194432  raw_data  (148, 56)   \n",
       "0        LinearRegression-NonNorm  0.194421  raw_data  (148, 56)   \n",
       "0  LinearRegression-Norm-NoInterc  0.194421  raw_data  (148, 56)   \n",
       "0         KNRegressor-distanceN50  0.157112  raw_data  (148, 56)   \n",
       "0          DecisionTreeRegressor- -0.091598  raw_data  (148, 56)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.323618     0.001      3.255712      0.001       3.973012  0.323225   \n",
       "0       0.323061     0.034      2.982000      0.002       3.973638  0.323012   \n",
       "0       0.322149     0.001      3.236000      0.001       3.977235  0.321786   \n",
       "0       0.312163     0.357      2.938091      0.001       4.006393  0.311805   \n",
       "0       0.312254     0.044      3.066000      0.002       4.007069  0.311573   \n",
       "0       0.227120     0.076      3.345408      0.000       4.248402  0.226152   \n",
       "0       0.208310     0.010      3.536831      0.000       4.309439  0.203756   \n",
       "0       0.199069     0.001      3.563308      0.000       4.334599  0.194432   \n",
       "0       0.199059     0.002      3.563337      0.000       4.334627  0.194421   \n",
       "0       0.199059     0.001      3.563337      0.000       4.334627  0.194421   \n",
       "0       0.158725     0.001      3.390357      0.002       4.433867  0.157112   \n",
       "0      -0.046991     0.001      3.500000      0.000       5.045790 -0.091598   \n",
       "\n",
       "   testSize  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdfs = run_tests(0.25, \"raw_data\", X_train, X_test, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2: regression tests with squared polynomial transformed data\n",
    " - test data is 25% of the complete dataframe\n",
    " - X has dimension of (148, 1653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.373514</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.373771</td>\n",
       "      <td>3.830</td>\n",
       "      <td>2.949451</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.822551</td>\n",
       "      <td>0.373514</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.298676</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.298767</td>\n",
       "      <td>0.197</td>\n",
       "      <td>3.106000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>4.044428</td>\n",
       "      <td>0.298676</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.229179</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.229205</td>\n",
       "      <td>0.199</td>\n",
       "      <td>3.320407</td>\n",
       "      <td>0.014</td>\n",
       "      <td>4.240085</td>\n",
       "      <td>0.229179</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.186622</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.348000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>4.362843</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.179482</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.182555</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.379868</td>\n",
       "      <td>0.023</td>\n",
       "      <td>4.374636</td>\n",
       "      <td>0.179482</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>0.144042</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.378447</td>\n",
       "      <td>0.019</td>\n",
       "      <td>4.478165</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-0.003802</td>\n",
       "      <td>0.167</td>\n",
       "      <td>3.738000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.849062</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.519491</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-0.513934</td>\n",
       "      <td>0.041</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.953150</td>\n",
       "      <td>-0.519491</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>-2.224096</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.189584</td>\n",
       "      <td>0.004</td>\n",
       "      <td>5.981832</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.671650</td>\n",
       "      <td>-2.224096</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>-2.269733</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.233243</td>\n",
       "      <td>0.013</td>\n",
       "      <td>6.015415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.732809</td>\n",
       "      <td>-2.269733</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>-2.270315</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.233819</td>\n",
       "      <td>0.004</td>\n",
       "      <td>6.015812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.733585</td>\n",
       "      <td>-2.270315</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-2.293503</td>\n",
       "      <td>poly^2</td>\n",
       "      <td>(148, 1653)</td>\n",
       "      <td>-2.259092</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6.012510</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.764493</td>\n",
       "      <td>-2.293503</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2 Setting        Xsize  \\\n",
       "0                GradientBoosting  0.373514  poly^2  (148, 1653)   \n",
       "0                BaggingRegressor  0.298676  poly^2  (148, 1653)   \n",
       "0             BayesianRidge-Score  0.229179  poly^2  (148, 1653)   \n",
       "0           KNRegressor-uniformN5  0.183900  poly^2  (148, 1653)   \n",
       "0          KNRegressor-distanceN5  0.179482  poly^2  (148, 1653)   \n",
       "0         KNRegressor-distanceN50  0.140186  poly^2  (148, 1653)   \n",
       "0           RandomForestRegressor -0.008138  poly^2  (148, 1653)   \n",
       "0          DecisionTreeRegressor- -0.519491  poly^2  (148, 1653)   \n",
       "0                           Ridge -2.224096  poly^2  (148, 1653)   \n",
       "0        LinearRegression-NonNorm -2.269733  poly^2  (148, 1653)   \n",
       "0                  Ridge-Alpha001 -2.270315  poly^2  (148, 1653)   \n",
       "0  LinearRegression-Norm-NoInterc -2.293503  poly^2  (148, 1653)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.373771     3.830      2.949451      0.001       3.822551  0.373514   \n",
       "0       0.298767     0.197      3.106000      0.003       4.044428  0.298676   \n",
       "0       0.229205     0.199      3.320407      0.014       4.240085  0.229179   \n",
       "0       0.186622     0.002      3.348000      0.016       4.362843  0.183900   \n",
       "0       0.182555     0.004      3.379868      0.023       4.374636  0.179482   \n",
       "0       0.144042     0.004      3.378447      0.019       4.478165  0.140186   \n",
       "0      -0.003802     0.167      3.738000      0.001       4.849062 -0.008138   \n",
       "0      -0.513934     0.041      4.560000      0.000       5.953150 -0.519491   \n",
       "0      -2.189584     0.004      5.981832      0.000       8.671650 -2.224096   \n",
       "0      -2.233243     0.013      6.015415      0.000       8.732809 -2.269733   \n",
       "0      -2.233819     0.004      6.015812      0.000       8.733585 -2.270315   \n",
       "0      -2.259092     0.020      6.012510      0.000       8.764493 -2.293503   \n",
       "\n",
       "   testSize  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  \n",
       "0      0.25  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly2 = poly.fit_transform(X_train)\n",
    "X_test_poly2 = poly.transform(X_test)\n",
    "\n",
    "resultdfs = run_tests(0.25, \"poly^\"+str(degree), X_train_poly2, X_test_poly2, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor - best params:  {'loss': 'ls', 'learning_rate': 0.1, 'random_state': 1607, 'max_features': 2, 'subsample': 1.0, 'max_depth': 5, 'n_estimators': 5000}\n"
     ]
    }
   ],
   "source": [
    "param_grid_bayesian_ridge = {\n",
    "    \"n_estimators\":[5000],\n",
    "    \"loss\":[\"ls\"],\n",
    "    \"subsample\": [1.0],\n",
    "    \"max_features\":[2,5,None],\n",
    "    \"learning_rate\":[0.1,0.2],\n",
    "    \"max_depth\":[2,5,6,7],\n",
    "    \"random_state\":[1607]\n",
    "}\n",
    "\n",
    "mod = GradientBoostingRegressor()\n",
    "CV_dtc = GridSearchCV(estimator=mod, param_grid=param_grid_bayesian_ridge, cv = 5, n_jobs=-1)\n",
    "CV_dtc.fit(X_train_poly2, y_train)\n",
    "rr_best_params = CV_dtc.best_params_\n",
    "print(\"GradientBoostingRegressor - best params: \",rr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.9999999952170383\n",
      "RR Mean absolute error: 2.94945\n",
      "RR Mean squared error: 3.82255\n",
      "RR Variance score: 0.37\n"
     ]
    }
   ],
   "source": [
    "if \"Grade\" in X_train_poly2:\n",
    "    X_train_poly2.drop([\"Grade\"], axis=1, inplace=True)\n",
    "\n",
    "# bayesian regression\n",
    "bregr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=5, random_state=1607, loss='ls')\n",
    "bregr.fit(X_train_poly2, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test_poly2)\n",
    "\n",
    "\n",
    "#print('RR Coefficients: \\n', bregr.coef_)\n",
    "print(\"R2\", bregr.score(X_train_poly2, y_train))\n",
    "print(\"RR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"RR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('RR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3: regression tests with Principal Components\n",
    " - test data is 33% of the complete dataframe\n",
    " - X has dimension of (148, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09874854 0.06221322 0.05948353 0.05483858 0.04929281]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.554436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514577</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.554436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514577</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.554474</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.514899</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.094131</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.094294</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3.563466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.596533</td>\n",
       "      <td>0.094131</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.094996</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.471908</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.604059</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>-0.023307</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.021306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.744000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.885407</td>\n",
       "      <td>-0.023307</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.030241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.717268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.903641</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.109187</td>\n",
       "      <td>0.236</td>\n",
       "      <td>3.729957</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.086649</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.152970</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.152005</td>\n",
       "      <td>0.022</td>\n",
       "      <td>3.918000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.185692</td>\n",
       "      <td>-0.152970</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.263973</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.262392</td>\n",
       "      <td>0.022</td>\n",
       "      <td>4.156000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.429586</td>\n",
       "      <td>-0.263973</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.780171</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>-0.762610</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.443601</td>\n",
       "      <td>-0.780171</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-4.351696</td>\n",
       "      <td>PCA10</td>\n",
       "      <td>(148, 5)</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.219561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.172324</td>\n",
       "      <td>-4.351696</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2 Setting     Xsize  expl_variance  \\\n",
       "0        LinearRegression-NonNorm  0.126147   PCA10  (148, 5)       0.126147   \n",
       "0                  Ridge-Alpha001  0.126147   PCA10  (148, 5)       0.126147   \n",
       "0                           Ridge  0.126022   PCA10  (148, 5)       0.126022   \n",
       "0             BayesianRidge-Score  0.094131   PCA10  (148, 5)       0.094294   \n",
       "0         KNRegressor-distanceN50  0.091163   PCA10  (148, 5)       0.094996   \n",
       "0           KNRegressor-uniformN5 -0.023307   PCA10  (148, 5)      -0.021306   \n",
       "0          KNRegressor-distanceN5 -0.030960   PCA10  (148, 5)      -0.030241   \n",
       "0                GradientBoosting -0.109349   PCA10  (148, 5)      -0.109187   \n",
       "0                BaggingRegressor -0.152970   PCA10  (148, 5)      -0.152005   \n",
       "0           RandomForestRegressor -0.263973   PCA10  (148, 5)      -0.262392   \n",
       "0          DecisionTreeRegressor- -0.780171   PCA10  (148, 5)      -0.762610   \n",
       "0  LinearRegression-Norm-NoInterc -4.351696   PCA10  (148, 5)       0.126147   \n",
       "\n",
       "   fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  testSize  \n",
       "0     0.001      3.554436      0.000       4.514577  0.126147       0.7  \n",
       "0     0.000      3.554436      0.000       4.514577  0.126147       0.7  \n",
       "0     0.001      3.554474      0.000       4.514899  0.126022       0.7  \n",
       "0     0.002      3.563466      0.000       4.596533  0.094131       0.7  \n",
       "0     0.000      3.471908      0.001       4.604059  0.091163       0.7  \n",
       "0     0.000      3.744000      0.000       4.885407 -0.023307       0.7  \n",
       "0     0.000      3.717268      0.000       4.903641 -0.030960       0.7  \n",
       "0     0.236      3.729957      0.001       5.086649 -0.109349       0.7  \n",
       "0     0.022      3.918000      0.001       5.185692 -0.152970       0.7  \n",
       "0     0.022      4.156000      0.001       5.429586 -0.263973       0.7  \n",
       "0     0.001      4.880000      0.000       6.443601 -0.780171       0.7  \n",
       "0     0.000     10.219561      0.000      11.172324 -4.351696       0.7  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# scale\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_pca_scaled = scaler.transform(X_train)\n",
    "X_test_pca_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)  \n",
    "\n",
    "# project to PCs\n",
    "X_train_pca = pca.fit_transform(X_train_pca_scaled)  \n",
    "X_test_pca = pca.transform(X_test_pca_scaled) \n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "resultdfs = run_tests(0.7, \"PCA10\", X_train_pca, X_test_pca, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... FIT.....\n",
      "[WARNING] [2019-01-06 21:59:42,270:AutoMLSMBO(1)::35c7353c3ab8368acb9f8df4b1e41259] Could not find meta-data directory /home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/autosklearn/metalearning/files/r2_regression_dense\n",
      "[WARNING] [2019-01-06 21:59:42,282:EnsembleBuilder(1):35c7353c3ab8368acb9f8df4b1e41259] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-01-06 21:59:42,309:EnsembleBuilder(1):35c7353c3ab8368acb9f8df4b1e41259] No models better than random - using Dummy Score!\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:17] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:01:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:32] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:32] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:02:33] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:03:08] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 2 trees, weight = 0.5\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 3 trees, weight = 0.333333\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:05:51] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:09:49] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:09:49] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 3.11939e-39\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:26] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:11:27] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:16:43] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:19:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:22:11] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:15] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:37] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:32:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:38] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:33:39] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:37:07] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:42] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[22:50:43] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:00:49] /workspace/src/gbm/gbtree.cc:492: drop 1 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:05] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[23:02:06] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      ".... FIT... END ...\n",
      "[(0.400000, SimpleRegressionPipeline({'preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:decision_tree:max_features': 1.0, 'regressor:__choice__': 'decision_tree', 'preprocessor:feature_agglomeration:linkage': 'complete', 'regressor:decision_tree:max_depth_factor': 1.6755483147950971, 'regressor:decision_tree:max_leaf_nodes': 'None', 'imputation:strategy': 'median', 'regressor:decision_tree:min_samples_leaf': 6, 'rescaling:quantile_transformer:n_quantiles': 1981, 'preprocessor:feature_agglomeration:affinity': 'euclidean', 'categorical_encoding:__choice__': 'no_encoding', 'rescaling:__choice__': 'quantile_transformer', 'regressor:decision_tree:min_weight_fraction_leaf': 0.0, 'regressor:decision_tree:min_impurity_decrease': 0.0, 'preprocessor:__choice__': 'feature_agglomeration', 'regressor:decision_tree:min_samples_split': 12, 'preprocessor:feature_agglomeration:n_clusters': 52, 'rescaling:quantile_transformer:output_distribution': 'normal', 'regressor:decision_tree:criterion': 'friedman_mse'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.240000, SimpleRegressionPipeline({'preprocessor:extra_trees_preproc_for_regression:max_features': 0.2697561591073123, 'preprocessor:extra_trees_preproc_for_regression:min_samples_split': 5, 'regressor:__choice__': 'ridge_regression', 'preprocessor:extra_trees_preproc_for_regression:max_depth': 'None', 'preprocessor:extra_trees_preproc_for_regression:n_estimators': 100, 'preprocessor:extra_trees_preproc_for_regression:min_samples_leaf': 13, 'regressor:ridge_regression:alpha': 1.356943709560768e-05, 'preprocessor:extra_trees_preproc_for_regression:min_weight_fraction_leaf': 0.0, 'regressor:ridge_regression:fit_intercept': 'True', 'preprocessor:extra_trees_preproc_for_regression:criterion': 'mse', 'preprocessor:extra_trees_preproc_for_regression:bootstrap': 'False', 'regressor:ridge_regression:tol': 0.00043986933231399616, 'categorical_encoding:__choice__': 'no_encoding', 'rescaling:__choice__': 'quantile_transformer', 'rescaling:quantile_transformer:n_quantiles': 1870, 'preprocessor:extra_trees_preproc_for_regression:max_leaf_nodes': 'None', 'rescaling:quantile_transformer:output_distribution': 'uniform', 'preprocessor:__choice__': 'extra_trees_preproc_for_regression', 'imputation:strategy': 'most_frequent'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.180000, SimpleRegressionPipeline({'regressor:xgradient_boosting:subsample': 0.49582225210790437, 'regressor:xgradient_boosting:reg_alpha': 1.182355135011492e-06, 'preprocessor:fast_ica:fun': 'cube', 'regressor:xgradient_boosting:colsample_bytree': 0.18861669871428016, 'regressor:xgradient_boosting:min_child_weight': 5, 'regressor:xgradient_boosting:base_score': 0.5, 'regressor:xgradient_boosting:gamma': 0, 'categorical_encoding:__choice__': 'no_encoding', 'preprocessor:fast_ica:n_components': 1623, 'preprocessor:__choice__': 'fast_ica', 'imputation:strategy': 'most_frequent', 'regressor:xgradient_boosting:max_depth': 1, 'regressor:__choice__': 'xgradient_boosting', 'regressor:xgradient_boosting:learning_rate': 0.18678214427775974, 'preprocessor:fast_ica:whiten': 'True', 'regressor:xgradient_boosting:max_delta_step': 0, 'regressor:xgradient_boosting:n_estimators': 512, 'rescaling:__choice__': 'none', 'regressor:xgradient_boosting:reg_lambda': 0.00020034750693665557, 'regressor:xgradient_boosting:colsample_bylevel': 0.22118867736622339, 'regressor:xgradient_boosting:booster': 'gbtree', 'regressor:xgradient_boosting:scale_pos_weight': 1, 'preprocessor:fast_ica:algorithm': 'parallel'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.080000, SimpleRegressionPipeline({'regressor:sgd:l1_ratio': 1.7881226172399005e-07, 'regressor:__choice__': 'sgd', 'regressor:sgd:penalty': 'elasticnet', 'regressor:sgd:learning_rate': 'constant', 'preprocessor:polynomial:interaction_only': 'True', 'regressor:sgd:tol': 1.885852799827104e-05, 'regressor:sgd:epsilon': 0.0011952344009921246, 'regressor:sgd:loss': 'epsilon_insensitive', 'rescaling:quantile_transformer:n_quantiles': 1000, 'preprocessor:polynomial:include_bias': 'False', 'regressor:sgd:alpha': 0.09787115626511754, 'regressor:sgd:average': 'False', 'categorical_encoding:__choice__': 'no_encoding', 'regressor:sgd:eta0': 0.0027389919861891036, 'rescaling:__choice__': 'quantile_transformer', 'regressor:sgd:fit_intercept': 'True', 'preprocessor:__choice__': 'polynomial', 'rescaling:quantile_transformer:output_distribution': 'uniform', 'preprocessor:polynomial:degree': 2, 'imputation:strategy': 'median'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.080000, SimpleRegressionPipeline({'categorical_encoding:one_hot_encoding:minimum_fraction': 0.026647218020812026, 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'preprocessor:feature_agglomeration:linkage': 'ward', 'regressor:gradient_boosting:min_impurity_decrease': 0.0, 'regressor:gradient_boosting:max_features': 0.5527448538321074, 'regressor:gradient_boosting:n_estimators': 477, 'regressor:gradient_boosting:max_leaf_nodes': 'None', 'regressor:gradient_boosting:min_weight_fraction_leaf': 0.0, 'preprocessor:__choice__': 'feature_agglomeration', 'regressor:gradient_boosting:learning_rate': 0.05651703628466033, 'imputation:strategy': 'most_frequent', 'preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:gradient_boosting:loss': 'ls', 'regressor:__choice__': 'gradient_boosting', 'regressor:gradient_boosting:subsample': 0.884086883773331, 'regressor:gradient_boosting:min_samples_leaf': 14, 'regressor:gradient_boosting:max_depth': 6, 'preprocessor:feature_agglomeration:affinity': 'euclidean', 'rescaling:__choice__': 'none', 'regressor:gradient_boosting:min_samples_split': 2, 'preprocessor:feature_agglomeration:n_clusters': 355, 'categorical_encoding:__choice__': 'one_hot_encoding'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "(0.020000, SimpleRegressionPipeline({'categorical_encoding:one_hot_encoding:minimum_fraction': 0.26332650675390973, 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'regressor:xgradient_boosting:subsample': 0.29934548938990285, 'regressor:xgradient_boosting:reg_alpha': 0.0019351903914494113, 'regressor:xgradient_boosting:colsample_bytree': 0.962707551371452, 'regressor:xgradient_boosting:min_child_weight': 6, 'regressor:xgradient_boosting:base_score': 0.5, 'preprocessor:pca:whiten': 'False', 'regressor:xgradient_boosting:gamma': 0, 'categorical_encoding:__choice__': 'one_hot_encoding', 'preprocessor:__choice__': 'pca', 'imputation:strategy': 'most_frequent', 'regressor:xgradient_boosting:max_depth': 10, 'regressor:__choice__': 'xgradient_boosting', 'regressor:xgradient_boosting:learning_rate': 0.1957203816974481, 'regressor:xgradient_boosting:max_delta_step': 0, 'regressor:xgradient_boosting:n_estimators': 512, 'preprocessor:pca:keep_variance': 0.7581707883325663, 'rescaling:__choice__': 'none', 'regressor:xgradient_boosting:reg_lambda': 0.001206800913309577, 'regressor:xgradient_boosting:colsample_bylevel': 0.11455154225516638, 'regressor:xgradient_boosting:scale_pos_weight': 1, 'regressor:xgradient_boosting:booster': 'gbtree'},\n",
      "dataset_properties={\n",
      "  'sparse': False,\n",
      "  'task': 4,\n",
      "  'target_type': 'regression',\n",
      "  'signed': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': False})),\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.17531314939689646\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.regression\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import autosklearn.regression\n",
    "\n",
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=4000,\n",
    "    per_run_time_limit=40,\n",
    "    #tmp_folder='/tmp/autosklearn_regression_example_tmp',\n",
    "    #output_folder='/tmp/autosklearn_regression_example_out'\n",
    ")\n",
    "print(\".... FIT.....\")\n",
    "automl.fit(X_train, y_train)\n",
    "print(\".... FIT... END ...\")\n",
    "\n",
    "print(automl.show_models())\n",
    "predictions = automl.predict(X_test)\n",
    "print(\"R2 score:\", sklearn.metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4: Stepwise algorithm - attribute selection (see R notebook file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stepwise algorithm in R, we came to the most important features for this data set.\n",
    "\n",
    "failures + goout + Mjobhealth + Fjobother + freetime + \n",
    "    log(studytime) + addressR + sexF + famsupno + schoolsupno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = [\"failures\",\"goout\",\"Mjob_health\",\"Fjob_other\",\"freetime\", \"studytime\", \"address_R\", \"sex_F\",\n",
    "         \"famsup_no\",\"schoolsup_no\"]\n",
    "\n",
    "#important_features = [\"failures\" , \"Mjob_health\" , \"traveltime\" , \"address_R\" , \"freetime\" , \n",
    "#    \"goout\" , \"famsup_no\" , \"reason_reputation\" , \"Fjob_services\" , \"studytime\" ,\n",
    "#    \"sex_F\" , \"Dalc\"]\n",
    "\n",
    "X_train2 = X_train[important_features]\n",
    "X_test2 = X_test[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failures</th>\n",
       "      <th>goout</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>freetime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>address_R</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>famsup_no</th>\n",
       "      <th>schoolsup_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   failures  goout  Mjob_health  Fjob_other  freetime  studytime  address_R  \\\n",
       "0         0      2            0           0         3          3          0   \n",
       "1         0      1            0           1         4          1          1   \n",
       "2         0      3            0           0         3          3          0   \n",
       "3         0      3            1           1         1          1          0   \n",
       "4         0      3            0           1         3          2          0   \n",
       "\n",
       "   sex_F  famsup_no  schoolsup_no  \n",
       "0      1          1             1  \n",
       "1      0          1             1  \n",
       "2      1          0             1  \n",
       "3      0          1             1  \n",
       "4      1          0             1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.386107</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.889000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.784887</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.383919</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911848</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790676</td>\n",
       "      <td>0.383919</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790684</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.378729</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.879489</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.808692</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>0.268415</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.269930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.244000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.130763</td>\n",
       "      <td>0.268415</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>0.239556</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.244175</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.320701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.211448</td>\n",
       "      <td>0.239556</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>0.123633</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.126613</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.493954</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.521065</td>\n",
       "      <td>0.123633</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.058211</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.719038</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.691056</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.031306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.473752</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.757171</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.056434</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.828033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.963855</td>\n",
       "      <td>-0.056434</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.524850</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.505604</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.390000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.963640</td>\n",
       "      <td>-0.524850</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.637960</td>\n",
       "      <td>raw_data</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.635530</td>\n",
       "      <td>0.286</td>\n",
       "      <td>4.704676</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.180867</td>\n",
       "      <td>-0.637960</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2   Setting      Xsize  \\\n",
       "0                           Ridge  0.385799  raw_data  (148, 10)   \n",
       "0                  Ridge-Alpha001  0.383919  raw_data  (148, 10)   \n",
       "0        LinearRegression-NonNorm  0.383917  raw_data  (148, 10)   \n",
       "0             BayesianRidge-Score  0.378049  raw_data  (148, 10)   \n",
       "0           KNRegressor-uniformN5  0.268415  raw_data  (148, 10)   \n",
       "0  LinearRegression-Norm-NoInterc  0.239556  raw_data  (148, 10)   \n",
       "0          KNRegressor-distanceN5  0.123633  raw_data  (148, 10)   \n",
       "0           RandomForestRegressor  0.056492  raw_data  (148, 10)   \n",
       "0         KNRegressor-distanceN50  0.029709  raw_data  (148, 10)   \n",
       "0                BaggingRegressor -0.056434  raw_data  (148, 10)   \n",
       "0          DecisionTreeRegressor- -0.524850  raw_data  (148, 10)   \n",
       "0                GradientBoosting -0.637960  raw_data  (148, 10)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.386107     0.001      2.889000      0.000       3.784887  0.385799   \n",
       "0       0.384167     0.001      2.911848      0.000       3.790676  0.383919   \n",
       "0       0.384165     0.001      2.911873      0.000       3.790684  0.383917   \n",
       "0       0.378729     0.003      2.879489      0.000       3.808692  0.378049   \n",
       "0       0.269930     0.000      3.244000      0.001       4.130763  0.268415   \n",
       "0       0.244175     0.001      3.320701      0.000       4.211448  0.239556   \n",
       "0       0.126613     0.000      3.493954      0.001       4.521065  0.123633   \n",
       "0       0.058211     0.026      3.719038      0.001       4.691056  0.056492   \n",
       "0       0.031306     0.000      3.473752      0.001       4.757171  0.029709   \n",
       "0      -0.047546     0.027      3.828033      0.002       4.963855 -0.056434   \n",
       "0      -0.505604     0.001      4.390000      0.000       5.963640 -0.524850   \n",
       "0      -0.635530     0.286      4.704676      0.002       6.180867 -0.637960   \n",
       "\n",
       "   testSize  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdfs = run_tests(0.33, \"raw_data\", X_train2, X_test2, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5: Stepwise algorithm with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>R2</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Xsize</th>\n",
       "      <th>expl_variance</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>mean_abs_err</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>r_mean_sq_err</th>\n",
       "      <th>score</th>\n",
       "      <th>testSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-NonNorm</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790684</td>\n",
       "      <td>0.383917</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge-Alpha001</td>\n",
       "      <td>0.383916</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384164</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.911873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.790685</td>\n",
       "      <td>0.383916</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.383408</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.383669</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.912039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.792247</td>\n",
       "      <td>0.383408</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BayesianRidge-Score</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.935405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.856468</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN50</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.022754</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.521739</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.777887</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.019</td>\n",
       "      <td>3.764633</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.831004</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-uniformN5</td>\n",
       "      <td>-0.098493</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.864000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.061699</td>\n",
       "      <td>-0.098493</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.097369</td>\n",
       "      <td>0.021</td>\n",
       "      <td>4.043167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.069375</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNRegressor-distanceN5</td>\n",
       "      <td>-0.186040</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.179801</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.002726</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.259536</td>\n",
       "      <td>-0.186040</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>-0.636195</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.633688</td>\n",
       "      <td>0.272</td>\n",
       "      <td>4.700318</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.177536</td>\n",
       "      <td>-0.636195</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor-</td>\n",
       "      <td>-0.934521</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>-0.933424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.717142</td>\n",
       "      <td>-0.934521</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression-Norm-NoInterc</td>\n",
       "      <td>-4.157632</td>\n",
       "      <td>bestPredictorsScaled</td>\n",
       "      <td>(148, 10)</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.295204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.967887</td>\n",
       "      <td>-4.157632</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Algorithm        R2               Setting      Xsize  \\\n",
       "0        LinearRegression-NonNorm  0.383917  bestPredictorsScaled  (148, 10)   \n",
       "0                  Ridge-Alpha001  0.383916  bestPredictorsScaled  (148, 10)   \n",
       "0                           Ridge  0.383408  bestPredictorsScaled  (148, 10)   \n",
       "0             BayesianRidge-Score  0.362348  bestPredictorsScaled  (148, 10)   \n",
       "0         KNRegressor-distanceN50  0.021240  bestPredictorsScaled  (148, 10)   \n",
       "0           RandomForestRegressor -0.000643  bestPredictorsScaled  (148, 10)   \n",
       "0           KNRegressor-uniformN5 -0.098493  bestPredictorsScaled  (148, 10)   \n",
       "0                BaggingRegressor -0.101826  bestPredictorsScaled  (148, 10)   \n",
       "0          KNRegressor-distanceN5 -0.186040  bestPredictorsScaled  (148, 10)   \n",
       "0                GradientBoosting -0.636195  bestPredictorsScaled  (148, 10)   \n",
       "0          DecisionTreeRegressor- -0.934521  bestPredictorsScaled  (148, 10)   \n",
       "0  LinearRegression-Norm-NoInterc -4.157632  bestPredictorsScaled  (148, 10)   \n",
       "\n",
       "   expl_variance  fit_time  mean_abs_err  pred_time  r_mean_sq_err     score  \\\n",
       "0       0.384165     0.001      2.911873      0.000       3.790684  0.383917   \n",
       "0       0.384164     0.001      2.911873      0.000       3.790685  0.383916   \n",
       "0       0.383669     0.001      2.912039      0.000       3.792247  0.383408   \n",
       "0       0.362927     0.002      2.935405      0.000       3.856468  0.362348   \n",
       "0       0.022754     0.000      3.521739      0.001       4.777887  0.021240   \n",
       "0       0.002488     0.019      3.764633      0.001       4.831004 -0.000643   \n",
       "0      -0.083058     0.000      3.864000      0.000       5.061699 -0.098493   \n",
       "0      -0.097369     0.021      4.043167      0.001       5.069375 -0.101826   \n",
       "0      -0.179801     0.000      4.002726      0.001       5.259536 -0.186040   \n",
       "0      -0.633688     0.272      4.700318      0.002       6.177536 -0.636195   \n",
       "0      -0.933424     0.000      5.020000      0.000       6.717142 -0.934521   \n",
       "0       0.384165     0.000     10.295204      0.000      10.967887 -4.157632   \n",
       "\n",
       "   testSize  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  \n",
       "0      0.33  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"Grade\" in X_train2:\n",
    "    X_train2.drop([\"Grade\"], axis=1, inplace=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train2)\n",
    "X_train2_scaled = scaler.transform(X_train2)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=X_train2.columns, index=X_train2.index)\n",
    "X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=X_test2.columns, index=X_test2.index)\n",
    "\n",
    "\n",
    "resultdfs = run_tests(0.33, \"bestPredictorsScaled\", X_train2_scaled, X_test2_scaled, y_train, y_test, False)\n",
    "resultdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression CV - with best setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - best params:  {'fit_intercept': True, 'normalize': True, 'max_iter': 1500, 'alpha': 0.3, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "param_grid_bayesian_ridge = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter' : [1500,2000],\n",
    "    'alpha' : [1.0,0.99,0.95,0.9,0.8,0.7,0.5,0.3,0.1,0.01],\n",
    "    'fit_intercept' : [True, False],\n",
    "    'normalize' : [True, False],\n",
    "    'tol' : [0.001,0.0001,0.002, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "mod = Ridge()\n",
    "CV_dtc = GridSearchCV(estimator=mod, param_grid=param_grid_bayesian_ridge, cv = 10, n_jobs=-1)\n",
    "CV_dtc.fit(X_train2, y_train)\n",
    "rr_best_params = CV_dtc.best_params_\n",
    "print(\"Ridge Regression - best params: \",rr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Coefficients: \n",
      " [-1.61212128 -0.4903169   2.15730588 -0.99224454  0.41652223  0.84901308\n",
      " -1.65484677 -1.1928781   0.53986413  0.99268073]\n",
      "RR Mean absolute error: 2.97137\n",
      "RR Mean squared error: 3.91269\n",
      "RR Variance score: 0.34\n"
     ]
    }
   ],
   "source": [
    "if \"Grade\" in X_train2_scaled:\n",
    "    X_train2_scaled.drop([\"Grade\"], axis=1, inplace=True)\n",
    "\n",
    "# bayesian regression\n",
    "bregr = Ridge(**rr_best_params)\n",
    "bregr.fit(X_train2, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2)\n",
    "\n",
    "\n",
    "print('RR Coefficients: \\n', bregr.coef_)\n",
    "print(\"RR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"RR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('RR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Coefficients: \n",
      " [-1.61212128 -0.4903169   2.15730588 -0.99224454  0.41652223  0.84901308\n",
      " -1.65484677 -1.1928781   0.53986413  0.99268073]\n",
      "RR Mean absolute error: 2.97137\n",
      "RR Mean squared error: 3.91269\n",
      "RR Variance score: 0.34\n"
     ]
    }
   ],
   "source": [
    "# bayesian regression\n",
    "bregr = Ridge(**rr_best_params)\n",
    "bregr.fit(X_train2, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2)\n",
    "\n",
    "\n",
    "print('RR Coefficients: \\n', bregr.coef_)\n",
    "print(\"RR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"RR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('RR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Coefficients: \n",
      " [-1.61233661 -0.81799009  0.66505167 -0.68552816  0.64651496  0.95385323\n",
      " -0.91095412 -0.82917865  0.45935026  0.44024104]\n",
      "LR Mean absolute error: 2.91187\n",
      "LR Mean squared error: 3.79068\n",
      "LR Variance score: 0.38\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "bregr = LinearRegression(normalize=False)\n",
    "bregr.fit(X_train2_scaled, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test2_scaled)\n",
    "\n",
    "\n",
    "print('LR Coefficients: \\n', bregr.coef_)\n",
    "print(\"LR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "print(\"LR Mean squared error: %.5f\"% np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print('LR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failures</th>\n",
       "      <th>goout</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>freetime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>address_R</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>famsup_no</th>\n",
       "      <th>schoolsup_no</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-1.082210</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>-1.114641</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>1.150271</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-2.024369</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>0.838166</td>\n",
       "      <td>-1.214175</td>\n",
       "      <td>2.025697</td>\n",
       "      <td>-1.161553</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>-1.114641</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>1.150271</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>-0.757937</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>3.929942</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>-2.315612</td>\n",
       "      <td>-1.214175</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>-1.161553</td>\n",
       "      <td>1.319371</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.448353</td>\n",
       "      <td>-0.140051</td>\n",
       "      <td>-0.254457</td>\n",
       "      <td>0.897150</td>\n",
       "      <td>-0.213093</td>\n",
       "      <td>-0.031952</td>\n",
       "      <td>-0.493657</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>-0.757937</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   failures     goout  Mjob_health  Fjob_other  freetime  studytime  \\\n",
       "0 -0.448353 -1.082210    -0.254457   -1.114641 -0.213093   1.150271   \n",
       "1 -0.448353 -2.024369    -0.254457    0.897150  0.838166  -1.214175   \n",
       "2 -0.448353 -0.140051    -0.254457   -1.114641 -0.213093   1.150271   \n",
       "3 -0.448353 -0.140051     3.929942    0.897150 -2.315612  -1.214175   \n",
       "4 -0.448353 -0.140051    -0.254457    0.897150 -0.213093  -0.031952   \n",
       "\n",
       "   address_R     sex_F  famsup_no  schoolsup_no  Grade  \n",
       "0  -0.493657  0.860916   1.319371      0.360237     16  \n",
       "1   2.025697 -1.161553   1.319371      0.360237     10  \n",
       "2  -0.493657  0.860916  -0.757937      0.360237     19  \n",
       "3  -0.493657 -1.161553   1.319371      0.360237     10  \n",
       "4  -0.493657  0.860916  -0.757937      0.360237      0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2_scaled[\"Grade\"]=y_train\n",
    "X_train2_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draws: 15\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 50\n",
      "LR Mean Squared Error : 3.93000\n",
      "LR Variance score: 0.34\n",
      "draws: 70\n",
      "LR Mean Squared Error : 3.87000\n",
      "LR Variance score: 0.36\n",
      "draws: 100\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 150\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 200\n",
      "LR Mean Squared Error : 3.78000\n",
      "LR Variance score: 0.39\n",
      "draws: 500\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 781\n",
      "LR Mean Squared Error : 3.82000\n",
      "LR Variance score: 0.37\n",
      "draws: 1000\n",
      "LR Mean Squared Error : 3.79000\n",
      "LR Variance score: 0.39\n",
      "draws: 1500\n",
      "LR Mean Squared Error : 3.80000\n",
      "LR Variance score: 0.38\n",
      "draws: 2001\n",
      "LR Mean Squared Error : 3.78000\n",
      "LR Variance score: 0.39\n",
      "draws: 3001\n",
      "LR Mean Squared Error : 3.77000\n",
      "LR Variance score: 0.39\n",
      "3.815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_train2_scaled[\"Grade\"] = y_train\n",
    "\n",
    "draws = [15,50,70,100,150,200,500,781,1000,1500,2001,3001]\n",
    "result = {}\n",
    "sample_size=0.7\n",
    "\n",
    "for draw_nr in draws:\n",
    "    print(\"draws:\", draw_nr)\n",
    "    ensamble_dframe = pd.DataFrame()\n",
    "    for i in range(0,draw_nr):\n",
    "        ssample = X_train2_scaled.sample(frac=sample_size, replace=True)\n",
    "        sampleX = ssample.drop([\"Grade\"], axis=1)\n",
    "        sampleY = ssample[\"Grade\"]\n",
    "        #print(sampleX.shape)\n",
    "        bregr = LinearRegression(normalize=False)\n",
    "        bregr.fit(sampleX, sampleY)\n",
    "\n",
    "        y_pred2 = bregr.predict(X_test2_scaled)\n",
    "        ensamble_dframe[i] = y_pred2\n",
    "        #print(\"BR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, y_pred2))\n",
    "        #print('BR Variance score: %.2f' % metrics.r2_score(y_test, y_pred2))\n",
    "    \n",
    "    \n",
    "    ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "    # round up gives even better result\n",
    "    ensamble_predict = np.ceil(ensamble_predict)\n",
    "    mae = metrics.mean_absolute_error(y_test, ensamble_predict)\n",
    "    r2 = metrics.r2_score(y_test, ensamble_predict)\n",
    "    mse = np.round(np.sqrt(metrics.mean_squared_error(y_test, ensamble_predict)),2)\n",
    "    #print(\"LR Mean Absolute Error : %.5f\"% mae)\n",
    "    print(\"LR Mean Squared Error : %.5f\"% mse)\n",
    "    print('LR Variance score: %.2f' % r2)\n",
    "    result[draw_nr] = mse\n",
    "    \n",
    "## Plot\n",
    "lists = sorted(result.items(), reverse=True) # sorted by key, return a list of tuples\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "plt.plot(x, y)\n",
    "plt.xticks(draws)\n",
    "numbers = [result[key] for key in result]\n",
    "mean_ = statistics.mean(numbers)\n",
    "print(mean_)\n",
    "#plt.yticks(result.values())\n",
    "plt.title(\"Linear Reg. Ensamble - Sample frac:\"+str(sample_size)+\", mean \"+str(np.round(mean_,2)))\n",
    "plt.xlabel(\"Sample draws\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Ensamble results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Root Squared Error: 3.77000\n",
      "LR Mean absolute error: 2.84000\n",
      "LR Variance score: 0.39\n"
     ]
    }
   ],
   "source": [
    "ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "# round up gives even better result\n",
    "ensamble_predict = np.ceil(ensamble_predict)\n",
    "print(\"LR Root Squared Error: %.5f\"% np.round(np.sqrt(metrics.mean_squared_error(y_test, ensamble_predict)),2))\n",
    "print(\"LR Mean absolute error: %.5f\"% metrics.mean_absolute_error(y_test, ensamble_predict))\n",
    "print('LR Variance score: %.2f' % metrics.r2_score(y_test, ensamble_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensamble_predict<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = df_dummy_train#[important_features]\n",
    "X_Test = df_dummy_test#[important_features]\n",
    "\n",
    "#X_Train.drop([\"Grade\"], axis=1, inplace=True)\n",
    "# scale train and test data\n",
    "scaler = preprocessing.StandardScaler().fit(X_Train)\n",
    "X_Train_scaled = scaler.transform(X_Train)\n",
    "X_Test_scaled = scaler.transform(X_Test)\n",
    "\n",
    "X_Train_sdf = pd.DataFrame(X_Train_scaled, columns=X_Train.columns, index=X_Train.index)\n",
    "X_Test_sdf = pd.DataFrame(X_Test_scaled, columns=X_Test.columns, index=X_Test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 57)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Train_sdf.drop([\"Grade\"], axis=1, inplace=True)\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly2 = poly.fit_transform(X_Train_sdf)\n",
    "X_test_poly2 = poly.transform(X_Test_sdf)\n",
    "# bayesian regression\n",
    "bregr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=5, random_state=1607, loss='ls')\n",
    "bregr.fit(X_train_poly2, train_grade_values)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = bregr.predict(X_test_poly2)\n",
    "\n",
    "# bayesian regression\n",
    "#X_Train_sdf.drop([\"Grade\"], axis=1, inplace=True)\n",
    "#bregr = Ridge(fit_intercept=True, normalize=True,max_iter=1500 ,alpha=0.3, tol=0.001)\n",
    "#bregr.fit(X_Train_sdf, train_grade_values)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_submit = y_pred2#bregr.predict(X_Test_sdf)\n",
    "# round\n",
    "y_pred_submit = np.ceil(y_pred_submit)\n",
    "y_pred_submit[y_pred_submit < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensamble submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 0\n",
      "predict: 1\n",
      "predict: 2\n",
      "predict: 3\n",
      "predict: 4\n",
      "predict: 5\n",
      "predict: 6\n",
      "predict: 7\n",
      "predict: 8\n",
      "predict: 9\n",
      "predict: 10\n",
      "predict: 11\n",
      "predict: 12\n",
      "predict: 13\n",
      "predict: 14\n",
      "predict: 15\n",
      "predict: 16\n",
      "predict: 17\n",
      "predict: 18\n",
      "predict: 19\n",
      "predict: 20\n",
      "predict: 21\n",
      "predict: 22\n",
      "predict: 23\n",
      "predict: 24\n",
      "predict: 25\n",
      "predict: 26\n",
      "predict: 27\n",
      "predict: 28\n",
      "predict: 29\n",
      "predict: 30\n",
      "predict: 31\n",
      "predict: 32\n",
      "predict: 33\n",
      "predict: 34\n",
      "predict: 35\n",
      "predict: 36\n",
      "predict: 37\n",
      "predict: 38\n",
      "predict: 39\n",
      "predict: 40\n",
      "predict: 41\n",
      "predict: 42\n",
      "predict: 43\n",
      "predict: 44\n",
      "predict: 45\n",
      "predict: 46\n",
      "predict: 47\n",
      "predict: 48\n",
      "predict: 49\n",
      "predict: 50\n",
      "predict: 51\n",
      "predict: 52\n",
      "predict: 53\n",
      "predict: 54\n",
      "predict: 55\n",
      "predict: 56\n",
      "predict: 57\n",
      "predict: 58\n",
      "predict: 59\n",
      "predict: 60\n",
      "predict: 61\n",
      "predict: 62\n",
      "predict: 63\n",
      "predict: 64\n",
      "predict: 65\n",
      "predict: 66\n",
      "predict: 67\n",
      "predict: 68\n",
      "predict: 69\n",
      "predict: 70\n",
      "predict: 71\n",
      "predict: 72\n",
      "predict: 73\n",
      "predict: 74\n",
      "predict: 75\n",
      "predict: 76\n",
      "predict: 77\n",
      "predict: 78\n",
      "predict: 79\n",
      "predict: 80\n",
      "predict: 81\n",
      "predict: 82\n",
      "predict: 83\n",
      "predict: 84\n",
      "predict: 85\n",
      "predict: 86\n",
      "predict: 87\n",
      "predict: 88\n",
      "predict: 89\n",
      "predict: 90\n",
      "predict: 91\n",
      "predict: 92\n",
      "predict: 93\n",
      "predict: 94\n",
      "predict: 95\n",
      "predict: 96\n",
      "predict: 97\n",
      "predict: 98\n",
      "predict: 99\n",
      "predict: 100\n",
      "predict: 101\n",
      "predict: 102\n",
      "predict: 103\n",
      "predict: 104\n",
      "predict: 105\n",
      "predict: 106\n",
      "predict: 107\n",
      "predict: 108\n",
      "predict: 109\n",
      "predict: 110\n",
      "predict: 111\n",
      "predict: 112\n",
      "predict: 113\n",
      "predict: 114\n",
      "predict: 115\n",
      "predict: 116\n",
      "predict: 117\n",
      "predict: 118\n",
      "predict: 119\n",
      "predict: 120\n",
      "predict: 121\n",
      "predict: 122\n",
      "predict: 123\n",
      "predict: 124\n",
      "predict: 125\n",
      "predict: 126\n",
      "predict: 127\n",
      "predict: 128\n",
      "predict: 129\n",
      "predict: 130\n",
      "predict: 131\n",
      "predict: 132\n",
      "predict: 133\n",
      "predict: 134\n",
      "predict: 135\n",
      "predict: 136\n",
      "predict: 137\n",
      "predict: 138\n",
      "predict: 139\n",
      "predict: 140\n",
      "predict: 141\n",
      "predict: 142\n",
      "predict: 143\n",
      "predict: 144\n",
      "predict: 145\n",
      "predict: 146\n",
      "predict: 147\n",
      "predict: 148\n",
      "predict: 149\n",
      "predict: 150\n",
      "predict: 151\n",
      "predict: 152\n",
      "predict: 153\n",
      "predict: 154\n",
      "predict: 155\n",
      "predict: 156\n",
      "predict: 157\n",
      "predict: 158\n",
      "predict: 159\n",
      "predict: 160\n",
      "predict: 161\n",
      "predict: 162\n",
      "predict: 163\n",
      "predict: 164\n",
      "predict: 165\n",
      "predict: 166\n",
      "predict: 167\n",
      "predict: 168\n",
      "predict: 169\n",
      "predict: 170\n",
      "predict: 171\n",
      "predict: 172\n",
      "predict: 173\n",
      "predict: 174\n",
      "predict: 175\n",
      "predict: 176\n",
      "predict: 177\n",
      "predict: 178\n",
      "predict: 179\n",
      "predict: 180\n",
      "predict: 181\n",
      "predict: 182\n",
      "predict: 183\n",
      "predict: 184\n",
      "predict: 185\n",
      "predict: 186\n",
      "predict: 187\n",
      "predict: 188\n",
      "predict: 189\n",
      "predict: 190\n",
      "predict: 191\n",
      "predict: 192\n",
      "predict: 193\n",
      "predict: 194\n",
      "predict: 195\n",
      "predict: 196\n",
      "predict: 197\n",
      "predict: 198\n",
      "predict: 199\n",
      "predict: 200\n",
      "predict: 201\n",
      "predict: 202\n",
      "predict: 203\n",
      "predict: 204\n",
      "predict: 205\n",
      "predict: 206\n",
      "predict: 207\n",
      "predict: 208\n",
      "predict: 209\n",
      "predict: 210\n",
      "predict: 211\n",
      "predict: 212\n",
      "predict: 213\n",
      "predict: 214\n",
      "predict: 215\n",
      "predict: 216\n",
      "predict: 217\n",
      "predict: 218\n",
      "predict: 219\n",
      "predict: 220\n",
      "predict: 221\n",
      "predict: 222\n",
      "predict: 223\n",
      "predict: 224\n",
      "predict: 225\n",
      "predict: 226\n",
      "predict: 227\n",
      "predict: 228\n",
      "predict: 229\n",
      "predict: 230\n",
      "predict: 231\n",
      "predict: 232\n",
      "predict: 233\n",
      "predict: 234\n",
      "predict: 235\n",
      "predict: 236\n",
      "predict: 237\n",
      "predict: 238\n",
      "predict: 239\n",
      "predict: 240\n",
      "predict: 241\n",
      "predict: 242\n",
      "predict: 243\n",
      "predict: 244\n",
      "predict: 245\n",
      "predict: 246\n",
      "predict: 247\n",
      "predict: 248\n",
      "predict: 249\n",
      "predict: 250\n",
      "predict: 251\n",
      "predict: 252\n",
      "predict: 253\n",
      "predict: 254\n",
      "predict: 255\n",
      "predict: 256\n",
      "predict: 257\n",
      "predict: 258\n",
      "predict: 259\n",
      "predict: 260\n",
      "predict: 261\n",
      "predict: 262\n",
      "predict: 263\n",
      "predict: 264\n",
      "predict: 265\n",
      "predict: 266\n",
      "predict: 267\n",
      "predict: 268\n",
      "predict: 269\n",
      "predict: 270\n",
      "predict: 271\n",
      "predict: 272\n",
      "predict: 273\n",
      "predict: 274\n",
      "predict: 275\n",
      "predict: 276\n",
      "predict: 277\n",
      "predict: 278\n",
      "predict: 279\n",
      "predict: 280\n",
      "predict: 281\n",
      "predict: 282\n",
      "predict: 283\n",
      "predict: 284\n",
      "predict: 285\n",
      "predict: 286\n",
      "predict: 287\n",
      "predict: 288\n",
      "predict: 289\n",
      "predict: 290\n",
      "predict: 291\n",
      "predict: 292\n",
      "predict: 293\n",
      "predict: 294\n",
      "predict: 295\n",
      "predict: 296\n",
      "predict: 297\n",
      "predict: 298\n",
      "predict: 299\n",
      "predict: 300\n",
      "predict: 301\n",
      "predict: 302\n",
      "predict: 303\n",
      "predict: 304\n",
      "predict: 305\n",
      "predict: 306\n",
      "predict: 307\n",
      "predict: 308\n",
      "predict: 309\n",
      "predict: 310\n",
      "predict: 311\n",
      "predict: 312\n",
      "predict: 313\n",
      "predict: 314\n",
      "predict: 315\n",
      "predict: 316\n",
      "predict: 317\n",
      "predict: 318\n",
      "predict: 319\n",
      "predict: 320\n",
      "predict: 321\n",
      "predict: 322\n",
      "predict: 323\n",
      "predict: 324\n",
      "predict: 325\n",
      "predict: 326\n",
      "predict: 327\n",
      "predict: 328\n",
      "predict: 329\n",
      "predict: 330\n",
      "predict: 331\n",
      "predict: 332\n",
      "predict: 333\n",
      "predict: 334\n",
      "predict: 335\n",
      "predict: 336\n",
      "predict: 337\n",
      "predict: 338\n",
      "predict: 339\n",
      "predict: 340\n",
      "predict: 341\n",
      "predict: 342\n",
      "predict: 343\n",
      "predict: 344\n",
      "predict: 345\n",
      "predict: 346\n",
      "predict: 347\n",
      "predict: 348\n",
      "predict: 349\n",
      "predict: 350\n",
      "predict: 351\n",
      "predict: 352\n",
      "predict: 353\n",
      "predict: 354\n",
      "predict: 355\n",
      "predict: 356\n",
      "predict: 357\n",
      "predict: 358\n",
      "predict: 359\n",
      "predict: 360\n",
      "predict: 361\n",
      "predict: 362\n",
      "predict: 363\n",
      "predict: 364\n",
      "predict: 365\n",
      "predict: 366\n",
      "predict: 367\n",
      "predict: 368\n",
      "predict: 369\n",
      "predict: 370\n",
      "predict: 371\n",
      "predict: 372\n",
      "predict: 373\n",
      "predict: 374\n",
      "predict: 375\n",
      "predict: 376\n",
      "predict: 377\n",
      "predict: 378\n",
      "predict: 379\n",
      "predict: 380\n",
      "predict: 381\n",
      "predict: 382\n",
      "predict: 383\n",
      "predict: 384\n",
      "predict: 385\n",
      "predict: 386\n",
      "predict: 387\n",
      "predict: 388\n",
      "predict: 389\n",
      "predict: 390\n",
      "predict: 391\n",
      "predict: 392\n",
      "predict: 393\n",
      "predict: 394\n",
      "predict: 395\n",
      "predict: 396\n",
      "predict: 397\n",
      "predict: 398\n",
      "predict: 399\n",
      "predict: 400\n",
      "predict: 401\n",
      "predict: 402\n",
      "predict: 403\n",
      "predict: 404\n",
      "predict: 405\n",
      "predict: 406\n",
      "predict: 407\n",
      "predict: 408\n",
      "predict: 409\n",
      "predict: 410\n",
      "predict: 411\n",
      "predict: 412\n",
      "predict: 413\n",
      "predict: 414\n",
      "predict: 415\n",
      "predict: 416\n",
      "predict: 417\n",
      "predict: 418\n",
      "predict: 419\n",
      "predict: 420\n",
      "predict: 421\n",
      "predict: 422\n",
      "predict: 423\n",
      "predict: 424\n",
      "predict: 425\n",
      "predict: 426\n",
      "predict: 427\n",
      "predict: 428\n",
      "predict: 429\n",
      "predict: 430\n",
      "predict: 431\n",
      "predict: 432\n",
      "predict: 433\n",
      "predict: 434\n",
      "predict: 435\n",
      "predict: 436\n",
      "predict: 437\n",
      "predict: 438\n",
      "predict: 439\n",
      "predict: 440\n",
      "predict: 441\n",
      "predict: 442\n",
      "predict: 443\n",
      "predict: 444\n",
      "predict: 445\n",
      "predict: 446\n",
      "predict: 447\n",
      "predict: 448\n",
      "predict: 449\n",
      "predict: 450\n",
      "predict: 451\n",
      "predict: 452\n",
      "predict: 453\n",
      "predict: 454\n",
      "predict: 455\n",
      "predict: 456\n",
      "predict: 457\n",
      "predict: 458\n",
      "predict: 459\n",
      "predict: 460\n",
      "predict: 461\n",
      "predict: 462\n",
      "predict: 463\n",
      "predict: 464\n",
      "predict: 465\n",
      "predict: 466\n",
      "predict: 467\n",
      "predict: 468\n",
      "predict: 469\n",
      "predict: 470\n",
      "predict: 471\n",
      "predict: 472\n",
      "predict: 473\n",
      "predict: 474\n",
      "predict: 475\n",
      "predict: 476\n",
      "predict: 477\n",
      "predict: 478\n",
      "predict: 479\n",
      "predict: 480\n",
      "predict: 481\n",
      "predict: 482\n",
      "predict: 483\n",
      "predict: 484\n",
      "predict: 485\n",
      "predict: 486\n",
      "predict: 487\n",
      "predict: 488\n",
      "predict: 489\n",
      "predict: 490\n",
      "predict: 491\n",
      "predict: 492\n",
      "predict: 493\n",
      "predict: 494\n",
      "predict: 495\n",
      "predict: 496\n",
      "predict: 497\n",
      "predict: 498\n",
      "predict: 499\n",
      "predict: 500\n",
      "predict: 501\n",
      "predict: 502\n",
      "predict: 503\n",
      "predict: 504\n",
      "predict: 505\n",
      "predict: 506\n",
      "predict: 507\n",
      "predict: 508\n",
      "predict: 509\n",
      "predict: 510\n",
      "predict: 511\n",
      "predict: 512\n",
      "predict: 513\n",
      "predict: 514\n",
      "predict: 515\n",
      "predict: 516\n",
      "predict: 517\n",
      "predict: 518\n",
      "predict: 519\n",
      "predict: 520\n",
      "predict: 521\n",
      "predict: 522\n",
      "predict: 523\n",
      "predict: 524\n",
      "predict: 525\n",
      "predict: 526\n",
      "predict: 527\n",
      "predict: 528\n",
      "predict: 529\n",
      "predict: 530\n",
      "predict: 531\n",
      "predict: 532\n",
      "predict: 533\n",
      "predict: 534\n",
      "predict: 535\n",
      "predict: 536\n",
      "predict: 537\n",
      "predict: 538\n",
      "predict: 539\n",
      "predict: 540\n",
      "predict: 541\n",
      "predict: 542\n",
      "predict: 543\n",
      "predict: 544\n",
      "predict: 545\n",
      "predict: 546\n",
      "predict: 547\n",
      "predict: 548\n",
      "predict: 549\n",
      "predict: 550\n",
      "predict: 551\n",
      "predict: 552\n",
      "predict: 553\n",
      "predict: 554\n",
      "predict: 555\n",
      "predict: 556\n",
      "predict: 557\n",
      "predict: 558\n",
      "predict: 559\n",
      "predict: 560\n",
      "predict: 561\n",
      "predict: 562\n",
      "predict: 563\n",
      "predict: 564\n",
      "predict: 565\n",
      "predict: 566\n",
      "predict: 567\n",
      "predict: 568\n",
      "predict: 569\n",
      "predict: 570\n",
      "predict: 571\n",
      "predict: 572\n",
      "predict: 573\n",
      "predict: 574\n",
      "predict: 575\n",
      "predict: 576\n",
      "predict: 577\n",
      "predict: 578\n",
      "predict: 579\n",
      "predict: 580\n",
      "predict: 581\n",
      "predict: 582\n",
      "predict: 583\n",
      "predict: 584\n",
      "predict: 585\n",
      "predict: 586\n",
      "predict: 587\n",
      "predict: 588\n",
      "predict: 589\n",
      "predict: 590\n",
      "predict: 591\n",
      "predict: 592\n",
      "predict: 593\n",
      "predict: 594\n",
      "predict: 595\n",
      "predict: 596\n",
      "predict: 597\n",
      "predict: 598\n",
      "predict: 599\n",
      "predict: 600\n",
      "predict: 601\n",
      "predict: 602\n",
      "predict: 603\n",
      "predict: 604\n",
      "predict: 605\n",
      "predict: 606\n",
      "predict: 607\n",
      "predict: 608\n",
      "predict: 609\n",
      "predict: 610\n",
      "predict: 611\n",
      "predict: 612\n",
      "predict: 613\n",
      "predict: 614\n",
      "predict: 615\n",
      "predict: 616\n",
      "predict: 617\n",
      "predict: 618\n",
      "predict: 619\n",
      "predict: 620\n",
      "predict: 621\n",
      "predict: 622\n",
      "predict: 623\n",
      "predict: 624\n",
      "predict: 625\n",
      "predict: 626\n",
      "predict: 627\n",
      "predict: 628\n",
      "predict: 629\n",
      "predict: 630\n",
      "predict: 631\n",
      "predict: 632\n",
      "predict: 633\n",
      "predict: 634\n",
      "predict: 635\n",
      "predict: 636\n",
      "predict: 637\n",
      "predict: 638\n",
      "predict: 639\n",
      "predict: 640\n",
      "predict: 641\n",
      "predict: 642\n",
      "predict: 643\n",
      "predict: 644\n",
      "predict: 645\n",
      "predict: 646\n",
      "predict: 647\n",
      "predict: 648\n",
      "predict: 649\n",
      "predict: 650\n",
      "predict: 651\n",
      "predict: 652\n",
      "predict: 653\n",
      "predict: 654\n",
      "predict: 655\n",
      "predict: 656\n",
      "predict: 657\n",
      "predict: 658\n",
      "predict: 659\n",
      "predict: 660\n",
      "predict: 661\n",
      "predict: 662\n",
      "predict: 663\n",
      "predict: 664\n",
      "predict: 665\n",
      "predict: 666\n",
      "predict: 667\n",
      "predict: 668\n",
      "predict: 669\n",
      "predict: 670\n",
      "predict: 671\n",
      "predict: 672\n",
      "predict: 673\n",
      "predict: 674\n",
      "predict: 675\n",
      "predict: 676\n",
      "predict: 677\n",
      "predict: 678\n",
      "predict: 679\n",
      "predict: 680\n",
      "predict: 681\n",
      "predict: 682\n",
      "predict: 683\n",
      "predict: 684\n",
      "predict: 685\n",
      "predict: 686\n",
      "predict: 687\n",
      "predict: 688\n",
      "predict: 689\n",
      "predict: 690\n",
      "predict: 691\n",
      "predict: 692\n",
      "predict: 693\n",
      "predict: 694\n",
      "predict: 695\n",
      "predict: 696\n",
      "predict: 697\n",
      "predict: 698\n",
      "predict: 699\n",
      "predict: 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 701\n",
      "predict: 702\n",
      "predict: 703\n",
      "predict: 704\n",
      "predict: 705\n",
      "predict: 706\n",
      "predict: 707\n",
      "predict: 708\n",
      "predict: 709\n",
      "predict: 710\n",
      "predict: 711\n",
      "predict: 712\n",
      "predict: 713\n",
      "predict: 714\n",
      "predict: 715\n",
      "predict: 716\n",
      "predict: 717\n",
      "predict: 718\n",
      "predict: 719\n",
      "predict: 720\n",
      "predict: 721\n",
      "predict: 722\n",
      "predict: 723\n",
      "predict: 724\n",
      "predict: 725\n",
      "predict: 726\n",
      "predict: 727\n",
      "predict: 728\n",
      "predict: 729\n",
      "predict: 730\n",
      "predict: 731\n",
      "predict: 732\n",
      "predict: 733\n",
      "predict: 734\n",
      "predict: 735\n",
      "predict: 736\n",
      "predict: 737\n",
      "predict: 738\n",
      "predict: 739\n",
      "predict: 740\n",
      "predict: 741\n",
      "predict: 742\n",
      "predict: 743\n",
      "predict: 744\n",
      "predict: 745\n",
      "predict: 746\n",
      "predict: 747\n",
      "predict: 748\n",
      "predict: 749\n",
      "predict: 750\n",
      "predict: 751\n",
      "predict: 752\n",
      "predict: 753\n",
      "predict: 754\n",
      "predict: 755\n",
      "predict: 756\n",
      "predict: 757\n",
      "predict: 758\n",
      "predict: 759\n",
      "predict: 760\n",
      "predict: 761\n",
      "predict: 762\n",
      "predict: 763\n",
      "predict: 764\n",
      "predict: 765\n",
      "predict: 766\n",
      "predict: 767\n",
      "predict: 768\n",
      "predict: 769\n",
      "predict: 770\n",
      "predict: 771\n",
      "predict: 772\n",
      "predict: 773\n",
      "predict: 774\n",
      "predict: 775\n",
      "predict: 776\n",
      "predict: 777\n",
      "predict: 778\n",
      "predict: 779\n",
      "predict: 780\n",
      "predict: 781\n",
      "predict: 782\n",
      "predict: 783\n",
      "predict: 784\n",
      "predict: 785\n",
      "predict: 786\n",
      "predict: 787\n",
      "predict: 788\n",
      "predict: 789\n",
      "predict: 790\n",
      "predict: 791\n",
      "predict: 792\n",
      "predict: 793\n",
      "predict: 794\n",
      "predict: 795\n",
      "predict: 796\n",
      "predict: 797\n",
      "predict: 798\n",
      "predict: 799\n",
      "predict: 800\n",
      "predict: 801\n",
      "predict: 802\n",
      "predict: 803\n",
      "predict: 804\n",
      "predict: 805\n",
      "predict: 806\n",
      "predict: 807\n",
      "predict: 808\n",
      "predict: 809\n",
      "predict: 810\n",
      "predict: 811\n",
      "predict: 812\n",
      "predict: 813\n",
      "predict: 814\n",
      "predict: 815\n",
      "predict: 816\n",
      "predict: 817\n",
      "predict: 818\n",
      "predict: 819\n",
      "predict: 820\n",
      "predict: 821\n",
      "predict: 822\n",
      "predict: 823\n",
      "predict: 824\n",
      "predict: 825\n",
      "predict: 826\n",
      "predict: 827\n",
      "predict: 828\n",
      "predict: 829\n",
      "predict: 830\n",
      "predict: 831\n",
      "predict: 832\n",
      "predict: 833\n",
      "predict: 834\n",
      "predict: 835\n",
      "predict: 836\n",
      "predict: 837\n",
      "predict: 838\n",
      "predict: 839\n",
      "predict: 840\n",
      "predict: 841\n",
      "predict: 842\n",
      "predict: 843\n",
      "predict: 844\n",
      "predict: 845\n",
      "predict: 846\n",
      "predict: 847\n",
      "predict: 848\n",
      "predict: 849\n",
      "predict: 850\n",
      "predict: 851\n",
      "predict: 852\n",
      "predict: 853\n",
      "predict: 854\n",
      "predict: 855\n",
      "predict: 856\n",
      "predict: 857\n",
      "predict: 858\n",
      "predict: 859\n",
      "predict: 860\n",
      "predict: 861\n",
      "predict: 862\n",
      "predict: 863\n",
      "predict: 864\n",
      "predict: 865\n",
      "predict: 866\n",
      "predict: 867\n",
      "predict: 868\n",
      "predict: 869\n",
      "predict: 870\n",
      "predict: 871\n",
      "predict: 872\n",
      "predict: 873\n",
      "predict: 874\n",
      "predict: 875\n",
      "predict: 876\n",
      "predict: 877\n",
      "predict: 878\n",
      "predict: 879\n",
      "predict: 880\n",
      "predict: 881\n",
      "predict: 882\n",
      "predict: 883\n",
      "predict: 884\n",
      "predict: 885\n",
      "predict: 886\n",
      "predict: 887\n",
      "predict: 888\n",
      "predict: 889\n",
      "predict: 890\n",
      "predict: 891\n",
      "predict: 892\n",
      "predict: 893\n",
      "predict: 894\n",
      "predict: 895\n",
      "predict: 896\n",
      "predict: 897\n",
      "predict: 898\n",
      "predict: 899\n",
      "predict: 900\n",
      "predict: 901\n",
      "predict: 902\n",
      "predict: 903\n",
      "predict: 904\n",
      "predict: 905\n",
      "predict: 906\n",
      "predict: 907\n",
      "predict: 908\n",
      "predict: 909\n",
      "predict: 910\n",
      "predict: 911\n",
      "predict: 912\n",
      "predict: 913\n",
      "predict: 914\n",
      "predict: 915\n",
      "predict: 916\n",
      "predict: 917\n",
      "predict: 918\n",
      "predict: 919\n",
      "predict: 920\n",
      "predict: 921\n",
      "predict: 922\n",
      "predict: 923\n",
      "predict: 924\n",
      "predict: 925\n",
      "predict: 926\n",
      "predict: 927\n",
      "predict: 928\n",
      "predict: 929\n",
      "predict: 930\n",
      "predict: 931\n",
      "predict: 932\n",
      "predict: 933\n",
      "predict: 934\n",
      "predict: 935\n",
      "predict: 936\n",
      "predict: 937\n",
      "predict: 938\n",
      "predict: 939\n",
      "predict: 940\n",
      "predict: 941\n",
      "predict: 942\n",
      "predict: 943\n",
      "predict: 944\n",
      "predict: 945\n",
      "predict: 946\n",
      "predict: 947\n",
      "predict: 948\n",
      "predict: 949\n",
      "predict: 950\n",
      "predict: 951\n",
      "predict: 952\n",
      "predict: 953\n",
      "predict: 954\n",
      "predict: 955\n",
      "predict: 956\n",
      "predict: 957\n",
      "predict: 958\n",
      "predict: 959\n",
      "predict: 960\n",
      "predict: 961\n",
      "predict: 962\n",
      "predict: 963\n",
      "predict: 964\n",
      "predict: 965\n",
      "predict: 966\n",
      "predict: 967\n",
      "predict: 968\n",
      "predict: 969\n",
      "predict: 970\n",
      "predict: 971\n",
      "predict: 972\n",
      "predict: 973\n",
      "predict: 974\n",
      "predict: 975\n",
      "predict: 976\n",
      "predict: 977\n",
      "predict: 978\n",
      "predict: 979\n",
      "predict: 980\n",
      "predict: 981\n",
      "predict: 982\n",
      "predict: 983\n",
      "predict: 984\n",
      "predict: 985\n",
      "predict: 986\n",
      "predict: 987\n",
      "predict: 988\n",
      "predict: 989\n",
      "predict: 990\n",
      "predict: 991\n",
      "predict: 992\n",
      "predict: 993\n",
      "predict: 994\n",
      "predict: 995\n",
      "predict: 996\n",
      "predict: 997\n",
      "predict: 998\n",
      "predict: 999\n",
      "predict: 1000\n",
      "predict: 1001\n",
      "predict: 1002\n",
      "predict: 1003\n",
      "predict: 1004\n",
      "predict: 1005\n",
      "predict: 1006\n",
      "predict: 1007\n",
      "predict: 1008\n",
      "predict: 1009\n",
      "predict: 1010\n",
      "predict: 1011\n",
      "predict: 1012\n",
      "predict: 1013\n",
      "predict: 1014\n",
      "predict: 1015\n",
      "predict: 1016\n",
      "predict: 1017\n",
      "predict: 1018\n",
      "predict: 1019\n",
      "predict: 1020\n",
      "predict: 1021\n",
      "predict: 1022\n",
      "predict: 1023\n",
      "predict: 1024\n",
      "predict: 1025\n",
      "predict: 1026\n",
      "predict: 1027\n",
      "predict: 1028\n",
      "predict: 1029\n",
      "predict: 1030\n",
      "predict: 1031\n",
      "predict: 1032\n",
      "predict: 1033\n",
      "predict: 1034\n",
      "predict: 1035\n",
      "predict: 1036\n",
      "predict: 1037\n",
      "predict: 1038\n",
      "predict: 1039\n",
      "predict: 1040\n",
      "predict: 1041\n",
      "predict: 1042\n",
      "predict: 1043\n",
      "predict: 1044\n",
      "predict: 1045\n",
      "predict: 1046\n",
      "predict: 1047\n",
      "predict: 1048\n",
      "predict: 1049\n",
      "predict: 1050\n",
      "predict: 1051\n",
      "predict: 1052\n",
      "predict: 1053\n",
      "predict: 1054\n",
      "predict: 1055\n",
      "predict: 1056\n",
      "predict: 1057\n",
      "predict: 1058\n",
      "predict: 1059\n",
      "predict: 1060\n",
      "predict: 1061\n",
      "predict: 1062\n",
      "predict: 1063\n",
      "predict: 1064\n",
      "predict: 1065\n",
      "predict: 1066\n",
      "predict: 1067\n",
      "predict: 1068\n",
      "predict: 1069\n",
      "predict: 1070\n",
      "predict: 1071\n",
      "predict: 1072\n",
      "predict: 1073\n",
      "predict: 1074\n",
      "predict: 1075\n",
      "predict: 1076\n",
      "predict: 1077\n",
      "predict: 1078\n",
      "predict: 1079\n",
      "predict: 1080\n",
      "predict: 1081\n",
      "predict: 1082\n",
      "predict: 1083\n",
      "predict: 1084\n",
      "predict: 1085\n",
      "predict: 1086\n",
      "predict: 1087\n",
      "predict: 1088\n",
      "predict: 1089\n",
      "predict: 1090\n",
      "predict: 1091\n",
      "predict: 1092\n",
      "predict: 1093\n",
      "predict: 1094\n",
      "predict: 1095\n",
      "predict: 1096\n",
      "predict: 1097\n",
      "predict: 1098\n",
      "predict: 1099\n",
      "predict: 1100\n",
      "predict: 1101\n",
      "predict: 1102\n",
      "predict: 1103\n",
      "predict: 1104\n",
      "predict: 1105\n",
      "predict: 1106\n",
      "predict: 1107\n",
      "predict: 1108\n",
      "predict: 1109\n",
      "predict: 1110\n",
      "predict: 1111\n",
      "predict: 1112\n",
      "predict: 1113\n",
      "predict: 1114\n",
      "predict: 1115\n",
      "predict: 1116\n",
      "predict: 1117\n",
      "predict: 1118\n",
      "predict: 1119\n",
      "predict: 1120\n",
      "predict: 1121\n",
      "predict: 1122\n",
      "predict: 1123\n",
      "predict: 1124\n",
      "predict: 1125\n",
      "predict: 1126\n",
      "predict: 1127\n",
      "predict: 1128\n",
      "predict: 1129\n",
      "predict: 1130\n",
      "predict: 1131\n",
      "predict: 1132\n",
      "predict: 1133\n",
      "predict: 1134\n",
      "predict: 1135\n",
      "predict: 1136\n",
      "predict: 1137\n",
      "predict: 1138\n",
      "predict: 1139\n",
      "predict: 1140\n",
      "predict: 1141\n",
      "predict: 1142\n",
      "predict: 1143\n",
      "predict: 1144\n",
      "predict: 1145\n",
      "predict: 1146\n",
      "predict: 1147\n",
      "predict: 1148\n",
      "predict: 1149\n",
      "predict: 1150\n",
      "predict: 1151\n",
      "predict: 1152\n",
      "predict: 1153\n",
      "predict: 1154\n",
      "predict: 1155\n",
      "predict: 1156\n",
      "predict: 1157\n",
      "predict: 1158\n",
      "predict: 1159\n",
      "predict: 1160\n",
      "predict: 1161\n",
      "predict: 1162\n",
      "predict: 1163\n",
      "predict: 1164\n",
      "predict: 1165\n",
      "predict: 1166\n",
      "predict: 1167\n",
      "predict: 1168\n",
      "predict: 1169\n",
      "predict: 1170\n",
      "predict: 1171\n",
      "predict: 1172\n",
      "predict: 1173\n",
      "predict: 1174\n",
      "predict: 1175\n",
      "predict: 1176\n",
      "predict: 1177\n",
      "predict: 1178\n",
      "predict: 1179\n",
      "predict: 1180\n",
      "predict: 1181\n",
      "predict: 1182\n",
      "predict: 1183\n",
      "predict: 1184\n",
      "predict: 1185\n",
      "predict: 1186\n",
      "predict: 1187\n",
      "predict: 1188\n",
      "predict: 1189\n",
      "predict: 1190\n",
      "predict: 1191\n",
      "predict: 1192\n",
      "predict: 1193\n",
      "predict: 1194\n",
      "predict: 1195\n",
      "predict: 1196\n",
      "predict: 1197\n",
      "predict: 1198\n",
      "predict: 1199\n",
      "predict: 1200\n",
      "predict: 1201\n",
      "predict: 1202\n",
      "predict: 1203\n",
      "predict: 1204\n",
      "predict: 1205\n",
      "predict: 1206\n",
      "predict: 1207\n",
      "predict: 1208\n",
      "predict: 1209\n",
      "predict: 1210\n",
      "predict: 1211\n",
      "predict: 1212\n",
      "predict: 1213\n",
      "predict: 1214\n",
      "predict: 1215\n",
      "predict: 1216\n",
      "predict: 1217\n",
      "predict: 1218\n",
      "predict: 1219\n",
      "predict: 1220\n",
      "predict: 1221\n",
      "predict: 1222\n",
      "predict: 1223\n",
      "predict: 1224\n",
      "predict: 1225\n",
      "predict: 1226\n",
      "predict: 1227\n",
      "predict: 1228\n",
      "predict: 1229\n",
      "predict: 1230\n",
      "predict: 1231\n",
      "predict: 1232\n",
      "predict: 1233\n",
      "predict: 1234\n",
      "predict: 1235\n",
      "predict: 1236\n",
      "predict: 1237\n",
      "predict: 1238\n",
      "predict: 1239\n",
      "predict: 1240\n",
      "predict: 1241\n",
      "predict: 1242\n",
      "predict: 1243\n",
      "predict: 1244\n",
      "predict: 1245\n",
      "predict: 1246\n",
      "predict: 1247\n",
      "predict: 1248\n",
      "predict: 1249\n",
      "predict: 1250\n",
      "predict: 1251\n",
      "predict: 1252\n",
      "predict: 1253\n",
      "predict: 1254\n",
      "predict: 1255\n",
      "predict: 1256\n",
      "predict: 1257\n",
      "predict: 1258\n",
      "predict: 1259\n",
      "predict: 1260\n",
      "predict: 1261\n",
      "predict: 1262\n",
      "predict: 1263\n",
      "predict: 1264\n",
      "predict: 1265\n",
      "predict: 1266\n",
      "predict: 1267\n",
      "predict: 1268\n",
      "predict: 1269\n",
      "predict: 1270\n",
      "predict: 1271\n",
      "predict: 1272\n",
      "predict: 1273\n",
      "predict: 1274\n",
      "predict: 1275\n",
      "predict: 1276\n",
      "predict: 1277\n",
      "predict: 1278\n",
      "predict: 1279\n",
      "predict: 1280\n",
      "predict: 1281\n",
      "predict: 1282\n",
      "predict: 1283\n",
      "predict: 1284\n",
      "predict: 1285\n",
      "predict: 1286\n",
      "predict: 1287\n",
      "predict: 1288\n",
      "predict: 1289\n",
      "predict: 1290\n",
      "predict: 1291\n",
      "predict: 1292\n",
      "predict: 1293\n",
      "predict: 1294\n",
      "predict: 1295\n",
      "predict: 1296\n",
      "predict: 1297\n",
      "predict: 1298\n",
      "predict: 1299\n",
      "predict: 1300\n",
      "predict: 1301\n",
      "predict: 1302\n",
      "predict: 1303\n",
      "predict: 1304\n",
      "predict: 1305\n",
      "predict: 1306\n",
      "predict: 1307\n",
      "predict: 1308\n",
      "predict: 1309\n",
      "predict: 1310\n",
      "predict: 1311\n",
      "predict: 1312\n",
      "predict: 1313\n",
      "predict: 1314\n",
      "predict: 1315\n",
      "predict: 1316\n",
      "predict: 1317\n",
      "predict: 1318\n",
      "predict: 1319\n",
      "predict: 1320\n",
      "predict: 1321\n",
      "predict: 1322\n",
      "predict: 1323\n",
      "predict: 1324\n",
      "predict: 1325\n",
      "predict: 1326\n",
      "predict: 1327\n",
      "predict: 1328\n",
      "predict: 1329\n",
      "predict: 1330\n",
      "predict: 1331\n",
      "predict: 1332\n",
      "predict: 1333\n",
      "predict: 1334\n",
      "predict: 1335\n",
      "predict: 1336\n",
      "predict: 1337\n",
      "predict: 1338\n",
      "predict: 1339\n",
      "predict: 1340\n",
      "predict: 1341\n",
      "predict: 1342\n",
      "predict: 1343\n",
      "predict: 1344\n",
      "predict: 1345\n",
      "predict: 1346\n",
      "predict: 1347\n",
      "predict: 1348\n",
      "predict: 1349\n",
      "predict: 1350\n",
      "predict: 1351\n",
      "predict: 1352\n",
      "predict: 1353\n",
      "predict: 1354\n",
      "predict: 1355\n",
      "predict: 1356\n",
      "predict: 1357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 1358\n",
      "predict: 1359\n",
      "predict: 1360\n",
      "predict: 1361\n",
      "predict: 1362\n",
      "predict: 1363\n",
      "predict: 1364\n",
      "predict: 1365\n",
      "predict: 1366\n",
      "predict: 1367\n",
      "predict: 1368\n",
      "predict: 1369\n",
      "predict: 1370\n",
      "predict: 1371\n",
      "predict: 1372\n",
      "predict: 1373\n",
      "predict: 1374\n",
      "predict: 1375\n",
      "predict: 1376\n",
      "predict: 1377\n",
      "predict: 1378\n",
      "predict: 1379\n",
      "predict: 1380\n",
      "predict: 1381\n",
      "predict: 1382\n",
      "predict: 1383\n",
      "predict: 1384\n",
      "predict: 1385\n",
      "predict: 1386\n",
      "predict: 1387\n",
      "predict: 1388\n",
      "predict: 1389\n",
      "predict: 1390\n",
      "predict: 1391\n",
      "predict: 1392\n",
      "predict: 1393\n",
      "predict: 1394\n",
      "predict: 1395\n",
      "predict: 1396\n",
      "predict: 1397\n",
      "predict: 1398\n",
      "predict: 1399\n",
      "predict: 1400\n",
      "predict: 1401\n",
      "predict: 1402\n",
      "predict: 1403\n",
      "predict: 1404\n",
      "predict: 1405\n",
      "predict: 1406\n",
      "predict: 1407\n",
      "predict: 1408\n",
      "predict: 1409\n",
      "predict: 1410\n",
      "predict: 1411\n",
      "predict: 1412\n",
      "predict: 1413\n",
      "predict: 1414\n",
      "predict: 1415\n",
      "predict: 1416\n",
      "predict: 1417\n",
      "predict: 1418\n",
      "predict: 1419\n",
      "predict: 1420\n",
      "predict: 1421\n",
      "predict: 1422\n",
      "predict: 1423\n",
      "predict: 1424\n",
      "predict: 1425\n",
      "predict: 1426\n",
      "predict: 1427\n",
      "predict: 1428\n",
      "predict: 1429\n",
      "predict: 1430\n",
      "predict: 1431\n",
      "predict: 1432\n",
      "predict: 1433\n",
      "predict: 1434\n",
      "predict: 1435\n",
      "predict: 1436\n",
      "predict: 1437\n",
      "predict: 1438\n",
      "predict: 1439\n",
      "predict: 1440\n",
      "predict: 1441\n",
      "predict: 1442\n",
      "predict: 1443\n",
      "predict: 1444\n",
      "predict: 1445\n",
      "predict: 1446\n",
      "predict: 1447\n",
      "predict: 1448\n",
      "predict: 1449\n",
      "predict: 1450\n",
      "predict: 1451\n",
      "predict: 1452\n",
      "predict: 1453\n",
      "predict: 1454\n",
      "predict: 1455\n",
      "predict: 1456\n",
      "predict: 1457\n",
      "predict: 1458\n",
      "predict: 1459\n",
      "predict: 1460\n",
      "predict: 1461\n",
      "predict: 1462\n",
      "predict: 1463\n",
      "predict: 1464\n",
      "predict: 1465\n",
      "predict: 1466\n",
      "predict: 1467\n",
      "predict: 1468\n",
      "predict: 1469\n",
      "predict: 1470\n",
      "predict: 1471\n",
      "predict: 1472\n",
      "predict: 1473\n",
      "predict: 1474\n",
      "predict: 1475\n",
      "predict: 1476\n",
      "predict: 1477\n",
      "predict: 1478\n",
      "predict: 1479\n",
      "predict: 1480\n",
      "predict: 1481\n",
      "predict: 1482\n",
      "predict: 1483\n",
      "predict: 1484\n",
      "predict: 1485\n",
      "predict: 1486\n",
      "predict: 1487\n",
      "predict: 1488\n",
      "predict: 1489\n",
      "predict: 1490\n",
      "predict: 1491\n",
      "predict: 1492\n",
      "predict: 1493\n",
      "predict: 1494\n",
      "predict: 1495\n",
      "predict: 1496\n",
      "predict: 1497\n",
      "predict: 1498\n",
      "predict: 1499\n",
      "predict: 1500\n",
      "predict: 1501\n",
      "predict: 1502\n",
      "predict: 1503\n",
      "predict: 1504\n",
      "predict: 1505\n",
      "predict: 1506\n",
      "predict: 1507\n",
      "predict: 1508\n",
      "predict: 1509\n",
      "predict: 1510\n",
      "predict: 1511\n",
      "predict: 1512\n",
      "predict: 1513\n",
      "predict: 1514\n",
      "predict: 1515\n",
      "predict: 1516\n",
      "predict: 1517\n",
      "predict: 1518\n",
      "predict: 1519\n",
      "predict: 1520\n",
      "predict: 1521\n",
      "predict: 1522\n",
      "predict: 1523\n",
      "predict: 1524\n",
      "predict: 1525\n",
      "predict: 1526\n",
      "predict: 1527\n",
      "predict: 1528\n",
      "predict: 1529\n",
      "predict: 1530\n",
      "predict: 1531\n",
      "predict: 1532\n",
      "predict: 1533\n",
      "predict: 1534\n",
      "predict: 1535\n",
      "predict: 1536\n",
      "predict: 1537\n",
      "predict: 1538\n",
      "predict: 1539\n",
      "predict: 1540\n",
      "predict: 1541\n",
      "predict: 1542\n",
      "predict: 1543\n",
      "predict: 1544\n",
      "predict: 1545\n",
      "predict: 1546\n",
      "predict: 1547\n",
      "predict: 1548\n",
      "predict: 1549\n",
      "predict: 1550\n",
      "predict: 1551\n",
      "predict: 1552\n",
      "predict: 1553\n",
      "predict: 1554\n",
      "predict: 1555\n",
      "predict: 1556\n",
      "predict: 1557\n",
      "predict: 1558\n",
      "predict: 1559\n",
      "predict: 1560\n",
      "predict: 1561\n",
      "predict: 1562\n",
      "predict: 1563\n",
      "predict: 1564\n",
      "predict: 1565\n",
      "predict: 1566\n",
      "predict: 1567\n",
      "predict: 1568\n",
      "predict: 1569\n",
      "predict: 1570\n",
      "predict: 1571\n",
      "predict: 1572\n",
      "predict: 1573\n",
      "predict: 1574\n",
      "predict: 1575\n",
      "predict: 1576\n",
      "predict: 1577\n",
      "predict: 1578\n",
      "predict: 1579\n",
      "predict: 1580\n",
      "predict: 1581\n",
      "predict: 1582\n",
      "predict: 1583\n",
      "predict: 1584\n",
      "predict: 1585\n",
      "predict: 1586\n",
      "predict: 1587\n",
      "predict: 1588\n",
      "predict: 1589\n",
      "predict: 1590\n",
      "predict: 1591\n",
      "predict: 1592\n",
      "predict: 1593\n",
      "predict: 1594\n",
      "predict: 1595\n",
      "predict: 1596\n",
      "predict: 1597\n",
      "predict: 1598\n",
      "predict: 1599\n",
      "predict: 1600\n",
      "predict: 1601\n",
      "predict: 1602\n",
      "predict: 1603\n",
      "predict: 1604\n",
      "predict: 1605\n",
      "predict: 1606\n",
      "predict: 1607\n",
      "predict: 1608\n",
      "predict: 1609\n",
      "predict: 1610\n",
      "predict: 1611\n",
      "predict: 1612\n",
      "predict: 1613\n",
      "predict: 1614\n",
      "predict: 1615\n",
      "predict: 1616\n",
      "predict: 1617\n",
      "predict: 1618\n",
      "predict: 1619\n",
      "predict: 1620\n",
      "predict: 1621\n",
      "predict: 1622\n",
      "predict: 1623\n",
      "predict: 1624\n",
      "predict: 1625\n",
      "predict: 1626\n",
      "predict: 1627\n",
      "predict: 1628\n",
      "predict: 1629\n",
      "predict: 1630\n",
      "predict: 1631\n",
      "predict: 1632\n",
      "predict: 1633\n",
      "predict: 1634\n",
      "predict: 1635\n",
      "predict: 1636\n",
      "predict: 1637\n",
      "predict: 1638\n",
      "predict: 1639\n",
      "predict: 1640\n",
      "predict: 1641\n",
      "predict: 1642\n",
      "predict: 1643\n",
      "predict: 1644\n",
      "predict: 1645\n",
      "predict: 1646\n",
      "predict: 1647\n",
      "predict: 1648\n",
      "predict: 1649\n",
      "predict: 1650\n",
      "predict: 1651\n",
      "predict: 1652\n",
      "predict: 1653\n",
      "predict: 1654\n",
      "predict: 1655\n",
      "predict: 1656\n",
      "predict: 1657\n",
      "predict: 1658\n",
      "predict: 1659\n",
      "predict: 1660\n",
      "predict: 1661\n",
      "predict: 1662\n",
      "predict: 1663\n",
      "predict: 1664\n",
      "predict: 1665\n",
      "predict: 1666\n",
      "predict: 1667\n",
      "predict: 1668\n",
      "predict: 1669\n",
      "predict: 1670\n",
      "predict: 1671\n",
      "predict: 1672\n",
      "predict: 1673\n",
      "predict: 1674\n",
      "predict: 1675\n",
      "predict: 1676\n",
      "predict: 1677\n",
      "predict: 1678\n",
      "predict: 1679\n",
      "predict: 1680\n",
      "predict: 1681\n",
      "predict: 1682\n",
      "predict: 1683\n",
      "predict: 1684\n",
      "predict: 1685\n",
      "predict: 1686\n",
      "predict: 1687\n",
      "predict: 1688\n",
      "predict: 1689\n",
      "predict: 1690\n",
      "predict: 1691\n",
      "predict: 1692\n",
      "predict: 1693\n",
      "predict: 1694\n",
      "predict: 1695\n",
      "predict: 1696\n",
      "predict: 1697\n",
      "predict: 1698\n",
      "predict: 1699\n",
      "predict: 1700\n",
      "predict: 1701\n",
      "predict: 1702\n",
      "predict: 1703\n",
      "predict: 1704\n",
      "predict: 1705\n",
      "predict: 1706\n",
      "predict: 1707\n",
      "predict: 1708\n",
      "predict: 1709\n",
      "predict: 1710\n",
      "predict: 1711\n",
      "predict: 1712\n",
      "predict: 1713\n",
      "predict: 1714\n",
      "predict: 1715\n",
      "predict: 1716\n",
      "predict: 1717\n",
      "predict: 1718\n",
      "predict: 1719\n",
      "predict: 1720\n",
      "predict: 1721\n",
      "predict: 1722\n",
      "predict: 1723\n",
      "predict: 1724\n",
      "predict: 1725\n",
      "predict: 1726\n",
      "predict: 1727\n",
      "predict: 1728\n",
      "predict: 1729\n",
      "predict: 1730\n",
      "predict: 1731\n",
      "predict: 1732\n",
      "predict: 1733\n",
      "predict: 1734\n",
      "predict: 1735\n",
      "predict: 1736\n",
      "predict: 1737\n",
      "predict: 1738\n",
      "predict: 1739\n",
      "predict: 1740\n",
      "predict: 1741\n",
      "predict: 1742\n",
      "predict: 1743\n",
      "predict: 1744\n",
      "predict: 1745\n",
      "predict: 1746\n",
      "predict: 1747\n",
      "predict: 1748\n",
      "predict: 1749\n",
      "predict: 1750\n",
      "predict: 1751\n",
      "predict: 1752\n",
      "predict: 1753\n",
      "predict: 1754\n",
      "predict: 1755\n",
      "predict: 1756\n",
      "predict: 1757\n",
      "predict: 1758\n",
      "predict: 1759\n",
      "predict: 1760\n",
      "predict: 1761\n",
      "predict: 1762\n",
      "predict: 1763\n",
      "predict: 1764\n",
      "predict: 1765\n",
      "predict: 1766\n",
      "predict: 1767\n",
      "predict: 1768\n",
      "predict: 1769\n",
      "predict: 1770\n",
      "predict: 1771\n",
      "predict: 1772\n",
      "predict: 1773\n",
      "predict: 1774\n",
      "predict: 1775\n",
      "predict: 1776\n",
      "predict: 1777\n",
      "predict: 1778\n",
      "predict: 1779\n",
      "predict: 1780\n",
      "predict: 1781\n",
      "predict: 1782\n",
      "predict: 1783\n",
      "predict: 1784\n",
      "predict: 1785\n",
      "predict: 1786\n",
      "predict: 1787\n",
      "predict: 1788\n",
      "predict: 1789\n",
      "predict: 1790\n",
      "predict: 1791\n",
      "predict: 1792\n",
      "predict: 1793\n",
      "predict: 1794\n",
      "predict: 1795\n",
      "predict: 1796\n",
      "predict: 1797\n",
      "predict: 1798\n",
      "predict: 1799\n",
      "predict: 1800\n",
      "predict: 1801\n",
      "predict: 1802\n",
      "predict: 1803\n",
      "predict: 1804\n",
      "predict: 1805\n",
      "predict: 1806\n",
      "predict: 1807\n",
      "predict: 1808\n",
      "predict: 1809\n",
      "predict: 1810\n",
      "predict: 1811\n",
      "predict: 1812\n",
      "predict: 1813\n",
      "predict: 1814\n",
      "predict: 1815\n",
      "predict: 1816\n",
      "predict: 1817\n",
      "predict: 1818\n",
      "predict: 1819\n",
      "predict: 1820\n",
      "predict: 1821\n",
      "predict: 1822\n",
      "predict: 1823\n",
      "predict: 1824\n",
      "predict: 1825\n",
      "predict: 1826\n",
      "predict: 1827\n",
      "predict: 1828\n",
      "predict: 1829\n",
      "predict: 1830\n",
      "predict: 1831\n",
      "predict: 1832\n",
      "predict: 1833\n",
      "predict: 1834\n",
      "predict: 1835\n",
      "predict: 1836\n",
      "predict: 1837\n",
      "predict: 1838\n",
      "predict: 1839\n",
      "predict: 1840\n",
      "predict: 1841\n",
      "predict: 1842\n",
      "predict: 1843\n",
      "predict: 1844\n",
      "predict: 1845\n",
      "predict: 1846\n",
      "predict: 1847\n",
      "predict: 1848\n",
      "predict: 1849\n",
      "predict: 1850\n",
      "predict: 1851\n",
      "predict: 1852\n",
      "predict: 1853\n",
      "predict: 1854\n",
      "predict: 1855\n",
      "predict: 1856\n",
      "predict: 1857\n",
      "predict: 1858\n",
      "predict: 1859\n",
      "predict: 1860\n",
      "predict: 1861\n",
      "predict: 1862\n",
      "predict: 1863\n",
      "predict: 1864\n",
      "predict: 1865\n",
      "predict: 1866\n",
      "predict: 1867\n",
      "predict: 1868\n",
      "predict: 1869\n",
      "predict: 1870\n",
      "predict: 1871\n",
      "predict: 1872\n",
      "predict: 1873\n",
      "predict: 1874\n",
      "predict: 1875\n",
      "predict: 1876\n",
      "predict: 1877\n",
      "predict: 1878\n",
      "predict: 1879\n",
      "predict: 1880\n",
      "predict: 1881\n",
      "predict: 1882\n",
      "predict: 1883\n",
      "predict: 1884\n",
      "predict: 1885\n",
      "predict: 1886\n",
      "predict: 1887\n",
      "predict: 1888\n",
      "predict: 1889\n",
      "predict: 1890\n",
      "predict: 1891\n",
      "predict: 1892\n",
      "predict: 1893\n",
      "predict: 1894\n",
      "predict: 1895\n",
      "predict: 1896\n",
      "predict: 1897\n",
      "predict: 1898\n",
      "predict: 1899\n",
      "predict: 1900\n",
      "predict: 1901\n",
      "predict: 1902\n",
      "predict: 1903\n",
      "predict: 1904\n",
      "predict: 1905\n",
      "predict: 1906\n",
      "predict: 1907\n",
      "predict: 1908\n",
      "predict: 1909\n",
      "predict: 1910\n",
      "predict: 1911\n",
      "predict: 1912\n",
      "predict: 1913\n",
      "predict: 1914\n",
      "predict: 1915\n",
      "predict: 1916\n",
      "predict: 1917\n",
      "predict: 1918\n",
      "predict: 1919\n",
      "predict: 1920\n",
      "predict: 1921\n",
      "predict: 1922\n",
      "predict: 1923\n",
      "predict: 1924\n",
      "predict: 1925\n",
      "predict: 1926\n",
      "predict: 1927\n",
      "predict: 1928\n",
      "predict: 1929\n",
      "predict: 1930\n",
      "predict: 1931\n",
      "predict: 1932\n",
      "predict: 1933\n",
      "predict: 1934\n",
      "predict: 1935\n",
      "predict: 1936\n",
      "predict: 1937\n",
      "predict: 1938\n",
      "predict: 1939\n",
      "predict: 1940\n",
      "predict: 1941\n",
      "predict: 1942\n",
      "predict: 1943\n",
      "predict: 1944\n",
      "predict: 1945\n",
      "predict: 1946\n",
      "predict: 1947\n",
      "predict: 1948\n",
      "predict: 1949\n",
      "predict: 1950\n",
      "predict: 1951\n",
      "predict: 1952\n",
      "predict: 1953\n",
      "predict: 1954\n",
      "predict: 1955\n",
      "predict: 1956\n",
      "predict: 1957\n",
      "predict: 1958\n",
      "predict: 1959\n",
      "predict: 1960\n",
      "predict: 1961\n",
      "predict: 1962\n",
      "predict: 1963\n",
      "predict: 1964\n",
      "predict: 1965\n",
      "predict: 1966\n",
      "predict: 1967\n",
      "predict: 1968\n",
      "predict: 1969\n",
      "predict: 1970\n",
      "predict: 1971\n",
      "predict: 1972\n",
      "predict: 1973\n",
      "predict: 1974\n",
      "predict: 1975\n",
      "predict: 1976\n",
      "predict: 1977\n",
      "predict: 1978\n",
      "predict: 1979\n",
      "predict: 1980\n",
      "predict: 1981\n",
      "predict: 1982\n",
      "predict: 1983\n",
      "predict: 1984\n",
      "predict: 1985\n",
      "predict: 1986\n",
      "predict: 1987\n",
      "predict: 1988\n",
      "predict: 1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 1990\n",
      "predict: 1991\n",
      "predict: 1992\n",
      "predict: 1993\n",
      "predict: 1994\n",
      "predict: 1995\n",
      "predict: 1996\n",
      "predict: 1997\n",
      "predict: 1998\n",
      "predict: 1999\n",
      "predict: 2000\n",
      "predict: 2001\n",
      "predict: 2002\n",
      "predict: 2003\n",
      "predict: 2004\n",
      "predict: 2005\n",
      "predict: 2006\n",
      "predict: 2007\n",
      "predict: 2008\n",
      "predict: 2009\n",
      "predict: 2010\n",
      "predict: 2011\n",
      "predict: 2012\n",
      "predict: 2013\n",
      "predict: 2014\n",
      "predict: 2015\n",
      "predict: 2016\n",
      "predict: 2017\n",
      "predict: 2018\n",
      "predict: 2019\n",
      "predict: 2020\n",
      "predict: 2021\n",
      "predict: 2022\n",
      "predict: 2023\n",
      "predict: 2024\n",
      "predict: 2025\n",
      "predict: 2026\n",
      "predict: 2027\n",
      "predict: 2028\n",
      "predict: 2029\n",
      "predict: 2030\n",
      "predict: 2031\n",
      "predict: 2032\n",
      "predict: 2033\n",
      "predict: 2034\n",
      "predict: 2035\n",
      "predict: 2036\n",
      "predict: 2037\n",
      "predict: 2038\n",
      "predict: 2039\n",
      "predict: 2040\n",
      "predict: 2041\n",
      "predict: 2042\n",
      "predict: 2043\n",
      "predict: 2044\n",
      "predict: 2045\n",
      "predict: 2046\n",
      "predict: 2047\n",
      "predict: 2048\n",
      "predict: 2049\n",
      "predict: 2050\n",
      "predict: 2051\n",
      "predict: 2052\n",
      "predict: 2053\n",
      "predict: 2054\n",
      "predict: 2055\n",
      "predict: 2056\n",
      "predict: 2057\n",
      "predict: 2058\n",
      "predict: 2059\n",
      "predict: 2060\n",
      "predict: 2061\n",
      "predict: 2062\n",
      "predict: 2063\n",
      "predict: 2064\n",
      "predict: 2065\n",
      "predict: 2066\n",
      "predict: 2067\n",
      "predict: 2068\n",
      "predict: 2069\n",
      "predict: 2070\n",
      "predict: 2071\n",
      "predict: 2072\n",
      "predict: 2073\n",
      "predict: 2074\n",
      "predict: 2075\n",
      "predict: 2076\n",
      "predict: 2077\n",
      "predict: 2078\n",
      "predict: 2079\n",
      "predict: 2080\n",
      "predict: 2081\n",
      "predict: 2082\n",
      "predict: 2083\n",
      "predict: 2084\n",
      "predict: 2085\n",
      "predict: 2086\n",
      "predict: 2087\n",
      "predict: 2088\n",
      "predict: 2089\n",
      "predict: 2090\n",
      "predict: 2091\n",
      "predict: 2092\n",
      "predict: 2093\n",
      "predict: 2094\n",
      "predict: 2095\n",
      "predict: 2096\n",
      "predict: 2097\n",
      "predict: 2098\n",
      "predict: 2099\n",
      "predict: 2100\n",
      "predict: 2101\n",
      "predict: 2102\n",
      "predict: 2103\n",
      "predict: 2104\n",
      "predict: 2105\n",
      "predict: 2106\n",
      "predict: 2107\n",
      "predict: 2108\n",
      "predict: 2109\n",
      "predict: 2110\n",
      "predict: 2111\n",
      "predict: 2112\n",
      "predict: 2113\n",
      "predict: 2114\n",
      "predict: 2115\n",
      "predict: 2116\n",
      "predict: 2117\n",
      "predict: 2118\n",
      "predict: 2119\n",
      "predict: 2120\n",
      "predict: 2121\n",
      "predict: 2122\n",
      "predict: 2123\n",
      "predict: 2124\n",
      "predict: 2125\n",
      "predict: 2126\n",
      "predict: 2127\n",
      "predict: 2128\n",
      "predict: 2129\n",
      "predict: 2130\n",
      "predict: 2131\n",
      "predict: 2132\n",
      "predict: 2133\n",
      "predict: 2134\n",
      "predict: 2135\n",
      "predict: 2136\n",
      "predict: 2137\n",
      "predict: 2138\n",
      "predict: 2139\n",
      "predict: 2140\n",
      "predict: 2141\n",
      "predict: 2142\n",
      "predict: 2143\n",
      "predict: 2144\n",
      "predict: 2145\n",
      "predict: 2146\n",
      "predict: 2147\n",
      "predict: 2148\n",
      "predict: 2149\n",
      "predict: 2150\n",
      "predict: 2151\n",
      "predict: 2152\n",
      "predict: 2153\n",
      "predict: 2154\n",
      "predict: 2155\n",
      "predict: 2156\n",
      "predict: 2157\n",
      "predict: 2158\n",
      "predict: 2159\n",
      "predict: 2160\n",
      "predict: 2161\n",
      "predict: 2162\n",
      "predict: 2163\n",
      "predict: 2164\n",
      "predict: 2165\n",
      "predict: 2166\n",
      "predict: 2167\n",
      "predict: 2168\n",
      "predict: 2169\n",
      "predict: 2170\n",
      "predict: 2171\n",
      "predict: 2172\n",
      "predict: 2173\n",
      "predict: 2174\n",
      "predict: 2175\n",
      "predict: 2176\n",
      "predict: 2177\n",
      "predict: 2178\n",
      "predict: 2179\n",
      "predict: 2180\n",
      "predict: 2181\n",
      "predict: 2182\n",
      "predict: 2183\n",
      "predict: 2184\n",
      "predict: 2185\n",
      "predict: 2186\n",
      "predict: 2187\n",
      "predict: 2188\n",
      "predict: 2189\n",
      "predict: 2190\n",
      "predict: 2191\n",
      "predict: 2192\n",
      "predict: 2193\n",
      "predict: 2194\n",
      "predict: 2195\n",
      "predict: 2196\n",
      "predict: 2197\n",
      "predict: 2198\n",
      "predict: 2199\n",
      "predict: 2200\n",
      "predict: 2201\n",
      "predict: 2202\n",
      "predict: 2203\n",
      "predict: 2204\n",
      "predict: 2205\n",
      "predict: 2206\n",
      "predict: 2207\n",
      "predict: 2208\n",
      "predict: 2209\n",
      "predict: 2210\n",
      "predict: 2211\n",
      "predict: 2212\n",
      "predict: 2213\n",
      "predict: 2214\n",
      "predict: 2215\n",
      "predict: 2216\n",
      "predict: 2217\n",
      "predict: 2218\n",
      "predict: 2219\n",
      "predict: 2220\n",
      "predict: 2221\n",
      "predict: 2222\n",
      "predict: 2223\n",
      "predict: 2224\n",
      "predict: 2225\n",
      "predict: 2226\n",
      "predict: 2227\n",
      "predict: 2228\n",
      "predict: 2229\n",
      "predict: 2230\n",
      "predict: 2231\n",
      "predict: 2232\n",
      "predict: 2233\n",
      "predict: 2234\n",
      "predict: 2235\n",
      "predict: 2236\n",
      "predict: 2237\n",
      "predict: 2238\n",
      "predict: 2239\n",
      "predict: 2240\n",
      "predict: 2241\n",
      "predict: 2242\n",
      "predict: 2243\n",
      "predict: 2244\n",
      "predict: 2245\n",
      "predict: 2246\n",
      "predict: 2247\n",
      "predict: 2248\n",
      "predict: 2249\n",
      "predict: 2250\n",
      "predict: 2251\n",
      "predict: 2252\n",
      "predict: 2253\n",
      "predict: 2254\n",
      "predict: 2255\n",
      "predict: 2256\n",
      "predict: 2257\n",
      "predict: 2258\n",
      "predict: 2259\n",
      "predict: 2260\n",
      "predict: 2261\n",
      "predict: 2262\n",
      "predict: 2263\n",
      "predict: 2264\n",
      "predict: 2265\n",
      "predict: 2266\n",
      "predict: 2267\n",
      "predict: 2268\n",
      "predict: 2269\n",
      "predict: 2270\n",
      "predict: 2271\n",
      "predict: 2272\n",
      "predict: 2273\n",
      "predict: 2274\n",
      "predict: 2275\n",
      "predict: 2276\n",
      "predict: 2277\n",
      "predict: 2278\n",
      "predict: 2279\n",
      "predict: 2280\n",
      "predict: 2281\n",
      "predict: 2282\n",
      "predict: 2283\n",
      "predict: 2284\n",
      "predict: 2285\n",
      "predict: 2286\n",
      "predict: 2287\n",
      "predict: 2288\n",
      "predict: 2289\n",
      "predict: 2290\n",
      "predict: 2291\n",
      "predict: 2292\n",
      "predict: 2293\n",
      "predict: 2294\n",
      "predict: 2295\n",
      "predict: 2296\n",
      "predict: 2297\n",
      "predict: 2298\n",
      "predict: 2299\n",
      "predict: 2300\n",
      "predict: 2301\n",
      "predict: 2302\n",
      "predict: 2303\n",
      "predict: 2304\n",
      "predict: 2305\n",
      "predict: 2306\n",
      "predict: 2307\n",
      "predict: 2308\n",
      "predict: 2309\n",
      "predict: 2310\n",
      "predict: 2311\n",
      "predict: 2312\n",
      "predict: 2313\n",
      "predict: 2314\n",
      "predict: 2315\n",
      "predict: 2316\n",
      "predict: 2317\n",
      "predict: 2318\n",
      "predict: 2319\n",
      "predict: 2320\n",
      "predict: 2321\n",
      "predict: 2322\n",
      "predict: 2323\n",
      "predict: 2324\n",
      "predict: 2325\n",
      "predict: 2326\n",
      "predict: 2327\n",
      "predict: 2328\n",
      "predict: 2329\n",
      "predict: 2330\n",
      "predict: 2331\n",
      "predict: 2332\n",
      "predict: 2333\n",
      "predict: 2334\n",
      "predict: 2335\n",
      "predict: 2336\n",
      "predict: 2337\n",
      "predict: 2338\n",
      "predict: 2339\n",
      "predict: 2340\n",
      "predict: 2341\n",
      "predict: 2342\n",
      "predict: 2343\n",
      "predict: 2344\n",
      "predict: 2345\n",
      "predict: 2346\n",
      "predict: 2347\n",
      "predict: 2348\n",
      "predict: 2349\n",
      "predict: 2350\n",
      "predict: 2351\n",
      "predict: 2352\n",
      "predict: 2353\n",
      "predict: 2354\n",
      "predict: 2355\n",
      "predict: 2356\n",
      "predict: 2357\n",
      "predict: 2358\n",
      "predict: 2359\n",
      "predict: 2360\n",
      "predict: 2361\n",
      "predict: 2362\n",
      "predict: 2363\n",
      "predict: 2364\n",
      "predict: 2365\n",
      "predict: 2366\n",
      "predict: 2367\n",
      "predict: 2368\n",
      "predict: 2369\n",
      "predict: 2370\n",
      "predict: 2371\n",
      "predict: 2372\n",
      "predict: 2373\n",
      "predict: 2374\n",
      "predict: 2375\n",
      "predict: 2376\n",
      "predict: 2377\n",
      "predict: 2378\n",
      "predict: 2379\n",
      "predict: 2380\n",
      "predict: 2381\n",
      "predict: 2382\n",
      "predict: 2383\n",
      "predict: 2384\n",
      "predict: 2385\n",
      "predict: 2386\n",
      "predict: 2387\n",
      "predict: 2388\n",
      "predict: 2389\n",
      "predict: 2390\n",
      "predict: 2391\n",
      "predict: 2392\n",
      "predict: 2393\n",
      "predict: 2394\n",
      "predict: 2395\n",
      "predict: 2396\n",
      "predict: 2397\n",
      "predict: 2398\n",
      "predict: 2399\n",
      "predict: 2400\n",
      "predict: 2401\n",
      "predict: 2402\n",
      "predict: 2403\n",
      "predict: 2404\n",
      "predict: 2405\n",
      "predict: 2406\n",
      "predict: 2407\n",
      "predict: 2408\n",
      "predict: 2409\n",
      "predict: 2410\n",
      "predict: 2411\n",
      "predict: 2412\n",
      "predict: 2413\n",
      "predict: 2414\n",
      "predict: 2415\n",
      "predict: 2416\n",
      "predict: 2417\n",
      "predict: 2418\n",
      "predict: 2419\n",
      "predict: 2420\n",
      "predict: 2421\n",
      "predict: 2422\n",
      "predict: 2423\n",
      "predict: 2424\n",
      "predict: 2425\n",
      "predict: 2426\n",
      "predict: 2427\n",
      "predict: 2428\n",
      "predict: 2429\n",
      "predict: 2430\n",
      "predict: 2431\n",
      "predict: 2432\n",
      "predict: 2433\n",
      "predict: 2434\n",
      "predict: 2435\n",
      "predict: 2436\n",
      "predict: 2437\n",
      "predict: 2438\n",
      "predict: 2439\n",
      "predict: 2440\n",
      "predict: 2441\n",
      "predict: 2442\n",
      "predict: 2443\n",
      "predict: 2444\n",
      "predict: 2445\n",
      "predict: 2446\n",
      "predict: 2447\n",
      "predict: 2448\n",
      "predict: 2449\n",
      "predict: 2450\n",
      "predict: 2451\n",
      "predict: 2452\n",
      "predict: 2453\n",
      "predict: 2454\n",
      "predict: 2455\n",
      "predict: 2456\n",
      "predict: 2457\n",
      "predict: 2458\n",
      "predict: 2459\n",
      "predict: 2460\n",
      "predict: 2461\n",
      "predict: 2462\n",
      "predict: 2463\n",
      "predict: 2464\n",
      "predict: 2465\n",
      "predict: 2466\n",
      "predict: 2467\n",
      "predict: 2468\n",
      "predict: 2469\n",
      "predict: 2470\n",
      "predict: 2471\n",
      "predict: 2472\n",
      "predict: 2473\n",
      "predict: 2474\n",
      "predict: 2475\n",
      "predict: 2476\n",
      "predict: 2477\n",
      "predict: 2478\n",
      "predict: 2479\n",
      "predict: 2480\n",
      "predict: 2481\n",
      "predict: 2482\n",
      "predict: 2483\n",
      "predict: 2484\n",
      "predict: 2485\n",
      "predict: 2486\n",
      "predict: 2487\n",
      "predict: 2488\n",
      "predict: 2489\n",
      "predict: 2490\n",
      "predict: 2491\n",
      "predict: 2492\n",
      "predict: 2493\n",
      "predict: 2494\n",
      "predict: 2495\n",
      "predict: 2496\n",
      "predict: 2497\n",
      "predict: 2498\n",
      "predict: 2499\n",
      "predict: 2500\n",
      "predict: 2501\n",
      "predict: 2502\n",
      "predict: 2503\n",
      "predict: 2504\n",
      "predict: 2505\n",
      "predict: 2506\n",
      "predict: 2507\n",
      "predict: 2508\n",
      "predict: 2509\n",
      "predict: 2510\n",
      "predict: 2511\n",
      "predict: 2512\n",
      "predict: 2513\n",
      "predict: 2514\n",
      "predict: 2515\n",
      "predict: 2516\n",
      "predict: 2517\n",
      "predict: 2518\n",
      "predict: 2519\n",
      "predict: 2520\n",
      "predict: 2521\n",
      "predict: 2522\n",
      "predict: 2523\n",
      "predict: 2524\n",
      "predict: 2525\n",
      "predict: 2526\n",
      "predict: 2527\n",
      "predict: 2528\n",
      "predict: 2529\n",
      "predict: 2530\n",
      "predict: 2531\n",
      "predict: 2532\n",
      "predict: 2533\n",
      "predict: 2534\n",
      "predict: 2535\n",
      "predict: 2536\n",
      "predict: 2537\n",
      "predict: 2538\n",
      "predict: 2539\n",
      "predict: 2540\n",
      "predict: 2541\n",
      "predict: 2542\n",
      "predict: 2543\n",
      "predict: 2544\n",
      "predict: 2545\n",
      "predict: 2546\n",
      "predict: 2547\n",
      "predict: 2548\n",
      "predict: 2549\n",
      "predict: 2550\n",
      "predict: 2551\n",
      "predict: 2552\n",
      "predict: 2553\n",
      "predict: 2554\n",
      "predict: 2555\n",
      "predict: 2556\n",
      "predict: 2557\n",
      "predict: 2558\n",
      "predict: 2559\n",
      "predict: 2560\n",
      "predict: 2561\n",
      "predict: 2562\n",
      "predict: 2563\n",
      "predict: 2564\n",
      "predict: 2565\n",
      "predict: 2566\n",
      "predict: 2567\n",
      "predict: 2568\n",
      "predict: 2569\n",
      "predict: 2570\n",
      "predict: 2571\n",
      "predict: 2572\n",
      "predict: 2573\n",
      "predict: 2574\n",
      "predict: 2575\n",
      "predict: 2576\n",
      "predict: 2577\n",
      "predict: 2578\n",
      "predict: 2579\n",
      "predict: 2580\n",
      "predict: 2581\n",
      "predict: 2582\n",
      "predict: 2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 2584\n",
      "predict: 2585\n",
      "predict: 2586\n",
      "predict: 2587\n",
      "predict: 2588\n",
      "predict: 2589\n",
      "predict: 2590\n",
      "predict: 2591\n",
      "predict: 2592\n",
      "predict: 2593\n",
      "predict: 2594\n",
      "predict: 2595\n",
      "predict: 2596\n",
      "predict: 2597\n",
      "predict: 2598\n",
      "predict: 2599\n",
      "predict: 2600\n",
      "predict: 2601\n",
      "predict: 2602\n",
      "predict: 2603\n",
      "predict: 2604\n",
      "predict: 2605\n",
      "predict: 2606\n",
      "predict: 2607\n",
      "predict: 2608\n",
      "predict: 2609\n",
      "predict: 2610\n",
      "predict: 2611\n",
      "predict: 2612\n",
      "predict: 2613\n",
      "predict: 2614\n",
      "predict: 2615\n",
      "predict: 2616\n",
      "predict: 2617\n",
      "predict: 2618\n",
      "predict: 2619\n",
      "predict: 2620\n",
      "predict: 2621\n",
      "predict: 2622\n",
      "predict: 2623\n",
      "predict: 2624\n",
      "predict: 2625\n",
      "predict: 2626\n",
      "predict: 2627\n",
      "predict: 2628\n",
      "predict: 2629\n",
      "predict: 2630\n",
      "predict: 2631\n",
      "predict: 2632\n",
      "predict: 2633\n",
      "predict: 2634\n",
      "predict: 2635\n",
      "predict: 2636\n",
      "predict: 2637\n",
      "predict: 2638\n",
      "predict: 2639\n",
      "predict: 2640\n",
      "predict: 2641\n",
      "predict: 2642\n",
      "predict: 2643\n",
      "predict: 2644\n",
      "predict: 2645\n",
      "predict: 2646\n",
      "predict: 2647\n",
      "predict: 2648\n",
      "predict: 2649\n",
      "predict: 2650\n",
      "predict: 2651\n",
      "predict: 2652\n",
      "predict: 2653\n",
      "predict: 2654\n",
      "predict: 2655\n",
      "predict: 2656\n",
      "predict: 2657\n",
      "predict: 2658\n",
      "predict: 2659\n",
      "predict: 2660\n",
      "predict: 2661\n",
      "predict: 2662\n",
      "predict: 2663\n",
      "predict: 2664\n",
      "predict: 2665\n",
      "predict: 2666\n",
      "predict: 2667\n",
      "predict: 2668\n",
      "predict: 2669\n",
      "predict: 2670\n",
      "predict: 2671\n",
      "predict: 2672\n",
      "predict: 2673\n",
      "predict: 2674\n",
      "predict: 2675\n",
      "predict: 2676\n",
      "predict: 2677\n",
      "predict: 2678\n",
      "predict: 2679\n",
      "predict: 2680\n",
      "predict: 2681\n",
      "predict: 2682\n",
      "predict: 2683\n",
      "predict: 2684\n",
      "predict: 2685\n",
      "predict: 2686\n",
      "predict: 2687\n",
      "predict: 2688\n",
      "predict: 2689\n",
      "predict: 2690\n",
      "predict: 2691\n",
      "predict: 2692\n",
      "predict: 2693\n",
      "predict: 2694\n",
      "predict: 2695\n",
      "predict: 2696\n",
      "predict: 2697\n",
      "predict: 2698\n",
      "predict: 2699\n",
      "predict: 2700\n",
      "predict: 2701\n",
      "predict: 2702\n",
      "predict: 2703\n",
      "predict: 2704\n",
      "predict: 2705\n",
      "predict: 2706\n",
      "predict: 2707\n",
      "predict: 2708\n",
      "predict: 2709\n",
      "predict: 2710\n",
      "predict: 2711\n",
      "predict: 2712\n",
      "predict: 2713\n",
      "predict: 2714\n",
      "predict: 2715\n",
      "predict: 2716\n",
      "predict: 2717\n",
      "predict: 2718\n",
      "predict: 2719\n",
      "predict: 2720\n",
      "predict: 2721\n",
      "predict: 2722\n",
      "predict: 2723\n",
      "predict: 2724\n",
      "predict: 2725\n",
      "predict: 2726\n",
      "predict: 2727\n",
      "predict: 2728\n",
      "predict: 2729\n",
      "predict: 2730\n",
      "predict: 2731\n",
      "predict: 2732\n",
      "predict: 2733\n",
      "predict: 2734\n",
      "predict: 2735\n",
      "predict: 2736\n",
      "predict: 2737\n",
      "predict: 2738\n",
      "predict: 2739\n",
      "predict: 2740\n",
      "predict: 2741\n",
      "predict: 2742\n",
      "predict: 2743\n",
      "predict: 2744\n",
      "predict: 2745\n",
      "predict: 2746\n",
      "predict: 2747\n",
      "predict: 2748\n",
      "predict: 2749\n",
      "predict: 2750\n",
      "predict: 2751\n",
      "predict: 2752\n",
      "predict: 2753\n",
      "predict: 2754\n",
      "predict: 2755\n",
      "predict: 2756\n",
      "predict: 2757\n",
      "predict: 2758\n",
      "predict: 2759\n",
      "predict: 2760\n",
      "predict: 2761\n",
      "predict: 2762\n",
      "predict: 2763\n",
      "predict: 2764\n",
      "predict: 2765\n",
      "predict: 2766\n",
      "predict: 2767\n",
      "predict: 2768\n",
      "predict: 2769\n",
      "predict: 2770\n",
      "predict: 2771\n",
      "predict: 2772\n",
      "predict: 2773\n",
      "predict: 2774\n",
      "predict: 2775\n",
      "predict: 2776\n",
      "predict: 2777\n",
      "predict: 2778\n",
      "predict: 2779\n",
      "predict: 2780\n",
      "predict: 2781\n",
      "predict: 2782\n",
      "predict: 2783\n",
      "predict: 2784\n",
      "predict: 2785\n",
      "predict: 2786\n",
      "predict: 2787\n",
      "predict: 2788\n",
      "predict: 2789\n",
      "predict: 2790\n",
      "predict: 2791\n",
      "predict: 2792\n",
      "predict: 2793\n",
      "predict: 2794\n",
      "predict: 2795\n",
      "predict: 2796\n",
      "predict: 2797\n",
      "predict: 2798\n",
      "predict: 2799\n",
      "predict: 2800\n",
      "predict: 2801\n",
      "predict: 2802\n",
      "predict: 2803\n",
      "predict: 2804\n",
      "predict: 2805\n",
      "predict: 2806\n",
      "predict: 2807\n",
      "predict: 2808\n",
      "predict: 2809\n",
      "predict: 2810\n",
      "predict: 2811\n",
      "predict: 2812\n",
      "predict: 2813\n",
      "predict: 2814\n",
      "predict: 2815\n",
      "predict: 2816\n",
      "predict: 2817\n",
      "predict: 2818\n",
      "predict: 2819\n",
      "predict: 2820\n",
      "predict: 2821\n",
      "predict: 2822\n",
      "predict: 2823\n",
      "predict: 2824\n",
      "predict: 2825\n",
      "predict: 2826\n",
      "predict: 2827\n",
      "predict: 2828\n",
      "predict: 2829\n",
      "predict: 2830\n",
      "predict: 2831\n",
      "predict: 2832\n",
      "predict: 2833\n",
      "predict: 2834\n",
      "predict: 2835\n",
      "predict: 2836\n",
      "predict: 2837\n",
      "predict: 2838\n",
      "predict: 2839\n",
      "predict: 2840\n",
      "predict: 2841\n",
      "predict: 2842\n",
      "predict: 2843\n",
      "predict: 2844\n",
      "predict: 2845\n",
      "predict: 2846\n",
      "predict: 2847\n",
      "predict: 2848\n",
      "predict: 2849\n",
      "predict: 2850\n",
      "predict: 2851\n",
      "predict: 2852\n",
      "predict: 2853\n",
      "predict: 2854\n",
      "predict: 2855\n",
      "predict: 2856\n",
      "predict: 2857\n",
      "predict: 2858\n",
      "predict: 2859\n",
      "predict: 2860\n",
      "predict: 2861\n",
      "predict: 2862\n",
      "predict: 2863\n",
      "predict: 2864\n",
      "predict: 2865\n",
      "predict: 2866\n",
      "predict: 2867\n",
      "predict: 2868\n",
      "predict: 2869\n",
      "predict: 2870\n",
      "predict: 2871\n",
      "predict: 2872\n",
      "predict: 2873\n",
      "predict: 2874\n",
      "predict: 2875\n",
      "predict: 2876\n",
      "predict: 2877\n",
      "predict: 2878\n",
      "predict: 2879\n",
      "predict: 2880\n",
      "predict: 2881\n",
      "predict: 2882\n",
      "predict: 2883\n",
      "predict: 2884\n",
      "predict: 2885\n",
      "predict: 2886\n",
      "predict: 2887\n",
      "predict: 2888\n",
      "predict: 2889\n",
      "predict: 2890\n",
      "predict: 2891\n",
      "predict: 2892\n",
      "predict: 2893\n",
      "predict: 2894\n",
      "predict: 2895\n",
      "predict: 2896\n",
      "predict: 2897\n",
      "predict: 2898\n",
      "predict: 2899\n",
      "predict: 2900\n",
      "predict: 2901\n",
      "predict: 2902\n",
      "predict: 2903\n",
      "predict: 2904\n",
      "predict: 2905\n",
      "predict: 2906\n",
      "predict: 2907\n",
      "predict: 2908\n",
      "predict: 2909\n",
      "predict: 2910\n",
      "predict: 2911\n",
      "predict: 2912\n",
      "predict: 2913\n",
      "predict: 2914\n",
      "predict: 2915\n",
      "predict: 2916\n",
      "predict: 2917\n",
      "predict: 2918\n",
      "predict: 2919\n",
      "predict: 2920\n",
      "predict: 2921\n",
      "predict: 2922\n",
      "predict: 2923\n",
      "predict: 2924\n",
      "predict: 2925\n",
      "predict: 2926\n",
      "predict: 2927\n",
      "predict: 2928\n",
      "predict: 2929\n",
      "predict: 2930\n",
      "predict: 2931\n",
      "predict: 2932\n",
      "predict: 2933\n",
      "predict: 2934\n",
      "predict: 2935\n",
      "predict: 2936\n",
      "predict: 2937\n",
      "predict: 2938\n",
      "predict: 2939\n",
      "predict: 2940\n",
      "predict: 2941\n",
      "predict: 2942\n",
      "predict: 2943\n",
      "predict: 2944\n",
      "predict: 2945\n",
      "predict: 2946\n",
      "predict: 2947\n",
      "predict: 2948\n",
      "predict: 2949\n",
      "predict: 2950\n",
      "predict: 2951\n",
      "predict: 2952\n",
      "predict: 2953\n",
      "predict: 2954\n",
      "predict: 2955\n",
      "predict: 2956\n",
      "predict: 2957\n",
      "predict: 2958\n",
      "predict: 2959\n",
      "predict: 2960\n",
      "predict: 2961\n",
      "predict: 2962\n",
      "predict: 2963\n",
      "predict: 2964\n",
      "predict: 2965\n",
      "predict: 2966\n",
      "predict: 2967\n",
      "predict: 2968\n",
      "predict: 2969\n",
      "predict: 2970\n",
      "predict: 2971\n",
      "predict: 2972\n",
      "predict: 2973\n",
      "predict: 2974\n",
      "predict: 2975\n",
      "predict: 2976\n",
      "predict: 2977\n",
      "predict: 2978\n",
      "predict: 2979\n",
      "predict: 2980\n",
      "predict: 2981\n",
      "predict: 2982\n",
      "predict: 2983\n",
      "predict: 2984\n",
      "predict: 2985\n",
      "predict: 2986\n",
      "predict: 2987\n",
      "predict: 2988\n",
      "predict: 2989\n",
      "predict: 2990\n",
      "predict: 2991\n",
      "predict: 2992\n",
      "predict: 2993\n",
      "predict: 2994\n",
      "predict: 2995\n",
      "predict: 2996\n",
      "predict: 2997\n",
      "predict: 2998\n",
      "predict: 2999\n",
      "predict: 3000\n"
     ]
    }
   ],
   "source": [
    "X_Train_sdf[\"Grade\"] = train_grade_values\n",
    "ensamble_dframe = pd.DataFrame()\n",
    "for i in range(0,3001):\n",
    "    print(\"predict:\",i)\n",
    "    ssample = X_Train_sdf.sample(frac=0.65, replace=True)\n",
    "    sampleX = ssample.drop([\"Grade\"], axis=1)\n",
    "    sampleY = ssample[\"Grade\"]\n",
    "    #print(sampleX.shape)\n",
    "    bregr = Ridge(fit_intercept=True, normalize=True,max_iter=1500 ,alpha=0.3, tol=0.001)\n",
    "    bregr.fit(sampleX, sampleY)\n",
    "    \n",
    "    y_pred2 = bregr.predict(X_Test_sdf)\n",
    "    ensamble_dframe[i] = y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble_predict = ensamble_dframe.mean(axis=1).values\n",
    "# round up gives even better result\n",
    "ensamble_predict = np.ceil(ensamble_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>376</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>375</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>367</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>126</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>332</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>127</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>237</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>305</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>306</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>110</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>208</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>326</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>167</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>192</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>337</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>363</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>290</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>41</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>271</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>251</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>61</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>269</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>244</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>36</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>364</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>198</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>235</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>157</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>386</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>247</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>81</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>314</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>31</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>346</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>46</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>76</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>107</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>146</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>248</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>105</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>73</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>279</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>340</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>213</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>316</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>342</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Grade\n",
       "0    312   10.0\n",
       "1    164   10.0\n",
       "2    245    7.0\n",
       "3    303   11.0\n",
       "4    260   12.0\n",
       "5    376   10.0\n",
       "6     86   12.0\n",
       "7    375    6.0\n",
       "8    367   13.0\n",
       "9      6   14.0\n",
       "10    84   12.0\n",
       "11    35   14.0\n",
       "12   114   12.0\n",
       "13    69   10.0\n",
       "14   126    8.0\n",
       "15   332   12.0\n",
       "16   127   13.0\n",
       "17    77   13.0\n",
       "18   237   10.0\n",
       "19   305    9.0\n",
       "20   306   11.0\n",
       "21   110   14.0\n",
       "22   208   13.0\n",
       "23    21   15.0\n",
       "24    24    9.0\n",
       "25   326   12.0\n",
       "26   167   12.0\n",
       "27   192    6.0\n",
       "28   337   11.0\n",
       "29   363   11.0\n",
       "..   ...    ...\n",
       "167  290   10.0\n",
       "168   41    9.0\n",
       "169  271   10.0\n",
       "170  251    3.0\n",
       "171   61   15.0\n",
       "172  269   13.0\n",
       "173  244   13.0\n",
       "174   36   14.0\n",
       "175  364    6.0\n",
       "176  198   10.0\n",
       "177  235   13.0\n",
       "178  157   10.0\n",
       "179  386   11.0\n",
       "180  247   13.0\n",
       "181   81    9.0\n",
       "182  314   11.0\n",
       "183   31   12.0\n",
       "184  346   11.0\n",
       "185   46   11.0\n",
       "186   76   13.0\n",
       "187  107   11.0\n",
       "188  146   11.0\n",
       "189  248   11.0\n",
       "190  105   12.0\n",
       "191   73   10.0\n",
       "192  279   11.0\n",
       "193  340    9.0\n",
       "194  213    8.0\n",
       "195  316   10.0\n",
       "196  342    6.0\n",
       "\n",
       "[197 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df = pd.DataFrame({'id': test_ID_rows, 'Grade': y_pred_submit})\n",
    "submit_df = submit_df[['id','Grade']]\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "submit_df.to_csv(\"student_grade_prediction.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
