
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ML-exer3}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k+kn}{import} \PY{n+nn}{os}
         
         \PY{c+c1}{\PYZsh{} if you want to use the GPU}
         \PY{c+c1}{\PYZsh{}device = \PYZsq{}gpu\PYZsq{}}
         \PY{c+c1}{\PYZsh{}os.environ[\PYZsq{}THEANO\PYZus{}FLAGS\PYZsq{}]=\PYZsq{}mode=FAST\PYZus{}RUN,device=\PYZsq{} + device + \PYZsq{},floatX=float32\PYZsq{}}
         
         \PY{k+kn}{import} \PY{n+nn}{argparse}
         \PY{k+kn}{import} \PY{n+nn}{csv}
         \PY{k+kn}{import} \PY{n+nn}{datetime}
         \PY{k+kn}{import} \PY{n+nn}{glob}
         \PY{k+kn}{import} \PY{n+nn}{math}
         \PY{k+kn}{import} \PY{n+nn}{sys}
         \PY{k+kn}{import} \PY{n+nn}{time}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
         \PY{k+kn}{from} \PY{n+nn}{theano} \PY{k}{import} \PY{n}{config}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{LabelEncoder}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{misc} \PY{k}{import} \PY{n}{imread}
         
         \PY{k+kn}{import} \PY{n+nn}{keras}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Convolution2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Activation}\PY{p}{,} \PY{n}{Flatten}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{normalization} \PY{k}{import} \PY{n}{BatchNormalization}
         
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} we initialize a random seed here to make the experiments repeatable with same results}
\end{Verbatim}


    \subsubsection{Load the Images from Training
Set}\label{load-the-images-from-training-set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}158}]:} \PY{n}{path}\PY{o}{=}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/home/xxx/MScBI/S4/ML/exer3/data/CarData/TrainImages/}\PY{l+s+s1}{\PYZsq{}}
          \PY{n}{files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*.pgm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Found}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{files}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Found 1050 files

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}159}]:} \PY{n}{images} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{image\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          
          \PY{k}{for} \PY{n}{filename} \PY{o+ow}{in} \PY{n}{files}\PY{p}{:}
              \PY{n}{image\PYZus{}names}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{basename}\PY{p}{(}\PY{n}{filename}\PY{p}{)}\PY{p}{)}
              \PY{k}{with} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{)} \PY{k}{as} \PY{n}{img}\PY{p}{:}
                  \PY{n}{images}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} we convert the images to a Numpy array and store them in a list}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}160}]:} \PY{n}{images}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}160}]:} array([[252, 252, 244, {\ldots},  36,  43,  36],
                 [244, 244, 244, {\ldots},  28,  28,  28],
                 [244, 244, 244, {\ldots},  19,  19,  19],
                 {\ldots},
                 [211, 204, 179, {\ldots}, 179, 179, 179],
                 [204, 204, 195, {\ldots}, 179, 164, 155],
                 [195, 195, 204, {\ldots}, 195, 164, 164]], dtype=uint8)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}161}]:} \PY{c+c1}{\PYZsh{} show a selected image to check}
          \PY{n}{i}\PY{o}{=}\PY{l+m+mi}{100}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{image\PYZus{}names}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
          \PY{n}{Image}\PY{o}{.}\PY{n}{fromarray}\PY{p}{(}\PY{n}{images}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
pos-467.pgm

    \end{Verbatim}
\texttt{\color{outcolor}Out[{\color{outcolor}161}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}162}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{images}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.ndarray'>

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}163}]:} \PY{n}{images}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{shape}   \PY{c+c1}{\PYZsh{} height x width   (Numpy ordering)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}163}]:} (40, 100)
\end{Verbatim}
            
    Note: Color RGB images have an additional dimension of depth 3, e.g.
(40, 100, 3)

    \subsubsection{Make 1 big array of list of
images}\label{make-1-big-array-of-list-of-images}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}164}]:} \PY{c+c1}{\PYZsh{} a list of many 40x100 images is made into 1 big array}
          \PY{c+c1}{\PYZsh{} config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)}
          \PY{n}{img\PYZus{}array} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{images}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{config}\PY{o}{.}\PY{n}{floatX}\PY{p}{)}
          \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}164}]:} (1050, 40, 100)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{n}{img\PYZus{}array}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}165}]:} array([[[188., 171., 219., {\ldots},  27.,  27.,  36.],
                  [211., 171., 219., {\ldots},  36.,  36.,  36.],
                  [227., 227., 180., {\ldots},  60.,  68., 100.],
                  {\ldots},
                  [140., 115., 115., {\ldots},  60.,  36.,  27.],
                  [251., 251., 243., {\ldots},   4.,   4.,  12.],
                  [251., 251., 251., {\ldots}, 203., 203., 196.]],
          
                 [[252., 252., 244., {\ldots},  36.,  43.,  36.],
                  [244., 244., 244., {\ldots},  28.,  28.,  28.],
                  [244., 244., 244., {\ldots},  19.,  19.,  19.],
                  {\ldots},
                  [211., 204., 179., {\ldots}, 179., 179., 179.],
                  [204., 204., 195., {\ldots}, 179., 164., 155.],
                  [195., 195., 204., {\ldots}, 195., 164., 164.]],
          
                 [[229., 221., 219., {\ldots},  20.,  26.,  20.],
                  [221., 216., 218., {\ldots},  18.,  23.,  18.],
                  [224., 226., 222., {\ldots},  18.,  20.,  18.],
                  {\ldots},
                  [116., 109., 116., {\ldots}, 119., 124., 122.],
                  [104., 101., 100., {\ldots}, 112., 112., 109.],
                  [100., 100., 106., {\ldots}, 104., 109., 116.]],
          
                 {\ldots},
          
                 [[202., 182., 167., {\ldots}, 120., 120., 120.],
                  [182., 138.,  52., {\ldots},  75., 138., 120.],
                  [182., 138.,  52., {\ldots},  52., 106., 167.],
                  {\ldots},
                  [240., 240., 242., {\ldots}, 226., 226., 226.],
                  [242., 242., 242., {\ldots}, 231., 226., 226.],
                  [245., 245., 242., {\ldots}, 231., 231., 231.]],
          
                 [[116.,  99., 116., {\ldots},  20.,  75.,  92.],
                  [116., 116.,  92., {\ldots},  12.,  92., 147.],
                  [ 92.,  92.,  51., {\ldots},  27., 147., 172.],
                  {\ldots},
                  [204., 196., 196., {\ldots}, 196., 187., 187.],
                  [196., 196., 196., {\ldots}, 228., 228., 228.],
                  [116., 123., 131., {\ldots}, 139., 131., 131.]],
          
                 [[204., 204., 204., {\ldots}, 155., 179., 187.],
                  [212., 212., 204., {\ldots}, 148., 155., 164.],
                  [212., 204., 187., {\ldots}, 148., 148., 139.],
                  {\ldots},
                  [187., 212., 212., {\ldots},  43.,  43.,  60.],
                  [212., 212., 212., {\ldots},  83.,  76.,  83.],
                  [219., 219., 212., {\ldots}, 212., 212., 212.]]])
\end{Verbatim}
            
    \subsubsection{Create the Groundtruth based on
filenames:}\label{create-the-groundtruth-based-on-filenames}

In this data set, images with cars start with "pos-" and images with no
cars start with "neg-". We create a numeric list here, containing 1 for
car images and 0 for non-car images.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}166}]:} \PY{n}{classes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{k}{for} \PY{n}{name} \PY{o+ow}{in} \PY{n}{image\PYZus{}names}\PY{p}{:}
              \PY{k}{if} \PY{n}{name}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                  \PY{n}{classes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{classes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}167}]:} \PY{c+c1}{\PYZsh{} look at the first 25 classes\PYZsh{} look a }
          \PY{n}{classes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{25}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}167}]:} [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}168}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Groundtruth Statistics:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          \PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n+nb}{set}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{classes}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Groundtruth Statistics:
Class 0 : 500
Class 1 : 550

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}169}]:} \PY{n}{baseline} \PY{o}{=} \PY{l+m+mi}{550} \PY{o}{*} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)}
          \PY{n}{baseline}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}169}]:} 0.5238095238095238
\end{Verbatim}
            
    \subsubsection{Standardization}\label{standardization}

\paragraph{Always standardize the data before feeding it into the Neural
Network!}\label{always-standardize-the-data-before-feeding-it-into-the-neural-network}

Here we use Zero-mean Unit-variance standardization which means we
deduct the mean and divide by the standard deviation. (Note: Here, we do
this "flat", i.e. one mean and std.dev. for the whole image is computed
over all pixels (not per pixel); in RGB images, standardization can be
done e.g. for each colour channel individually; in other/non-image data
sets, attribute-wise standardization should be applied).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}170}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}170}]:} (0.0, 255.0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}171}]:} \PY{n}{mean} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
          \PY{n}{stddev} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
          \PY{n}{mean}\PY{p}{,} \PY{n}{stddev}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}171}]:} (129.58246809523808, 74.27669459504543)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}172}]:} \PY{n}{img\PYZus{}array}  \PY{o}{=} \PY{p}{(}\PY{n}{img\PYZus{}array} \PY{o}{\PYZhy{}} \PY{n}{mean}\PY{p}{)} \PY{o}{/} \PY{n}{stddev}
          \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}172}]:} (1.3948292256654462e-16, 0.9999999999999998)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}173}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}173}]:} (-1.744591204572555, 1.6885179475006928)
\end{Verbatim}
            
    \subsubsection{Creating NN Models in
Keras}\label{creating-nn-models-in-keras}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Fully Connected NN For a fully connected neural network, the x and y
  axis of an image do not play a role at all. All pixels are considered
  as a completely individual input to the neural network. Therefore the
  2D image arrays have to be flattened to a vector.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}180}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}180}]:} (1050, 40, 100)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}174}]:} \PY{c+c1}{\PYZsh{}  flatten images to vectors\PYZsh{}  flatt }
          \PY{n}{images\PYZus{}flat} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{images\PYZus{}flat}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(1050, 4000)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}179}]:} \PY{n}{images\PYZus{}flat}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}179}]:} (4000,)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}175}]:} \PY{c+c1}{\PYZsh{} find out input shape for NN, which is just a long vector (40x100 = 4000)}
          \PY{n}{input\PYZus{}shape} \PY{o}{=} \PY{n}{images\PYZus{}flat}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
4000

    \end{Verbatim}

    \subsubsection{Creating a Model}\label{creating-a-model}

In Keras, one can choose between a Sequential model and a Graph model.
Sequential models are the standard case. Graph models are for parallel
networks and use the functional API (see Music/Speech tutorial).

Here we create a sequential model with 2 fully connected (a.k.a.
'dense') layers containing 256 units each.

The output unit is a Single sigmoid unit which can predict values
between 0 and 1.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}176}]:} \PY{n}{input\PYZus{}shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}176}]:} 4000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{c+c1}{\PYZsh{} simple Fully\PYZhy{}connected network}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_1 (Dense)              (None, 256)               1024256   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 256)               65792     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 1)                 257       
=================================================================
Total params: 1,090,305
Trainable params: 1,090,305
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Defining Loss Function and Optimizer Strategy: Gradient
Descent}\label{defining-loss-function-and-optimizer-strategy-gradient-descent}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{c+c1}{\PYZsh{} Define a loss function }
         \PY{n}{loss} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}  \PY{c+c1}{\PYZsh{} \PYZsq{}categorical\PYZus{}crossentropy\PYZsq{} for multi\PYZhy{}class problems}
         
         \PY{c+c1}{\PYZsh{} Optimizer = Stochastic Gradient Descent}
         \PY{n}{optimizer} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}} 
         
         \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{c+c1}{\PYZsh{} This creates the whole model structure in memory. }
         \PY{c+c1}{\PYZsh{} If you use GPU computation, here GPU compatible structures and code is generated.}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Training the model on the input
dataset}\label{training-the-model-on-the-input-dataset}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}155}]:} \PY{n}{images\PYZus{}flat}\PY{o}{.}\PY{n}{shape}\PY{c+c1}{\PYZsh{}, classes.shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}155}]:} (1050, 4000)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}210}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}210}]:} 1050
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{15}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{images\PYZus{}flat}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{o}{=}\PY{n}{epochs}\PY{p}{)} \PY{c+c1}{\PYZsh{}, validation\PYZus{}data=validation\PYZus{}data)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:2: UserWarning: The `nb\_epoch` argument in `fit` has been renamed `epochs`.
  from ipykernel import kernelapp as app

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/15
1050/1050 [==============================] - 1s 545us/step - loss: 0.4830 - acc: 0.7752
Epoch 2/15
1050/1050 [==============================] - 0s 340us/step - loss: 0.3160 - acc: 0.8600
Epoch 3/15
1050/1050 [==============================] - 0s 320us/step - loss: 0.2468 - acc: 0.8876
Epoch 4/15
1050/1050 [==============================] - 0s 321us/step - loss: 0.1927 - acc: 0.9257
Epoch 5/15
1050/1050 [==============================] - 0s 357us/step - loss: 0.1550 - acc: 0.9371
Epoch 6/15
1050/1050 [==============================] - 0s 331us/step - loss: 0.1318 - acc: 0.9638
Epoch 7/15
1050/1050 [==============================] - 0s 346us/step - loss: 0.1097 - acc: 0.9743
Epoch 8/15
1050/1050 [==============================] - 0s 348us/step - loss: 0.0945 - acc: 0.9743
Epoch 9/15
1050/1050 [==============================] - 0s 353us/step - loss: 0.0762 - acc: 0.9867
Epoch 10/15
1050/1050 [==============================] - 0s 321us/step - loss: 0.0645 - acc: 0.9924
Epoch 11/15
1050/1050 [==============================] - 0s 325us/step - loss: 0.0552 - acc: 0.9914
Epoch 12/15
1050/1050 [==============================] - 0s 345us/step - loss: 0.0476 - acc: 0.9943
Epoch 13/15
1050/1050 [==============================] - 0s 326us/step - loss: 0.0393 - acc: 0.9981
Epoch 14/15
1050/1050 [==============================] - 0s 323us/step - loss: 0.0340 - acc: 0.9990
Epoch 15/15
1050/1050 [==============================] - 0s 318us/step - loss: 0.0297 - acc: 0.9990

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} <keras.callbacks.History at 0x7f9a11cfb0b8>
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{} verify Accuracy on Train set\PYZsh{} verify }
         \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{images\PYZus{}flat}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{classes}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}51}]:} 1.0
\end{Verbatim}
            
    100\% Accuracy - perfect, no? This is the accuracy on the training set.
A (large, especially fully connected network with sufficient number of
units) can easily learn the entire training set (especially a small one
like here).

This very likely leads to overfitting. That's why we test on an
independent test set.

    \subsubsection{Testing with Test Data
Set}\label{testing-with-test-data-set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{path} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/home/xxx/MScBI/S4/ML/exer3/data/CarData/TestImages}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*.pgm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Found}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{files}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Found 170 files

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{c+c1}{\PYZsh{}from resize\PYZus{}and\PYZus{}crop import resize\PYZus{}and\PYZus{}crop}
         \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
         \PY{k+kn}{from} \PY{n+nn}{resizeimage} \PY{k}{import} \PY{n}{resizeimage}
         
         
         \PY{n}{test\PYZus{}images} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{filename} \PY{o+ow}{in} \PY{n}{files}\PY{p}{:}
             \PY{k}{with} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{)} \PY{k}{as} \PY{n}{img}\PY{p}{:}
                 \PY{n}{img\PYZus{}resized} \PY{o}{=} \PY{n}{resizeimage}\PY{o}{.}\PY{n}{resize\PYZus{}cover}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{40}\PY{p}{]}\PY{p}{)}\PY{c+c1}{\PYZsh{}resize\PYZus{}and\PYZus{}crop(img,(100,40))}
                 \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{img\PYZus{}resized}\PY{p}{)}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{}print img.size, img\PYZus{}resized.size}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{i}\PY{o}{=}\PY{l+m+mi}{4}
         \PY{n}{Image}\PY{o}{.}\PY{n}{fromarray}\PY{p}{(}\PY{n}{test\PYZus{}images}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}59}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{} make 1 big array again from list}
         \PY{n}{test\PYZus{}images} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{test\PYZus{}images}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Standardize Test Set}\label{standardize-test-set}

The test data has to be standardized in the same way as the training
data for compatibility with the model! That means, we take the mean and
standard deviation of the training data to transform also the test data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}62}]:} (122.36643676470588, 72.08565062179893)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n}{test\PYZus{}images} \PY{o}{=} \PY{p}{(}\PY{n}{test\PYZus{}images} \PY{o}{\PYZhy{}} \PY{n}{mean}\PY{p}{)} \PY{o}{/} \PY{n}{stddev}
\end{Verbatim}


    \subsubsection{Flatten Images for Full
model}\label{flatten-images-for-full-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{n}{test\PYZus{}images\PYZus{}flat} \PY{o}{=} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}images\PYZus{}flat}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(170, 4000)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{n}{test\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{test\PYZus{}images\PYZus{}flat}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} show 30 first predictions}
         \PY{n}{test\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}66}]:} array([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
                1, 0, 0, 1, 1, 1, 1, 0], dtype=int32)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{} Groundtruth:}
         \PY{c+c1}{\PYZsh{} this TEST SET contains ONLY CARS on images! }
         \PY{c+c1}{\PYZsh{} Thus all the test classes are 1}
         \PY{n}{test\PYZus{}classes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{files}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{c+c1}{\PYZsh{} let\PYZsq{}s count the number of ones ...}
         \PY{n}{test\PYZus{}pred}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}68}]:} 72
\end{Verbatim}
            
    As ALL our test classes are 1, counting the number of 1's and dividing
by number of files gives us the Accuracy:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{n}{test\PYZus{}pred}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{170.0}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}69}]:} 0.4235294117647059
\end{Verbatim}
            
    The real way to do it is to compare the predictions (test\_pred) with
the ground truth (test\_classes) and sum up the correct ones. This is
exactly what the scikit-learn function accuracy\_score does:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{n}{acc} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}classes}\PY{p}{,} \PY{n}{test\PYZus{}pred}\PY{p}{)}
         \PY{n}{acc}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}70}]:} 0.4235294117647059
\end{Verbatim}
            
    Accuracy on the Test Set is rather low.

\subsubsection{Convolutional Neural
Network}\label{convolutional-neural-network}

A Convolutional Neural Network (ConvNet or CNN) is a type of (deep)
Neural Network that is well-suited for 2D axes data, such as images, as
it is optimized for learning from spatial proximity. Its core elements
are 2D filter kernels which essentially learn the weights of the Neural
Network, and downscaling functions such as Max Pooling.

A CNN can have one or more Convolution layers, each of them having an
arbitrary number of N filters (which define the depth of the CNN layer),
following typically by a pooling step, which groups neighboring pixels
together and thus reduces the image resolution by retaining only the
maximum values of neighboring pixels.

Input Our input to the CNN is the standardized version of the original
image array.

Adding the channel For CNNs, we need to add a dimension for the color
channel to the data. RGB images typically have an 3rd dimension with the
color. For greyscale images we need to add an extra dimension for
compatibility with the CNN implementation.

In Theano, traditionally the color channel was the first dimension in
the image shape. In Tensorflow, the color channel is the last dimension
in the image shape.

This can be configured now in \textasciitilde{}/.keras/keras.json:
"image\_dim\_ordering": "th" or "tf" with "tf" (Tensorflow) being the
default image ordering even though you use Theano. Depending on this,
use one of the code lines below.

For greyscale images, we add the number 1 as the depth of the additional
dimension of the input shape (for RGB color images, the number of
channels is 3).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{n}{keras}\PY{o}{.}\PY{n}{backend}\PY{o}{.}\PY{n}{image\PYZus{}dim\PYZus{}ordering}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}72}]:} 'tf'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:} ((1050, 40, 100), (170, 40, 100))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{n}{n\PYZus{}channels} \PY{o}{=} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} for grey\PYZhy{}scale, 3 for RGB, but usually already present in the data}
         
         \PY{k}{if} \PY{n}{keras}\PY{o}{.}\PY{n}{backend}\PY{o}{.}\PY{n}{image\PYZus{}dim\PYZus{}ordering}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{th}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Theano ordering (\PYZti{}/.keras/keras.json: \PYZdq{}image\PYZus{}dim\PYZus{}ordering\PYZdq{}: \PYZdq{}th\PYZdq{})}
             \PY{n}{train\PYZus{}img} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{n\PYZus{}channels}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
             \PY{n}{test\PYZus{}img} \PY{o}{=} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{n\PYZus{}channels}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{k}{else}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Tensorflow ordering (\PYZti{}/.keras/keras.json: \PYZdq{}image\PYZus{}dim\PYZus{}ordering\PYZdq{}: \PYZdq{}tf\PYZdq{})}
             \PY{n}{train\PYZus{}img} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{n\PYZus{}channels}\PY{p}{)}
             \PY{n}{test\PYZus{}img} \PY{o}{=} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}images}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{n\PYZus{}channels}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{train\PYZus{}img}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:} ((1050, 40, 100), (1050, 40, 100, 1))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{} we store the new shape of the images in the \PYZsq{}input\PYZus{}shape\PYZsq{} variable.}
         \PY{c+c1}{\PYZsh{} take all dimensions except the 0th one (which is the number of images)}
             
         \PY{n}{input\PYZus{}shape} \PY{o}{=} \PY{n}{train\PYZus{}img}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}  
         \PY{n}{input\PYZus{}shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}79}]:} (40, 100, 1)
\end{Verbatim}
            
    \subsubsection{Create the CNN model}\label{create-the-cnn-model}

You may try to change the following to see the impact on the result:

number of filters filter kernel size (e.g. 3 x 3, 5 x 5, ...) adding/not
adding Batch Normalization adding/not adding ReLU Activation adding/not
adding Max Pooling changing Pooling size (e.g. 1 x 2, 2 x 2, 2 x 1, or
more) adding/changing/removing Dropout

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{k}{def} \PY{n+nf}{createMyModel}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
             \PY{n}{n\PYZus{}filters} \PY{o}{=} \PY{l+m+mi}{16}
             \PY{c+c1}{\PYZsh{} this applies n\PYZus{}filters convolution filters of size 5x5 resp. 3x3 each in the 2 layers below}
         
             \PY{c+c1}{\PYZsh{} Layer 1}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Convolution2D}\PY{p}{(}\PY{n}{n\PYZus{}filters}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{border\PYZus{}mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} input shape: 100x100 images with 3 channels \PYZhy{}\PYZgt{} input\PYZus{}shape should be (3, 100, 100) }
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} ReLu activation}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} reducing image resolution by half}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} random \PYZdq{}deletion\PYZdq{} of \PYZpc{}\PYZhy{}portion of units in each batch}
         
             \PY{c+c1}{\PYZsh{} Layer 2}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Convolution2D}\PY{p}{(}\PY{n}{n\PYZus{}filters}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} input\PYZus{}shape is only needed in 1st layer}
             \PY{c+c1}{\PYZsh{}model.add(BatchNormalization())}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}model.add(MaxPooling2D(pool\PYZus{}size=(2, 2)))}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Note: Keras does automatic shape inference.}
             
             \PY{c+c1}{\PYZsh{} Full Layer}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n}{model} \PY{o}{=} \PY{n}{createMyModel}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="valid", input\_shape=(40, 100, {\ldots})`

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv2d\_1 (Conv2D)            (None, 38, 98, 16)        160       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_1 (Batch (None, 38, 98, 16)        64        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_1 (Activation)    (None, 38, 98, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_1 (MaxPooling2 (None, 19, 49, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)          (None, 19, 49, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_2 (Conv2D)            (None, 17, 47, 16)        2320      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_2 (Activation)    (None, 17, 47, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_2 (Dropout)          (None, 17, 47, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_1 (Flatten)          (None, 12784)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_4 (Dense)              (None, 256)               3272960   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_3 (Activation)    (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_3 (Dropout)          (None, 256)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_5 (Dense)              (None, 1)                 257       
=================================================================
Total params: 3,275,761
Trainable params: 3,275,729
Non-trainable params: 32
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`

    \end{Verbatim}

    \subsubsection{Training the CNN}\label{training-the-cnn}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{c+c1}{\PYZsh{} Compiling the model}
         \PY{n}{loss} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}} 
         \PY{n}{optimizer} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}} 
         \PY{c+c1}{\PYZsh{}optimizer = SGD(lr=0.001)  \PYZsh{} possibility to adapt the learn rate}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{} TRAINING the model}
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{15}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}img}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{o}{=}\PY{n}{epochs}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:3: UserWarning: The `nb\_epoch` argument in `fit` has been renamed `epochs`.
  app.launch\_new\_instance()

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.3912 - acc: 0.8200
Epoch 2/15
1050/1050 [==============================] - 4s 4ms/step - loss: 0.1541 - acc: 0.9486
Epoch 3/15
1050/1050 [==============================] - 5s 4ms/step - loss: 0.0818 - acc: 0.9771
Epoch 4/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0723 - acc: 0.9752
Epoch 5/15
1050/1050 [==============================] - 5s 4ms/step - loss: 0.0481 - acc: 0.9905
Epoch 6/15
1050/1050 [==============================] - 4s 4ms/step - loss: 0.0403 - acc: 0.9848
Epoch 7/15
1050/1050 [==============================] - 4s 4ms/step - loss: 0.0269 - acc: 0.9933
Epoch 8/15
1050/1050 [==============================] - 4s 4ms/step - loss: 0.0250 - acc: 0.9905
Epoch 9/15
1050/1050 [==============================] - 5s 4ms/step - loss: 0.0180 - acc: 0.9952
Epoch 10/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0165 - acc: 0.9962
Epoch 11/15
1050/1050 [==============================] - 5s 4ms/step - loss: 0.0107 - acc: 0.9981
Epoch 12/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0123 - acc: 0.9981
Epoch 13/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0094 - acc: 0.9971
Epoch 14/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0106 - acc: 0.9971
Epoch 15/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0059 - acc: 0.9990

    \end{Verbatim}

    Again, our Accuracy rises quickly to almost 100\%, with the CNN now even
faster than with the Fully Connected Network. But is our model really
good at predicting unseen data?

    \subsubsection{Training with Validation
Data}\label{training-with-validation-data}

We split off 10 \% of the training data and use it as independend
validation set to verify how good we are on an independent data (not
used for training) in each epoch

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{c+c1}{\PYZsh{} recreate and recompile the model (otherwise we continue learning)}
         \PY{n}{model} \PY{o}{=} \PY{n}{createMyModel}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} train with showing accuracy on split off validation data}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}img}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)} \PY{c+c1}{\PYZsh{} portion of val. data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="valid", input\_shape=(40, 100, {\ldots})`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:6: UserWarning: The `nb\_epoch` argument in `fit` has been renamed `epochs`.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 945 samples, validate on 105 samples
Epoch 1/15
945/945 [==============================] - 5s 5ms/step - loss: 0.4265 - acc: 0.8042 - val\_loss: 0.1043 - val\_acc: 0.9619
Epoch 2/15
945/945 [==============================] - 4s 4ms/step - loss: 0.1219 - acc: 0.9608 - val\_loss: 0.0912 - val\_acc: 0.9619
Epoch 3/15
945/945 [==============================] - 4s 4ms/step - loss: 0.0691 - acc: 0.9820 - val\_loss: 0.0667 - val\_acc: 0.9714
Epoch 4/15
945/945 [==============================] - 5s 6ms/step - loss: 0.0739 - acc: 0.9778 - val\_loss: 0.0487 - val\_acc: 0.9905
Epoch 5/15
945/945 [==============================] - 5s 5ms/step - loss: 0.0502 - acc: 0.9841 - val\_loss: 0.0410 - val\_acc: 0.9905
Epoch 6/15
945/945 [==============================] - 5s 5ms/step - loss: 0.0351 - acc: 0.9905 - val\_loss: 0.0175 - val\_acc: 1.0000
Epoch 7/15
945/945 [==============================] - 5s 5ms/step - loss: 0.0332 - acc: 0.9926 - val\_loss: 0.0387 - val\_acc: 0.9905
Epoch 8/15
945/945 [==============================] - 4s 5ms/step - loss: 0.0262 - acc: 0.9937 - val\_loss: 0.0298 - val\_acc: 0.9905
Epoch 9/15
945/945 [==============================] - 6s 7ms/step - loss: 0.0214 - acc: 0.9947 - val\_loss: 0.0109 - val\_acc: 1.0000
Epoch 10/15
945/945 [==============================] - 6s 6ms/step - loss: 0.0185 - acc: 0.9958 - val\_loss: 0.0148 - val\_acc: 1.0000
Epoch 11/15
945/945 [==============================] - 6s 6ms/step - loss: 0.0109 - acc: 0.9958 - val\_loss: 0.0301 - val\_acc: 0.9905
Epoch 12/15
945/945 [==============================] - 5s 6ms/step - loss: 0.0187 - acc: 0.9915 - val\_loss: 0.0109 - val\_acc: 1.0000
Epoch 13/15
945/945 [==============================] - 6s 6ms/step - loss: 0.0168 - acc: 0.9947 - val\_loss: 0.0183 - val\_acc: 1.0000
Epoch 14/15
945/945 [==============================] - 6s 6ms/step - loss: 0.0162 - acc: 0.9968 - val\_loss: 0.0054 - val\_acc: 1.0000
Epoch 15/15
945/945 [==============================] - 5s 6ms/step - loss: 0.0087 - acc: 0.9979 - val\_loss: 0.0226 - val\_acc: 0.9905

    \end{Verbatim}

    The results on the split-off validation data are quite high (usually
similar, but not as high as on the training data).

    Using Test Set as Validation Set Note: This usually is not recommended
as during experimentation you will overfit also to the test data.

We show it here only for demonstration purposes to see how (bad) the
validation accuracy is on our independet test data. The recommended way
is to have a separate training, validation and test set (i.e. 3 splits
or separate data sets).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{} recreate and recompile the model (otherwise we continue learning)}
         \PY{n}{model} \PY{o}{=} \PY{n}{createMyModel}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} show result on Test Data while training }
         \PY{c+c1}{\PYZsh{} we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)}
         \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{n}{test\PYZus{}img}\PY{p}{,} \PY{n}{test\PYZus{}classes}\PY{p}{)}
         
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}img}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{validation\PYZus{}data}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="valid", input\_shape=(40, 100, {\ldots})`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:9: UserWarning: The `nb\_epoch` argument in `fit` has been renamed `epochs`.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 1050 samples, validate on 170 samples
Epoch 1/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.4356 - acc: 0.7971 - val\_loss: 1.8624 - val\_acc: 0.1647
Epoch 2/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.1598 - acc: 0.9286 - val\_loss: 3.5212 - val\_acc: 0.0706
Epoch 3/15
1050/1050 [==============================] - 6s 5ms/step - loss: 0.0993 - acc: 0.9695 - val\_loss: 3.8364 - val\_acc: 0.0824
Epoch 4/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0671 - acc: 0.9867 - val\_loss: 3.7887 - val\_acc: 0.1118
Epoch 5/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0437 - acc: 0.9876 - val\_loss: 5.4205 - val\_acc: 0.0471
Epoch 6/15
1050/1050 [==============================] - 5s 5ms/step - loss: 0.0547 - acc: 0.9810 - val\_loss: 4.4170 - val\_acc: 0.1118
Epoch 7/15
1050/1050 [==============================] - 7s 7ms/step - loss: 0.0394 - acc: 0.9867 - val\_loss: 6.6829 - val\_acc: 0.0235
Epoch 8/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0262 - acc: 0.9943 - val\_loss: 6.0193 - val\_acc: 0.0353
Epoch 9/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0254 - acc: 0.9933 - val\_loss: 5.1429 - val\_acc: 0.0824
Epoch 10/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0209 - acc: 0.9943 - val\_loss: 6.0147 - val\_acc: 0.0588
Epoch 11/15
1050/1050 [==============================] - 6s 5ms/step - loss: 0.0221 - acc: 0.9962 - val\_loss: 6.6300 - val\_acc: 0.0294
Epoch 12/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0133 - acc: 0.9971 - val\_loss: 6.4157 - val\_acc: 0.0471
Epoch 13/15
1050/1050 [==============================] - 6s 6ms/step - loss: 0.0125 - acc: 0.9971 - val\_loss: 6.9895 - val\_acc: 0.0471
Epoch 14/15
1050/1050 [==============================] - 7s 7ms/step - loss: 0.0089 - acc: 0.9981 - val\_loss: 7.2110 - val\_acc: 0.0353
Epoch 15/15
1050/1050 [==============================] - 8s 8ms/step - loss: 0.0100 - acc: 0.9971 - val\_loss: 6.9989 - val\_acc: 0.0588

    \end{Verbatim}

    On the test set we perform really bad: less than 10\% Accuracy!

    \subsubsection{Verifying Accuracy on Test
Set}\label{verifying-accuracy-on-test-set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{n}{test\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{test\PYZus{}img}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} show 35 first predictions}
         \PY{n}{test\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{35}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}86}]:} array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{test\PYZus{}img}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}classes}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}87}]:} 0.058823529411764705
\end{Verbatim}
            
    Plotting the Training Curve The model.fit function returns a history
including the evolution of training and validation loss and accuracy. We
can plot it to see a nice training curve.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{n}{hist} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}
         \PY{n}{hist}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}89}]:} dict\_keys(['val\_acc', 'val\_loss', 'acc', 'loss'])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{c+c1}{\PYZsh{} this will only work if you have matplotlib installed}
         
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline 
         
         \PY{n}{colors} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{measure} \PY{o+ow}{in} \PY{n}{hist}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{color} \PY{o}{=} \PY{n}{colors}\PY{p}{[}\PY{n}{measure}\PY{p}{]}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{hist}\PY{p}{[}\PY{n}{measure}\PY{p}{]}\PY{p}{,} \PY{n}{color} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{measure}\PY{p}{)}  \PY{c+c1}{\PYZsh{} use last 2 values to draw line}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scatterpoints} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{frameon}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:} <matplotlib.legend.Legend at 0x7f99df34e390>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Conclusion: We are completely overfitting! While we achieve nearly 100\%
on the Training Set, the generalization to the Test Set is really bad,
with an Accuracy of about only 10\%.

\subsubsection{Data Augmentation}\label{data-augmentation}

Increase the training set by adding more images: Rotate, shift, flip and
scale the original images to generate additional examples that will help
the Neural Network to generalize better.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{ImageDataGenerator}
         
         \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
             \PY{n}{rotation\PYZus{}range}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
             \PY{n}{width\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
             \PY{n}{height\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
             \PY{n}{zoom\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
             \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{c+c1}{\PYZsh{} ImageDataGenerator needs the classes as Numpy array instead of normal list}
         \PY{n}{classes\PYZus{}array} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{classes}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} enforce repeatable result}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{} recreate and recompile the model (otherwise we continue learning)}
         \PY{n}{model} \PY{o}{=} \PY{n}{createMyModel}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} fits the model on batches with real\PYZhy{}time data augmentation:}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}generator}\PY{p}{(}\PY{n}{datagen}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{train\PYZus{}img}\PY{p}{,} \PY{n}{classes\PYZus{}array}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{,}
                             \PY{n}{samples\PYZus{}per\PYZus{}epoch}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}img}\PY{p}{)}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{o}{=}\PY{n}{epochs}\PY{p}{,}
                             \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{validation\PYZus{}data}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="valid", input\_shape=(40, 100, {\ldots})`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:8: UserWarning: The semantics of the Keras 2 argument `steps\_per\_epoch` is not the same as the Keras 1 argument `samples\_per\_epoch`. `steps\_per\_epoch` is the number of batches to draw from the generator at each epoch. Basically steps\_per\_epoch = samples\_per\_epoch/batch\_size. Similarly `nb\_val\_samples`->`validation\_steps` and `val\_samples`->`steps` arguments have changed. Update your method calls accordingly.
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:8: UserWarning: Update your `fit\_generator` call to the Keras 2 API: `fit\_generator(<keras\_pre{\ldots}, validation\_data=(array([[[{\ldots}, steps\_per\_epoch=65, epochs=15)`

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/15
65/65 [==============================] - 6s 99ms/step - loss: 0.6709 - acc: 0.6183 - val\_loss: 0.8638 - val\_acc: 0.4529
Epoch 2/15
65/65 [==============================] - 6s 87ms/step - loss: 0.5431 - acc: 0.7290 - val\_loss: 1.0396 - val\_acc: 0.3647
Epoch 3/15
65/65 [==============================] - 6s 87ms/step - loss: 0.5039 - acc: 0.7523 - val\_loss: 1.0941 - val\_acc: 0.3706
Epoch 4/15
65/65 [==============================] - 6s 93ms/step - loss: 0.4064 - acc: 0.8183 - val\_loss: 0.8195 - val\_acc: 0.5353
Epoch 5/15
65/65 [==============================] - 6s 98ms/step - loss: 0.3711 - acc: 0.8460 - val\_loss: 0.2933 - val\_acc: 0.8647
Epoch 6/15
65/65 [==============================] - 7s 115ms/step - loss: 0.3510 - acc: 0.8502 - val\_loss: 0.4393 - val\_acc: 0.7647
Epoch 7/15
65/65 [==============================] - 6s 99ms/step - loss: 0.3135 - acc: 0.8856 - val\_loss: 0.2945 - val\_acc: 0.8588
Epoch 8/15
65/65 [==============================] - 9s 139ms/step - loss: 0.3064 - acc: 0.8612 - val\_loss: 0.4455 - val\_acc: 0.8000
Epoch 9/15
65/65 [==============================] - 8s 120ms/step - loss: 0.3174 - acc: 0.8677 - val\_loss: 0.5499 - val\_acc: 0.7588
Epoch 10/15
65/65 [==============================] - 10s 159ms/step - loss: 0.2920 - acc: 0.8773 - val\_loss: 0.5513 - val\_acc: 0.7588
Epoch 11/15
65/65 [==============================] - 8s 120ms/step - loss: 0.2959 - acc: 0.8846 - val\_loss: 0.6764 - val\_acc: 0.6588
Epoch 12/15
65/65 [==============================] - 8s 119ms/step - loss: 0.2696 - acc: 0.8835 - val\_loss: 1.4055 - val\_acc: 0.3941
Epoch 13/15
65/65 [==============================] - 8s 120ms/step - loss: 0.2585 - acc: 0.8985 - val\_loss: 0.4723 - val\_acc: 0.7765
Epoch 14/15
65/65 [==============================] - 8s 120ms/step - loss: 0.2489 - acc: 0.8985 - val\_loss: 0.9309 - val\_acc: 0.5647
Epoch 15/15
65/65 [==============================] - 8s 118ms/step - loss: 0.2538 - acc: 0.8912 - val\_loss: 0.5559 - val\_acc: 0.7588

    \end{Verbatim}

    \subsubsection{Verifying Accuracy on Test Set (with Data
Augmentation)}\label{verifying-accuracy-on-test-set-with-data-augmentation}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{test\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{test\PYZus{}img}\PY{p}{)}
         \PY{n}{test\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{35}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} show 35 first predictions}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}95}]:} array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
                1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], dtype=int32)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{test\PYZus{}img}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}classes}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}96}]:} 0.7588235294117647
\end{Verbatim}
            
    Conclusion: With Data Augmentation, the model has more diverse training
examples and generalizes much better. The Accuracy on the Test Set
increases from less than 10\% to more than 55\% up to 75\%!

\subsubsection{Plotting the Training Curve with Data
Augmentation}\label{plotting-the-training-curve-with-data-augmentation}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{n}{hist} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Curve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{measure} \PY{o+ow}{in} \PY{n}{hist}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{color} \PY{o}{=} \PY{n}{colors}\PY{p}{[}\PY{n}{measure}\PY{p}{]}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{hist}\PY{p}{[}\PY{n}{measure}\PY{p}{]}\PY{p}{,} \PY{n}{color} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{measure}\PY{p}{)}  \PY{c+c1}{\PYZsh{} use last 2 values to draw line}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scatterpoints} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{frameon}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}97}]:} <matplotlib.legend.Legend at 0x7f99b4c7bf28>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    This graph is not ideal, but we see both training (blue) and validation
(green) Accuracy going up, the training loss going continuously down and
the validation loss fluctuation, but not exploding like previously.

    \subsection{Second Part}\label{second-part}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} We need to construct our data set; unfortunately, we don\PYZsq{}t simply have a \PYZdq{}loadFruitImageDataSet()\PYZdq{} function in SK\PYZhy{}learn...}
         \PY{c+c1}{\PYZsh{} So we need to }
         \PY{c+c1}{\PYZsh{}\PYZsh{} Download our data set \PYZam{} extract it (one\PYZhy{}time effort)}
         \PY{c+c1}{\PYZsh{}\PYZsh{} Run an image feature extraction}
         \PY{c+c1}{\PYZsh{}\PYZsh{} Create the create the ground truth (label assignment, target, ...) }
         
         
         \PY{c+c1}{\PYZsh{} path to our image folder}
         \PY{c+c1}{\PYZsh{} For the first run, download the images from http://data.vicos.si/datasets/FIDS30/FIDS30.zip, and unzip them to your folder}
         \PY{n}{imagePath}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/xxx/MScBI/S4/ML/exer3/data/FIDS30/}\PY{l+s+s2}{\PYZdq{}}
         
         
         \PY{c+c1}{\PYZsh{} Find all images in that folder; there are like 1.000 different ways to do this in Python, we chose this one :\PYZhy{})}
         \PY{k+kn}{import} \PY{n+nn}{glob}\PY{o}{,} \PY{n+nn}{os}
         \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{n}{imagePath}\PY{p}{)}
         \PY{n}{fileNames} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*/*.jpg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{numberOfFiles}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{fileNames}\PY{p}{)}
         \PY{n}{targetLabels}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Found }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{numberOfFiles}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ files}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The first step \PYZhy{} create the ground truth (label assignment, target, ...) }
         \PY{c+c1}{\PYZsh{} For that, iterate over the files, and obtain the class label for each file}
         \PY{c+c1}{\PYZsh{} Basically, the class name is in the full path name, so we simply use that}
         \PY{k}{for} \PY{n}{fileName} \PY{o+ow}{in} \PY{n}{fileNames}\PY{p}{:}
             \PY{n}{pathSepIndex} \PY{o}{=} \PY{n}{fileName}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{targetLabels}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{fileName}\PY{p}{[}\PY{p}{:}\PY{n}{pathSepIndex}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} sk\PYZhy{}learn can only handle labels in numeric format \PYZhy{} we have them as strings though...}
         \PY{c+c1}{\PYZsh{} Thus we use the LabelEncoder, which does a mapping to Integer numbers}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
         \PY{n}{le} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
         \PY{n}{le}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{targetLabels}\PY{p}{)} \PY{c+c1}{\PYZsh{} this basically finds all unique class names, and assigns them to the numbers}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Found the following classes: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{le}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} now we transform our labels to integers}
         \PY{n}{target} \PY{o}{=} \PY{n}{le}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{targetLabels}\PY{p}{)}\PY{p}{;} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Transformed labels (first elements: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{target}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{150}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} If we want to find again the label for an integer value, we can do something like this:}
         \PY{c+c1}{\PYZsh{} print list(le.inverse\PYZus{}transform([0, 18, 1]))}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{... done label encoding}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Found 971 files

Found the following classes: ['acerolas', 'apples', 'apricots', 'avocados', 'bananas', 'blackberries', 'blueberries', 'cantaloupes', 'cherries', 'coconuts', 'figs', 'grapefruits', 'grapes', 'guava', 'kiwifruit', 'lemons', 'limes', 'mangos', 'olives', 'oranges', 'passionfruit', 'peaches', 'pears', 'pineapples', 'plums', 'pomegranates', 'raspberries', 'strawberries', 'tomatoes', 'watermelons']
Transformed labels (first elements: [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25
 25 25 25 25 25 25 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21
 21 21 21 21 21 21 21 21 21 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26
 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26
 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13
 13 13 13 13 13 13 13 13 13 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15
 15 15 15 15 15 15]
{\ldots} done label encoding

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{le}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:} ['acerolas', 'apples', 'apricots', 'avocados', 'bananas']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{targetLabels}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{l+m+mi}{25}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} ['pomegranates',
          'pomegranates',
          'pomegranates',
          'pomegranates',
          'pomegranates']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{} Before we do actual feature extaction just for curiosity, let\PYZsq{}s look at one image, to illustrate what we are going to do}
         
         \PY{c+c1}{\PYZsh{} import the necessary packages}
         \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
         
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         
         \PY{c+c1}{\PYZsh{} import libraries for image feature extraction}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{cv2}
         \PY{c+c1}{\PYZsh{} For OpenCV (need Version 2.4+) for Python 2.7, on Ubuntu Linux, just \PYZdq{}sudo apt install python\PYZhy{}opencv\PYZdq{}}
         \PY{c+c1}{\PYZsh{} In other OS, that might be different.}
         
         \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
         
         \PY{n}{demoImageName} \PY{o}{=} \PY{n}{fileNames}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{demoImage} \PY{o}{=} \PY{n}{imagePath} \PY{o}{+} \PY{n}{demoImageName}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Showing demo feature extraction on image }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{demoImage}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} load the image \PYZam{} plot it}
         \PY{n}{imagePIL} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{demoImage}\PY{p}{)}
         \PY{n}{imgplot} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{imagePIL}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{demoImageName}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} now we compute a colour histogram using the histogram function in pillow}
         \PY{c+c1}{\PYZsh{} This gives us one histogram with 768 values, which is 3 x 256 values for each colour}
         \PY{c+c1}{\PYZsh{} For each colour channel, each value repesent the count how many pixels have that colour intensity}
         \PY{n}{featureVector}\PY{o}{=}\PY{n}{imagePIL}\PY{o}{.}\PY{n}{histogram}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} We plot this histogram}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{featureVector}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{256}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{featureVector}\PY{p}{[}\PY{l+m+mi}{257}\PY{p}{:}\PY{l+m+mi}{512}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{featureVector}\PY{p}{[}\PY{l+m+mi}{513}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bins}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} of Pixels}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colour Histogram, using PIL}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} An alternative is to use open CV}
         \PY{n}{imageOpenCV} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{demoImage}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} OpenCV is a bit weird, because it changes the channel order, it stores them as BGR, instead of RGB}
         \PY{c+c1}{\PYZsh{} So if we want to display the image, we have to invert it}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{imageOpenCV}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{chans} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{imageOpenCV}\PY{p}{)} \PY{c+c1}{\PYZsh{} split the image in the different channels (RGB, but in open CV, it is BGR, actually..)}
         \PY{n}{colors} \PY{o}{=} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{g}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colour Histogram, using OpenCV}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bins}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} of Pixels}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{featuresOpenCV} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} loop over the image channels}
         \PY{k}{for} \PY{p}{(}\PY{n}{chan}\PY{p}{,} \PY{n}{color}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{chans}\PY{p}{,} \PY{n}{colors}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} create a histogram for the current channel and add it to the resulting histograms array (of arrays)}
             \PY{c+c1}{\PYZsh{} We can specifiy here in the 4th argument how many bins we want \PYZhy{} 256 means the same as in the previous histogram}
             \PY{n}{histOpenCV} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{chan}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{256}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}
             \PY{n}{featuresOpenCV}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{histOpenCV}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} plot the histogram of the current colour}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{histOpenCV}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{n}{color}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} Now we have a 2D\PYZhy{}array \PYZhy{} 256 values for each of 3 colour channels.}
         \PY{c+c1}{\PYZsh{} To input this into our machine learning, we need to \PYZdq{}flatten\PYZdq{} the features into one larger 1D array}
         \PY{c+c1}{\PYZsh{} the size of this will be 3 x 256 = 768 values}
         \PY{n}{featureVectorOpenCV} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{featuresOpenCV}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} show all the plots}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Showing demo feature extraction on image /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/37.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{} so NOW we actually extract features from our images}
         \PY{k+kn}{import} \PY{n+nn}{datetime}
         
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Extracting features using PIL/PILLOW}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ (}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The simplest approach is via the PIL/PILLOW package; here we get a histogram over each RGB channel}
         \PY{c+c1}{\PYZsh{} Note: this doesn\PYZsq{}t really represent colours, as a colour is made up of the combination of the three channels!}
         \PY{n}{data}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}data\PYZus{}features = []}
         \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{fileName} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{fileNames}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{imagePIL} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{imagePath} \PY{o}{+} \PY{n}{fileName}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Not all images in our dataset are in RGB color scheme (e.g. indexed colours)}
             \PY{c+c1}{\PYZsh{} We need to make sure that they are RGB , otherwise we can\PYZsq{}t expect to have exactly three RGB channels..}
             \PY{n}{imagePIL} \PY{o}{=} \PY{n}{imagePIL}\PY{o}{.}\PY{n}{convert}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RGB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}     
             \PY{n}{featureVector}\PY{o}{=}\PY{n}{imagePIL}\PY{o}{.}\PY{n}{histogram}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO}
             \PY{c+c1}{\PYZsh{}image\PYZus{}features = extract\PYZus{}features(imagePath + fileName)}
             
             \PY{k}{if} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureVector}\PY{p}{)} \PY{o}{!=} \PY{l+m+mi}{768}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} just a sanity check; with the transformation to RGB, this should never happen}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Unexpected length of feature vector: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureVector}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ in file: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{fileName}\PY{p}{)}
         
             \PY{n}{data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{featureVector}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}data\PYZus{}features.append((image\PYZus{}features))}
         
             
         \PY{c+c1}{\PYZsh{} Next, we extract a few more features using OpenCV}
         
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Extracting features using OpenCV}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ (}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{dataOpenCV\PYZus{}1D}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         \PY{n}{dataOpenCV\PYZus{}2D}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         \PY{n}{dataOpenCV\PYZus{}3D}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} use our own simple function to flatten the 2D arrays}
         \PY{n}{flatten} \PY{o}{=} \PY{k}{lambda} \PY{n}{l}\PY{p}{:} \PY{p}{[}\PY{n}{item} \PY{k}{for} \PY{n}{sublist} \PY{o+ow}{in} \PY{n}{l} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n}{sublist}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{fileName} \PY{o+ow}{in} \PY{n}{fileNames}\PY{p}{:}
         
             \PY{c+c1}{\PYZsh{} the easiest way would to do the following:}
             \PY{c+c1}{\PYZsh{} imageOpenCV = cv2.imread(imagePath + fileName)}
         
             \PY{c+c1}{\PYZsh{} However, we have the same issue as before, and it is more difficult in OpenCV to convert to an RGB image}
             \PY{c+c1}{\PYZsh{} Thus we do this using PIL, and then convert to OpenCV ....}
             \PY{n}{imagePIL} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{imagePath} \PY{o}{+} \PY{n}{fileName}\PY{p}{)}
             \PY{n}{imagePIL} \PY{o}{=} \PY{n}{imagePIL}\PY{o}{.}\PY{n}{convert}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RGB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{imageOpenCV} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{imagePIL}\PY{p}{)} 
             \PY{c+c1}{\PYZsh{} Convert RGB to BGR }
             \PY{n}{imageOpenCV} \PY{o}{=} \PY{n}{imageOpenCV}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)} 
         
             \PY{c+c1}{\PYZsh{} Now we split the image in the three channels, B / G / R}
             \PY{n}{chans} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{imageOpenCV}\PY{p}{)}
             \PY{n}{colors} \PY{o}{=} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{g}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} First we do also features per channel, but this time, we aggregate them into a smaller number of bins}
             \PY{c+c1}{\PYZsh{} I.e. we do not have 256 values per channel, but less}
             \PY{n}{featuresOpenCV\PYZus{}1D} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{bins\PYZus{}1D}\PY{o}{=}\PY{l+m+mi}{64}
             \PY{k}{for} \PY{p}{(}\PY{n}{chan}\PY{p}{,} \PY{n}{color}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{chans}\PY{p}{,} \PY{n}{colors}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} we compute the histogram over each channel}
                 \PY{n}{histOpenCV} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{chan}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{n}{bins\PYZus{}1D}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}
                 \PY{n}{featuresOpenCV\PYZus{}1D}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{histOpenCV}\PY{p}{)}
             \PY{n}{featureVectorOpenCV\PYZus{}1D} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{featuresOpenCV\PYZus{}1D}\PY{p}{)} \PY{c+c1}{\PYZsh{} and append this to our feature vector}
             
             \PY{n}{dataOpenCV\PYZus{}1D}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{featureVectorOpenCV\PYZus{}1D}\PY{p}{)} \PY{c+c1}{\PYZsh{} now we append the feature vector to the dataset so far}
         
             \PY{k}{if} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureVectorOpenCV\PYZus{}1D}\PY{p}{)} \PY{o}{!=} \PY{n}{bins\PYZus{}1D}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} sanity check, in case we had a wrong number of channels...}
                 \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Unexpected length of feature vector: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureVectorOpenCV\PYZus{}1D}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ in file: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{fileName}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Next \PYZhy{} features that look at two channels at the same time}
             \PY{c+c1}{\PYZsh{} E.g. we look at when green and blue have both \PYZdq{}high values\PYZdq{}}
             \PY{c+c1}{\PYZsh{} We reduce the size of bins further, to not have a too long feature vector}
             \PY{n}{featuresOpenCV\PYZus{}2D} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{bins2D}\PY{o}{=}\PY{l+m+mi}{16}
             \PY{c+c1}{\PYZsh{} look at all combinations of channels (R \PYZam{} B, R \PYZam{} G, B \PYZam{} G)}
             \PY{n}{featuresOpenCV\PYZus{}2D}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{n}{bins2D}\PY{p}{,} \PY{n}{bins2D}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n}{featuresOpenCV\PYZus{}2D}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{n}{bins2D}\PY{p}{,} \PY{n}{bins2D}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n}{featuresOpenCV\PYZus{}2D}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{chans}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{n}{bins2D}\PY{p}{,} \PY{n}{bins2D}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} and add that to our dataset}
             \PY{n}{featureVectorOpenCV\PYZus{}2D} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{featuresOpenCV\PYZus{}2D}\PY{p}{)}
             \PY{n}{dataOpenCV\PYZus{}2D}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{featureVectorOpenCV\PYZus{}2D}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} finally, we look at all three channels at the same time.}
             \PY{c+c1}{\PYZsh{} We further reduce our bin size, because otherwise, this would become very large...}
             \PY{n}{featuresOpenCV\PYZus{}3D} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{calcHist}\PY{p}{(}\PY{p}{[}\PY{n}{imageOpenCV}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{]}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} append to our dataset}
             \PY{n}{featureVectorOpenCV\PYZus{}3D} \PY{o}{=} \PY{n}{featuresOpenCV\PYZus{}3D}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
             \PY{n}{dataOpenCV\PYZus{}3D}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{featureVectorOpenCV\PYZus{}3D}\PY{p}{)}
         
                 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.... done}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ (}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Extracting features using PIL/PILLOW (2019-02-11 21:22:52.698195)
Extracting features using OpenCV (2019-02-11 21:23:57.909665)
{\ldots} done (2019-02-11 21:25:49.198275)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} 768
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{c+c1}{\PYZsh{} Feature extractor}
         \PY{k}{def} \PY{n+nf}{extract\PYZus{}features}\PY{p}{(}\PY{n}{image\PYZus{}path}\PY{p}{,} \PY{n}{vector\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{:}
             \PY{n}{image} \PY{o}{=} \PY{n}{imread}\PY{p}{(}\PY{n}{image\PYZus{}path}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RGB}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{k}{try}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Using KAZE, cause SIFT, ORB and other was moved to additional module}
                 \PY{c+c1}{\PYZsh{} which is adding addtional pain during install}
                 \PY{n}{alg} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{KAZE\PYZus{}create}\PY{p}{(}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Dinding image keypoints}
                 \PY{n}{kps} \PY{o}{=} \PY{n}{alg}\PY{o}{.}\PY{n}{detect}\PY{p}{(}\PY{n}{image}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Getting first 64 of them. }
                 \PY{c+c1}{\PYZsh{} Number of keypoints is varies depend on image size and color pallet}
                 \PY{c+c1}{\PYZsh{} Sorting them based on keypoint response value(bigger is better)}
                 \PY{n}{kps} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{kps}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{n}{x}\PY{o}{.}\PY{n}{response}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{vector\PYZus{}size}\PY{p}{]}
                 \PY{c+c1}{\PYZsh{} computing descriptors vector}
                 \PY{n}{kps}\PY{p}{,} \PY{n}{dsc} \PY{o}{=} \PY{n}{alg}\PY{o}{.}\PY{n}{compute}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{kps}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Flatten all of them in one big vector \PYZhy{} our feature vector}
                 \PY{n}{dsc} \PY{o}{=} \PY{n}{dsc}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Making descriptor of same size}
                 \PY{c+c1}{\PYZsh{} Descriptor vector size is 64}
                 \PY{n}{needed\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n}{vector\PYZus{}size} \PY{o}{*} \PY{l+m+mi}{64}\PY{p}{)}
                 \PY{k}{if} \PY{n}{dsc}\PY{o}{.}\PY{n}{size} \PY{o}{\PYZlt{}} \PY{n}{needed\PYZus{}size}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} if we have less the 32 descriptors then just adding zeros at the}
                     \PY{c+c1}{\PYZsh{} end of our feature vector}
                     \PY{n}{dsc} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{dsc}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{needed\PYZus{}size} \PY{o}{\PYZhy{}} \PY{n}{dsc}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{]}\PY{p}{)}
             \PY{k}{except} \PY{n}{cv2}\PY{o}{.}\PY{n}{error} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Error: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{e}\PY{p}{)}
                 \PY{k}{return} \PY{k+kc}{None}
         
             \PY{k}{return} \PY{n}{dsc}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{data\PYZus{}feature\PYZus{}vectors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{fileName} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{fileNames}\PY{p}{)}\PY{p}{:}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.... START}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ (}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Opening}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{imagePath} \PY{o}{+} \PY{n}{fileName}\PY{p}{)} \PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{image\PYZus{}feature\PYZus{}vec} \PY{o}{=} \PY{n}{extract\PYZus{}features}\PY{p}{(}\PY{n}{imagePath} \PY{o}{+} \PY{n}{fileName}\PY{p}{)}
             
             
             \PY{n}{data\PYZus{}feature\PYZus{}vectors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{image\PYZus{}feature\PYZus{}vec}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.... END}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ (}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/22.jpg  {\ldots}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/ipykernel/\_\_main\_\_.py:3: DeprecationWarning: `imread` is deprecated!
`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.
Use ``imageio.imread`` instead.
  app.launch\_new\_instance()

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pomegranates/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/peaches/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/raspberries/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/guava/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/lemons/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apricots/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/watermelons/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cherries/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/oranges/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/58.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/59.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/56.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blueberries/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/acerolas/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/strawberries/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/passionfruit/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pineapples/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/coconuts/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/56.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/57.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/limes/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/plums/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/apples/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/cantaloupes/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/58.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/56.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/57.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/kiwifruit/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapefruits/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/olives/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/56.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/57.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/mangos/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/avocados/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/33.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/59.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/48.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/blackberries/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/57.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/grapes/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/24.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/pears/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/40.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/39.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/50.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/6.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/bananas/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/51.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/25.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/30.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/figs/15.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/22.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/37.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/41.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/20.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/7.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/29.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/32.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/11.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/45.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/28.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/1.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/47.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/49.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/13.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/44.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/4.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/43.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/36.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/12.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/34.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/10.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/19.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/9.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/54.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/27.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/14.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/0.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/18.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/21.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/23.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/31.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/26.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/8.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/5.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/16.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/46.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/53.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/35.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/3.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/2.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/17.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/52.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/42.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/38.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/55.jpg  {\ldots}
Opening /home/xxx/MScBI/S4/ML/exer3/data/FIDS30/tomatoes/6.jpg  {\ldots}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}features}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} 4096
\end{Verbatim}
            
    \subsubsection{Configure Neural Network}\label{configure-neural-network}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{SGD}
         
         \PY{k}{def} \PY{n+nf}{get\PYZus{}model}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} simple Fully\PYZhy{}connected network}
             
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}model.add(Dense(256))}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}model.add(Dense(500))}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{targetLabels}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Define a loss function }
             \PY{n}{loss} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}  \PY{c+c1}{\PYZsh{} \PYZsq{}categorical\PYZus{}crossentropy\PYZsq{} for multi\PYZhy{}class problems}
             \PY{c+c1}{\PYZsh{} Optimizer = Stochastic Gradient Descent}
             \PY{n}{optimizer} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}} 
             \PY{c+c1}{\PYZsh{}optimizer = SGD(lr=0.01, decay=1e\PYZhy{}6, momentum=0.9, nesterov=True)\PYZsh{}SGD(lr=0.01)}
             \PY{c+c1}{\PYZsh{}optimizer = \PYZdq{}adam\PYZdq{}}
             \PY{c+c1}{\PYZsh{} Compiling the model}
             \PY{c+c1}{\PYZsh{} This creates the whole model structure in memory. }
             \PY{c+c1}{\PYZsh{} If you use GPU computation, here GPU compatible structures and code is generated.}
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{target}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} (192,)
\end{Verbatim}
            
    \subsubsection{Run algorithms on different
datasets}\label{run-algorithms-on-different-datasets}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{c+c1}{\PYZsh{} And now we finally classify }
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{metrics}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{KFold}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{neighbors}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{linear\PYZus{}model}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{naive\PYZus{}bayes}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{tree}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{ensemble}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{svm}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{f1\PYZus{}score}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}\PY{p}{,} \PY{n}{precision\PYZus{}score}
         
         \PY{c+c1}{\PYZsh{} these are our feature sets; we will use each of them individually to train classifiers}
         \PY{n}{trainingSets} \PY{o}{=} \PY{p}{[}\PY{n}{data}\PY{p}{,} \PY{n}{data\PYZus{}feature\PYZus{}vectors}\PY{p}{,} \PY{n}{dataOpenCV\PYZus{}1D}\PY{p}{,} \PY{n}{dataOpenCV\PYZus{}2D}\PY{p}{,} \PY{n}{dataOpenCV\PYZus{}3D}\PY{p}{]}
         \PY{n}{trainingSetNames} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{raw\PYZhy{}data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{keypoints}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hist\PYZhy{}1D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hist\PYZhy{}2D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hist\PYZhy{}3D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} set up a number of classifiers}
         \PY{n}{classifiers} \PY{o}{=} \PY{p}{[}\PY{n}{neighbors}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                        \PY{n}{naive\PYZus{}bayes}\PY{o}{.}\PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}\PY{p}{,} 
                        \PY{n}{tree}\PY{o}{.}\PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                        \PY{n}{ensemble}\PY{o}{.}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                        \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                        \PY{n}{svm}\PY{o}{.}\PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                       \PY{p}{]}
         
         \PY{n}{classifier\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNN}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{NaiveBayes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DecisionTree}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RandomForest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LinearSVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Now iterate over the datasets \PYZam{} classifiers, and train...}
         \PY{k}{for} \PY{n}{train}\PY{p}{,} \PY{n}{ds\PYZus{}name} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{trainingSets}\PY{p}{,} \PY{n}{trainingSetNames}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Classifier: MLP, dataset(features): }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ds\PYZus{}name}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert to float}
             \PY{n}{train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{config}\PY{o}{.}\PY{n}{floatX}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} normalize}
             \PY{n}{mean} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             \PY{n}{stddev} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
             \PY{n}{train}  \PY{o}{=} \PY{p}{(}\PY{n}{train} \PY{o}{\PYZhy{}} \PY{n}{mean}\PY{p}{)} \PY{o}{/} \PY{n}{stddev}
         
             \PY{c+c1}{\PYZsh{} split to train, validate, test}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{target}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} encode y\PYZus{}test for NN}
             \PY{n}{encoder} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
             \PY{n}{encoder}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{encoded\PYZus{}Y} \PY{o}{=} \PY{n}{encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert integers to dummy variables (i.e. one hot encoded)}
             \PY{n}{y\PYZus{}NN\PYZus{}train} \PY{o}{=} \PY{n}{np\PYZus{}utils}\PY{o}{.}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{encoded\PYZus{}Y}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}dummy\PYZus{}y}
         
         
             \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50}
             \PY{n}{input\PYZus{}shape} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{model} \PY{o}{=} \PY{n}{get\PYZus{}model}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}NN\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)} \PY{c+c1}{\PYZsh{}, validation\PYZus{}data=validation\PYZus{}data)}
             \PY{c+c1}{\PYZsh{} verify Accuracy on Train set\PYZsh{} verify }
             \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}print(\PYZdq{}Precision: \PYZdq{}, round(precision\PYZus{}score(y\PYZus{}NN\PYZus{}test, predictions, average=\PYZsq{}micro\PYZsq{}),2))}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{target\PYZus{}names}\PY{o}{=}\PY{n+nb}{list}\PY{p}{(}\PY{n}{le}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{classifier}\PY{p}{,} \PY{n}{name} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{classifiers}\PY{p}{,} \PY{n}{classifier\PYZus{}names}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Classifier: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{name}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ , dataset(features): }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ds\PYZus{}name}\PY{p}{)}
                 \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
                 \PY{n}{pred} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{pred}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{micro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                 
                 
             
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    \PYZsh{}print(input\PYZus{}shape2)}
         \PY{l+s+sd}{    train\PYZus{}hist = np.array(train, dtype=config.floatX)}
         \PY{l+s+sd}{    input\PYZus{}shape = train\PYZus{}hist.shape[1]}
         \PY{l+s+sd}{    model = get\PYZus{}model(input\PYZus{}shape)}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    mean = train\PYZus{}hist.mean()}
         \PY{l+s+sd}{    stddev = train\PYZus{}hist.std()}
         \PY{l+s+sd}{    train\PYZus{}hist  = (train\PYZus{}hist \PYZhy{} mean) / stddev}
         \PY{l+s+sd}{    \PYZsh{}img\PYZus{}array.mean(), img\PYZus{}array.std()}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    epochs = 50}
         \PY{l+s+sd}{    model.fit(train\PYZus{}hist, dummy\PYZus{}y, batch\PYZus{}size=32, nb\PYZus{}epoch=epochs) \PYZsh{}, validation\PYZus{}data=validation\PYZus{}data)}
         \PY{l+s+sd}{    \PYZsh{} verify Accuracy on Train set\PYZsh{} verify }
         \PY{l+s+sd}{    \PYZsh{}predictions = model.predict\PYZus{}classes(train\PYZus{}hist)}
         \PY{l+s+sd}{    \PYZsh{}precision\PYZus{}score(dummy\PYZus{}y, predictions, average=\PYZsq{}micro\PYZsq{})}
         \PY{l+s+sd}{    print(\PYZdq{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZdq{})}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Classifier: MLP, dataset(features):  raw-data
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_16 (Dense)             (None, 500)               384500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_17 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_18 (Dense)             (None, 30)                15030     
=================================================================
Total params: 650,030
Trainable params: 650,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Train on 698 samples, validate on 78 samples
Epoch 1/50
698/698 [==============================] - 1s 722us/step - loss: 3.4344 - acc: 0.0430 - val\_loss: 3.3442 - val\_acc: 0.0385
Epoch 2/50
698/698 [==============================] - 0s 295us/step - loss: 3.3607 - acc: 0.0458 - val\_loss: 3.3386 - val\_acc: 0.0385
Epoch 3/50
698/698 [==============================] - 0s 284us/step - loss: 3.3422 - acc: 0.0516 - val\_loss: 3.3384 - val\_acc: 0.0641
Epoch 4/50
698/698 [==============================] - 0s 279us/step - loss: 3.3264 - acc: 0.0516 - val\_loss: 3.3338 - val\_acc: 0.0641
Epoch 5/50
698/698 [==============================] - 0s 283us/step - loss: 3.3154 - acc: 0.0530 - val\_loss: 3.3311 - val\_acc: 0.0641
Epoch 6/50
698/698 [==============================] - 0s 289us/step - loss: 3.3092 - acc: 0.0544 - val\_loss: 3.3251 - val\_acc: 0.0641
Epoch 7/50
698/698 [==============================] - 0s 278us/step - loss: 3.2995 - acc: 0.0587 - val\_loss: 3.3289 - val\_acc: 0.0641
Epoch 8/50
698/698 [==============================] - 0s 269us/step - loss: 3.2906 - acc: 0.0630 - val\_loss: 3.3247 - val\_acc: 0.0513
Epoch 9/50
698/698 [==============================] - 0s 275us/step - loss: 3.2778 - acc: 0.0630 - val\_loss: 3.3183 - val\_acc: 0.0513
Epoch 10/50
698/698 [==============================] - 0s 277us/step - loss: 3.2717 - acc: 0.0673 - val\_loss: 3.3127 - val\_acc: 0.0641
Epoch 11/50
698/698 [==============================] - 0s 285us/step - loss: 3.2737 - acc: 0.0688 - val\_loss: 3.3185 - val\_acc: 0.0513
Epoch 12/50
698/698 [==============================] - 0s 279us/step - loss: 3.2603 - acc: 0.0688 - val\_loss: 3.3121 - val\_acc: 0.0641
Epoch 13/50
698/698 [==============================] - 0s 283us/step - loss: 3.2512 - acc: 0.0688 - val\_loss: 3.3105 - val\_acc: 0.0769
Epoch 14/50
698/698 [==============================] - 0s 278us/step - loss: 3.2458 - acc: 0.0716 - val\_loss: 3.3036 - val\_acc: 0.0897
Epoch 15/50
698/698 [==============================] - 0s 288us/step - loss: 3.2390 - acc: 0.0745 - val\_loss: 3.3027 - val\_acc: 0.0897
Epoch 16/50
698/698 [==============================] - 0s 281us/step - loss: 3.2412 - acc: 0.0759 - val\_loss: 3.3043 - val\_acc: 0.0897
Epoch 17/50
698/698 [==============================] - 0s 269us/step - loss: 3.2272 - acc: 0.0788 - val\_loss: 3.3003 - val\_acc: 0.0897
Epoch 18/50
698/698 [==============================] - 0s 288us/step - loss: 3.2238 - acc: 0.0903 - val\_loss: 3.2942 - val\_acc: 0.0769
Epoch 19/50
698/698 [==============================] - 0s 279us/step - loss: 3.2189 - acc: 0.0845 - val\_loss: 3.2922 - val\_acc: 0.0641
Epoch 20/50
698/698 [==============================] - 0s 274us/step - loss: 3.2090 - acc: 0.0845 - val\_loss: 3.2873 - val\_acc: 0.0641
Epoch 21/50
698/698 [==============================] - 0s 276us/step - loss: 3.2072 - acc: 0.0917 - val\_loss: 3.2916 - val\_acc: 0.0641
Epoch 22/50
698/698 [==============================] - 0s 285us/step - loss: 3.1981 - acc: 0.0903 - val\_loss: 3.2815 - val\_acc: 0.0641
Epoch 23/50
698/698 [==============================] - 0s 280us/step - loss: 3.1898 - acc: 0.0903 - val\_loss: 3.2774 - val\_acc: 0.0769
Epoch 24/50
698/698 [==============================] - 0s 270us/step - loss: 3.1894 - acc: 0.0888 - val\_loss: 3.2802 - val\_acc: 0.0769
Epoch 25/50
698/698 [==============================] - 0s 284us/step - loss: 3.1788 - acc: 0.0917 - val\_loss: 3.2735 - val\_acc: 0.0641
Epoch 26/50
698/698 [==============================] - 0s 270us/step - loss: 3.1750 - acc: 0.0946 - val\_loss: 3.2777 - val\_acc: 0.0641
Epoch 27/50
698/698 [==============================] - 0s 286us/step - loss: 3.1657 - acc: 0.0931 - val\_loss: 3.2701 - val\_acc: 0.0641
Epoch 28/50
698/698 [==============================] - 0s 269us/step - loss: 3.1545 - acc: 0.0946 - val\_loss: 3.2609 - val\_acc: 0.0641
Epoch 29/50
698/698 [==============================] - 0s 283us/step - loss: 3.1467 - acc: 0.0989 - val\_loss: 3.2716 - val\_acc: 0.0769
Epoch 30/50
698/698 [==============================] - 0s 273us/step - loss: 3.1555 - acc: 0.0989 - val\_loss: 3.2684 - val\_acc: 0.0897
Epoch 31/50
698/698 [==============================] - 0s 291us/step - loss: 3.1428 - acc: 0.0888 - val\_loss: 3.2645 - val\_acc: 0.1026
Epoch 32/50
698/698 [==============================] - 0s 293us/step - loss: 3.1425 - acc: 0.0931 - val\_loss: 3.2699 - val\_acc: 0.1026
Epoch 33/50
698/698 [==============================] - 0s 277us/step - loss: 3.1280 - acc: 0.0989 - val\_loss: 3.2659 - val\_acc: 0.0897
Epoch 34/50
698/698 [==============================] - 0s 288us/step - loss: 3.1253 - acc: 0.1032 - val\_loss: 3.2668 - val\_acc: 0.0897
Epoch 35/50
698/698 [==============================] - 0s 279us/step - loss: 3.1186 - acc: 0.1060 - val\_loss: 3.2596 - val\_acc: 0.0897
Epoch 36/50
698/698 [==============================] - 0s 292us/step - loss: 3.1080 - acc: 0.1017 - val\_loss: 3.2600 - val\_acc: 0.0897
Epoch 37/50
698/698 [==============================] - 0s 305us/step - loss: 3.1034 - acc: 0.1074 - val\_loss: 3.2473 - val\_acc: 0.0897
Epoch 38/50
698/698 [==============================] - 0s 287us/step - loss: 3.0942 - acc: 0.1089 - val\_loss: 3.2373 - val\_acc: 0.0897
Epoch 39/50
698/698 [==============================] - 0s 269us/step - loss: 3.0818 - acc: 0.1160 - val\_loss: 3.2351 - val\_acc: 0.0769
Epoch 40/50
698/698 [==============================] - 0s 272us/step - loss: 3.0859 - acc: 0.1146 - val\_loss: 3.2489 - val\_acc: 0.0897
Epoch 41/50
698/698 [==============================] - 0s 288us/step - loss: 3.0744 - acc: 0.1218 - val\_loss: 3.2312 - val\_acc: 0.0897
Epoch 42/50
698/698 [==============================] - 0s 286us/step - loss: 3.0641 - acc: 0.1232 - val\_loss: 3.2362 - val\_acc: 0.0897
Epoch 43/50
698/698 [==============================] - 0s 292us/step - loss: 3.0532 - acc: 0.1332 - val\_loss: 3.2536 - val\_acc: 0.0897
Epoch 44/50
698/698 [==============================] - 0s 310us/step - loss: 3.0485 - acc: 0.1390 - val\_loss: 3.2926 - val\_acc: 0.0897
Epoch 45/50
698/698 [==============================] - 0s 284us/step - loss: 3.0497 - acc: 0.1433 - val\_loss: 3.2806 - val\_acc: 0.1154
Epoch 46/50
698/698 [==============================] - 0s 278us/step - loss: 3.0308 - acc: 0.1447 - val\_loss: 3.2612 - val\_acc: 0.1026
Epoch 47/50
698/698 [==============================] - 0s 294us/step - loss: 3.0190 - acc: 0.1404 - val\_loss: 3.2624 - val\_acc: 0.1282
Epoch 48/50
698/698 [==============================] - 0s 274us/step - loss: 3.0315 - acc: 0.1476 - val\_loss: 3.2556 - val\_acc: 0.0897
Epoch 49/50
698/698 [==============================] - 0s 288us/step - loss: 3.0100 - acc: 0.1490 - val\_loss: 3.3262 - val\_acc: 0.1154
Epoch 50/50
698/698 [==============================] - 0s 288us/step - loss: 3.0135 - acc: 0.1547 - val\_loss: 3.2605 - val\_acc: 0.0897
              precision    recall  f1-score   support

    acerolas       0.00      0.00      0.00         4
      apples       0.00      0.00      0.00         5
    apricots       0.00      0.00      0.00         5
    avocados       0.00      0.00      0.00         4
     bananas       0.14      0.33      0.19         9
blackberries       0.00      0.00      0.00         8
 blueberries       0.00      0.00      0.00         8
 cantaloupes       0.00      0.00      0.00         5
    cherries       0.00      0.00      0.00         9
    coconuts       0.00      0.00      0.00         3
        figs       0.00      0.00      0.00         3
 grapefruits       0.00      0.00      0.00         5
      grapes       0.00      0.00      0.00         9
       guava       0.00      0.00      0.00         7
   kiwifruit       0.00      0.00      0.00         9
      lemons       0.00      0.00      0.00         4
       limes       0.50      0.25      0.33         8
      mangos       0.00      0.00      0.00         4
      olives       0.00      0.00      0.00         8
     oranges       0.00      0.00      0.00         7
passionfruit       0.00      0.00      0.00         2
     peaches       1.00      0.33      0.50         6
       pears       0.00      0.00      0.00         4
  pineapples       0.00      0.00      0.00         8
       plums       0.00      0.00      0.00         9
pomegranates       0.00      0.00      0.00         7
 raspberries       0.33      0.11      0.17         9
strawberries       0.05      0.62      0.09         8
    tomatoes       0.03      0.11      0.04         9
 watermelons       0.00      0.00      0.00         9

 avg / total       0.08      0.07      0.05       195

Classifier:  KNN  , dataset(features):  raw-data

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn\_for)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Precision:  0.07
Classifier:  NaiveBayes  , dataset(features):  raw-data
Precision:  0.11
Classifier:  DecisionTree  , dataset(features):  raw-data
Precision:  0.13
Classifier:  RandomForest  , dataset(features):  raw-data
Precision:  0.17
Classifier:  SVC  , dataset(features):  raw-data
Precision:  0.04
Classifier:  LinearSVC  , dataset(features):  raw-data
Precision:  0.22
Classifier: MLP, dataset(features):  keypoints
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_19 (Dense)             (None, 500)               2048500   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_20 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_21 (Dense)             (None, 30)                15030     
=================================================================
Total params: 2,314,030
Trainable params: 2,314,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Train on 698 samples, validate on 78 samples
Epoch 1/50
698/698 [==============================] - 1s 1ms/step - loss: 3.4863 - acc: 0.0387 - val\_loss: 3.3819 - val\_acc: 0.0513
Epoch 2/50
698/698 [==============================] - 1s 767us/step - loss: 3.3239 - acc: 0.0559 - val\_loss: 3.3323 - val\_acc: 0.0513
Epoch 3/50
698/698 [==============================] - 1s 770us/step - loss: 3.2158 - acc: 0.0473 - val\_loss: 3.3545 - val\_acc: 0.0641
Epoch 4/50
698/698 [==============================] - 1s 953us/step - loss: 3.0463 - acc: 0.0544 - val\_loss: 3.4349 - val\_acc: 0.0385
Epoch 5/50
698/698 [==============================] - 1s 889us/step - loss: 2.7942 - acc: 0.1160 - val\_loss: 3.5609 - val\_acc: 0.0769
Epoch 6/50
698/698 [==============================] - 1s 879us/step - loss: 2.4100 - acc: 0.2092 - val\_loss: 3.6788 - val\_acc: 0.0513
Epoch 7/50
698/698 [==============================] - 1s 887us/step - loss: 1.9662 - acc: 0.3682 - val\_loss: 3.8955 - val\_acc: 0.0513
Epoch 8/50
698/698 [==============================] - 1s 916us/step - loss: 1.5996 - acc: 0.5057 - val\_loss: 4.0762 - val\_acc: 0.0769
Epoch 9/50
698/698 [==============================] - 1s 923us/step - loss: 1.3263 - acc: 0.6232 - val\_loss: 4.0705 - val\_acc: 0.0769
Epoch 10/50
698/698 [==============================] - 1s 1ms/step - loss: 0.7544 - acc: 0.8052 - val\_loss: 4.6190 - val\_acc: 0.0769
Epoch 11/50
698/698 [==============================] - 1s 1ms/step - loss: 0.3930 - acc: 0.9370 - val\_loss: 4.2170 - val\_acc: 0.0769
Epoch 12/50
698/698 [==============================] - 1s 1ms/step - loss: 0.3708 - acc: 0.9427 - val\_loss: 3.7720 - val\_acc: 0.1026
Epoch 13/50
698/698 [==============================] - 1s 1ms/step - loss: 0.1593 - acc: 0.9957 - val\_loss: 3.7604 - val\_acc: 0.1282
Epoch 14/50
698/698 [==============================] - 1s 1ms/step - loss: 0.1111 - acc: 0.9971 - val\_loss: 3.7158 - val\_acc: 0.1154
Epoch 15/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0986 - acc: 0.9957 - val\_loss: 3.7038 - val\_acc: 0.1538
Epoch 16/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0765 - acc: 1.0000 - val\_loss: 3.6716 - val\_acc: 0.1154
Epoch 17/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0674 - acc: 1.0000 - val\_loss: 3.6833 - val\_acc: 0.1410
Epoch 18/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0583 - acc: 1.0000 - val\_loss: 3.7063 - val\_acc: 0.1410
Epoch 19/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0525 - acc: 1.0000 - val\_loss: 3.7021 - val\_acc: 0.1410
Epoch 20/50
698/698 [==============================] - 1s 975us/step - loss: 0.0475 - acc: 1.0000 - val\_loss: 3.6580 - val\_acc: 0.1538
Epoch 21/50
698/698 [==============================] - 1s 884us/step - loss: 0.0432 - acc: 1.0000 - val\_loss: 3.6734 - val\_acc: 0.1410
Epoch 22/50
698/698 [==============================] - 1s 910us/step - loss: 0.0399 - acc: 1.0000 - val\_loss: 3.7280 - val\_acc: 0.1410
Epoch 23/50
698/698 [==============================] - 1s 931us/step - loss: 0.0371 - acc: 1.0000 - val\_loss: 3.7068 - val\_acc: 0.1154
Epoch 24/50
698/698 [==============================] - 1s 891us/step - loss: 0.0344 - acc: 1.0000 - val\_loss: 3.6915 - val\_acc: 0.1282
Epoch 25/50
698/698 [==============================] - 1s 915us/step - loss: 0.0319 - acc: 1.0000 - val\_loss: 3.7043 - val\_acc: 0.1282
Epoch 26/50
698/698 [==============================] - 1s 968us/step - loss: 0.0302 - acc: 1.0000 - val\_loss: 3.7287 - val\_acc: 0.1282
Epoch 27/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0286 - acc: 1.0000 - val\_loss: 3.6812 - val\_acc: 0.1282
Epoch 28/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0268 - acc: 1.0000 - val\_loss: 3.7280 - val\_acc: 0.1282
Epoch 29/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0256 - acc: 1.0000 - val\_loss: 3.7504 - val\_acc: 0.1410
Epoch 30/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0244 - acc: 1.0000 - val\_loss: 3.7405 - val\_acc: 0.1282
Epoch 31/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0233 - acc: 1.0000 - val\_loss: 3.7425 - val\_acc: 0.1410
Epoch 32/50
698/698 [==============================] - 1s 896us/step - loss: 0.0221 - acc: 1.0000 - val\_loss: 3.7184 - val\_acc: 0.1410
Epoch 33/50
698/698 [==============================] - 1s 875us/step - loss: 0.0212 - acc: 1.0000 - val\_loss: 3.7306 - val\_acc: 0.1282
Epoch 34/50
698/698 [==============================] - 1s 901us/step - loss: 0.0203 - acc: 1.0000 - val\_loss: 3.7632 - val\_acc: 0.1410
Epoch 35/50
698/698 [==============================] - 1s 894us/step - loss: 0.0195 - acc: 1.0000 - val\_loss: 3.7507 - val\_acc: 0.1282
Epoch 36/50
698/698 [==============================] - 1s 894us/step - loss: 0.0187 - acc: 1.0000 - val\_loss: 3.7475 - val\_acc: 0.1410
Epoch 37/50
698/698 [==============================] - 1s 889us/step - loss: 0.0182 - acc: 1.0000 - val\_loss: 3.7544 - val\_acc: 0.1410
Epoch 38/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0174 - acc: 1.0000 - val\_loss: 3.7607 - val\_acc: 0.1282
Epoch 39/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0168 - acc: 1.0000 - val\_loss: 3.7441 - val\_acc: 0.1410
Epoch 40/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0163 - acc: 1.0000 - val\_loss: 3.7680 - val\_acc: 0.1282
Epoch 41/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0157 - acc: 1.0000 - val\_loss: 3.7725 - val\_acc: 0.1410
Epoch 42/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0153 - acc: 1.0000 - val\_loss: 3.7844 - val\_acc: 0.1282
Epoch 43/50
698/698 [==============================] - 1s 986us/step - loss: 0.0148 - acc: 1.0000 - val\_loss: 3.7788 - val\_acc: 0.1410
Epoch 44/50
698/698 [==============================] - 1s 861us/step - loss: 0.0143 - acc: 1.0000 - val\_loss: 3.7850 - val\_acc: 0.1282
Epoch 45/50
698/698 [==============================] - 1s 872us/step - loss: 0.0140 - acc: 1.0000 - val\_loss: 3.7837 - val\_acc: 0.1282
Epoch 46/50
698/698 [==============================] - 1s 884us/step - loss: 0.0135 - acc: 1.0000 - val\_loss: 3.7644 - val\_acc: 0.1410
Epoch 47/50
698/698 [==============================] - 1s 859us/step - loss: 0.0132 - acc: 1.0000 - val\_loss: 3.7868 - val\_acc: 0.1282
Epoch 48/50
698/698 [==============================] - 1s 879us/step - loss: 0.0128 - acc: 1.0000 - val\_loss: 3.7911 - val\_acc: 0.1282
Epoch 49/50
698/698 [==============================] - 1s 897us/step - loss: 0.0125 - acc: 1.0000 - val\_loss: 3.7819 - val\_acc: 0.1282
Epoch 50/50
698/698 [==============================] - 1s 1ms/step - loss: 0.0122 - acc: 1.0000 - val\_loss: 3.7921 - val\_acc: 0.1282
              precision    recall  f1-score   support

    acerolas       0.00      0.00      0.00         4
      apples       0.00      0.00      0.00         5
    apricots       0.00      0.00      0.00         5
    avocados       0.00      0.00      0.00         4
     bananas       0.38      0.33      0.35         9
blackberries       0.23      0.38      0.29         8
 blueberries       0.60      0.38      0.46         8
 cantaloupes       0.29      0.40      0.33         5
    cherries       0.00      0.00      0.00         9
    coconuts       0.00      0.00      0.00         3
        figs       0.00      0.00      0.00         3
 grapefruits       0.00      0.00      0.00         5
      grapes       0.00      0.00      0.00         9
       guava       0.00      0.00      0.00         7
   kiwifruit       0.40      0.22      0.29         9
      lemons       0.00      0.00      0.00         4
       limes       0.00      0.00      0.00         8
      mangos       0.09      0.25      0.13         4
      olives       0.00      0.00      0.00         8
     oranges       0.00      0.00      0.00         7
passionfruit       0.00      0.00      0.00         2
     peaches       0.00      0.00      0.00         6
       pears       0.25      0.25      0.25         4
  pineapples       0.38      0.38      0.38         8
       plums       0.50      0.11      0.18         9
pomegranates       0.00      0.00      0.00         7
 raspberries       0.13      0.22      0.17         9
strawberries       0.05      0.12      0.07         8
    tomatoes       0.14      0.11      0.12         9
 watermelons       0.25      0.11      0.15         9

 avg / total       0.15      0.12      0.12       195

Classifier:  KNN  , dataset(features):  keypoints
Precision:  0.07
Classifier:  NaiveBayes  , dataset(features):  keypoints
Precision:  0.15
Classifier:  DecisionTree  , dataset(features):  keypoints
Precision:  0.06
Classifier:  RandomForest  , dataset(features):  keypoints
Precision:  0.05
Classifier:  SVC  , dataset(features):  keypoints
Precision:  0.12
Classifier:  LinearSVC  , dataset(features):  keypoints
Precision:  0.14
Classifier: MLP, dataset(features):  hist-1D
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_22 (Dense)             (None, 500)               96500     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_23 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_24 (Dense)             (None, 30)                15030     
=================================================================
Total params: 362,030
Trainable params: 362,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Train on 698 samples, validate on 78 samples
Epoch 1/50
698/698 [==============================] - 1s 790us/step - loss: 3.4544 - acc: 0.0201 - val\_loss: 3.3810 - val\_acc: 0.0513
Epoch 2/50
698/698 [==============================] - 0s 267us/step - loss: 3.3778 - acc: 0.0315 - val\_loss: 3.3708 - val\_acc: 0.0513
Epoch 3/50
698/698 [==============================] - 0s 248us/step - loss: 3.3571 - acc: 0.0315 - val\_loss: 3.3639 - val\_acc: 0.0641
Epoch 4/50
698/698 [==============================] - 0s 297us/step - loss: 3.3418 - acc: 0.0458 - val\_loss: 3.3584 - val\_acc: 0.0513
Epoch 5/50
698/698 [==============================] - 0s 297us/step - loss: 3.3287 - acc: 0.0516 - val\_loss: 3.3551 - val\_acc: 0.0513
Epoch 6/50
698/698 [==============================] - 0s 308us/step - loss: 3.3174 - acc: 0.0587 - val\_loss: 3.3509 - val\_acc: 0.0513
Epoch 7/50
698/698 [==============================] - 0s 294us/step - loss: 3.3093 - acc: 0.0602 - val\_loss: 3.3485 - val\_acc: 0.0513
Epoch 8/50
698/698 [==============================] - 0s 304us/step - loss: 3.3021 - acc: 0.0716 - val\_loss: 3.3463 - val\_acc: 0.0513
Epoch 9/50
698/698 [==============================] - 0s 299us/step - loss: 3.2942 - acc: 0.0774 - val\_loss: 3.3447 - val\_acc: 0.0513
Epoch 10/50
698/698 [==============================] - 0s 296us/step - loss: 3.2873 - acc: 0.0802 - val\_loss: 3.3428 - val\_acc: 0.0513
Epoch 11/50
698/698 [==============================] - 0s 282us/step - loss: 3.2811 - acc: 0.0845 - val\_loss: 3.3352 - val\_acc: 0.0513
Epoch 12/50
698/698 [==============================] - 0s 297us/step - loss: 3.2734 - acc: 0.0903 - val\_loss: 3.3281 - val\_acc: 0.0513
Epoch 13/50
698/698 [==============================] - 0s 288us/step - loss: 3.2662 - acc: 0.0960 - val\_loss: 3.3210 - val\_acc: 0.0513
Epoch 14/50
698/698 [==============================] - 0s 290us/step - loss: 3.2624 - acc: 0.1032 - val\_loss: 3.3219 - val\_acc: 0.0513
Epoch 15/50
698/698 [==============================] - 0s 269us/step - loss: 3.2530 - acc: 0.1074 - val\_loss: 3.3200 - val\_acc: 0.0513
Epoch 16/50
698/698 [==============================] - 0s 272us/step - loss: 3.2455 - acc: 0.1117 - val\_loss: 3.3125 - val\_acc: 0.0513
Epoch 17/50
698/698 [==============================] - 0s 301us/step - loss: 3.2408 - acc: 0.1032 - val\_loss: 3.3097 - val\_acc: 0.0513
Epoch 18/50
698/698 [==============================] - 0s 277us/step - loss: 3.2310 - acc: 0.1060 - val\_loss: 3.3044 - val\_acc: 0.0513
Epoch 19/50
698/698 [==============================] - 0s 275us/step - loss: 3.2258 - acc: 0.1060 - val\_loss: 3.3024 - val\_acc: 0.0513
Epoch 20/50
698/698 [==============================] - 0s 185us/step - loss: 3.2195 - acc: 0.1103 - val\_loss: 3.2931 - val\_acc: 0.0641
Epoch 21/50
698/698 [==============================] - 0s 316us/step - loss: 3.2094 - acc: 0.1074 - val\_loss: 3.2814 - val\_acc: 0.0641
Epoch 22/50
698/698 [==============================] - 0s 357us/step - loss: 3.2100 - acc: 0.1117 - val\_loss: 3.2772 - val\_acc: 0.0769
Epoch 23/50
698/698 [==============================] - 0s 393us/step - loss: 3.1977 - acc: 0.1103 - val\_loss: 3.2809 - val\_acc: 0.0769
Epoch 24/50
698/698 [==============================] - 0s 377us/step - loss: 3.1957 - acc: 0.1103 - val\_loss: 3.2609 - val\_acc: 0.0769
Epoch 25/50
698/698 [==============================] - 0s 385us/step - loss: 3.1863 - acc: 0.1160 - val\_loss: 3.2536 - val\_acc: 0.0769
Epoch 26/50
698/698 [==============================] - 0s 375us/step - loss: 3.1765 - acc: 0.1175 - val\_loss: 3.2677 - val\_acc: 0.0897
Epoch 27/50
698/698 [==============================] - 0s 360us/step - loss: 3.1717 - acc: 0.1175 - val\_loss: 3.2619 - val\_acc: 0.0897
Epoch 28/50
698/698 [==============================] - 0s 375us/step - loss: 3.1646 - acc: 0.1175 - val\_loss: 3.2464 - val\_acc: 0.0769
Epoch 29/50
698/698 [==============================] - 0s 375us/step - loss: 3.1552 - acc: 0.1175 - val\_loss: 3.2595 - val\_acc: 0.0897
Epoch 30/50
698/698 [==============================] - 0s 433us/step - loss: 3.1558 - acc: 0.1289 - val\_loss: 3.2494 - val\_acc: 0.0769
Epoch 31/50
698/698 [==============================] - 0s 356us/step - loss: 3.1446 - acc: 0.1246 - val\_loss: 3.2481 - val\_acc: 0.0769
Epoch 32/50
698/698 [==============================] - 0s 398us/step - loss: 3.1426 - acc: 0.1289 - val\_loss: 3.2455 - val\_acc: 0.0769
Epoch 33/50
698/698 [==============================] - 0s 337us/step - loss: 3.1362 - acc: 0.1275 - val\_loss: 3.2429 - val\_acc: 0.0769
Epoch 34/50
698/698 [==============================] - 0s 349us/step - loss: 3.1235 - acc: 0.1275 - val\_loss: 3.2435 - val\_acc: 0.0897
Epoch 35/50
698/698 [==============================] - 0s 334us/step - loss: 3.1223 - acc: 0.1390 - val\_loss: 3.2248 - val\_acc: 0.0769
Epoch 36/50
698/698 [==============================] - 0s 357us/step - loss: 3.1171 - acc: 0.1390 - val\_loss: 3.2418 - val\_acc: 0.0897
Epoch 37/50
698/698 [==============================] - 0s 299us/step - loss: 3.1063 - acc: 0.1418 - val\_loss: 3.2420 - val\_acc: 0.0897
Epoch 38/50
698/698 [==============================] - 0s 313us/step - loss: 3.1052 - acc: 0.1418 - val\_loss: 3.2328 - val\_acc: 0.0769
Epoch 39/50
698/698 [==============================] - 0s 304us/step - loss: 3.0940 - acc: 0.1404 - val\_loss: 3.2238 - val\_acc: 0.0897
Epoch 40/50
698/698 [==============================] - 0s 298us/step - loss: 3.0982 - acc: 0.1404 - val\_loss: 3.2300 - val\_acc: 0.0769
Epoch 41/50
698/698 [==============================] - 0s 294us/step - loss: 3.0808 - acc: 0.1490 - val\_loss: 3.2348 - val\_acc: 0.0769
Epoch 42/50
698/698 [==============================] - 0s 297us/step - loss: 3.0834 - acc: 0.1490 - val\_loss: 3.2362 - val\_acc: 0.0897
Epoch 43/50
698/698 [==============================] - 0s 299us/step - loss: 3.0694 - acc: 0.1519 - val\_loss: 3.2249 - val\_acc: 0.1026
Epoch 44/50
698/698 [==============================] - 0s 284us/step - loss: 3.0673 - acc: 0.1519 - val\_loss: 3.2557 - val\_acc: 0.1026
Epoch 45/50
698/698 [==============================] - 0s 293us/step - loss: 3.0631 - acc: 0.1590 - val\_loss: 3.2605 - val\_acc: 0.1026
Epoch 46/50
698/698 [==============================] - 0s 277us/step - loss: 3.0601 - acc: 0.1504 - val\_loss: 3.2268 - val\_acc: 0.1026
Epoch 47/50
698/698 [==============================] - 0s 297us/step - loss: 3.0538 - acc: 0.1590 - val\_loss: 3.2106 - val\_acc: 0.0897
Epoch 48/50
698/698 [==============================] - 0s 303us/step - loss: 3.0457 - acc: 0.1519 - val\_loss: 3.2372 - val\_acc: 0.1154
Epoch 49/50
698/698 [==============================] - 0s 285us/step - loss: 3.0380 - acc: 0.1605 - val\_loss: 3.1815 - val\_acc: 0.1282
Epoch 50/50
698/698 [==============================] - 0s 301us/step - loss: 3.0426 - acc: 0.1648 - val\_loss: 3.2287 - val\_acc: 0.1154
              precision    recall  f1-score   support

    acerolas       0.00      0.00      0.00         4
      apples       0.00      0.00      0.00         5
    apricots       0.00      0.00      0.00         5
    avocados       0.00      0.00      0.00         4
     bananas       0.50      0.33      0.40         9
blackberries       0.00      0.00      0.00         8
 blueberries       0.00      0.00      0.00         8
 cantaloupes       0.00      0.00      0.00         5
    cherries       0.50      0.11      0.18         9
    coconuts       0.00      0.00      0.00         3
        figs       0.00      0.00      0.00         3
 grapefruits       0.50      0.20      0.29         5
      grapes       0.00      0.00      0.00         9
       guava       0.00      0.00      0.00         7
   kiwifruit       0.00      0.00      0.00         9
      lemons       0.00      0.00      0.00         4
       limes       0.67      0.25      0.36         8
      mangos       0.00      0.00      0.00         4
      olives       0.00      0.00      0.00         8
     oranges       0.00      0.00      0.00         7
passionfruit       0.00      0.00      0.00         2
     peaches       0.50      0.17      0.25         6
       pears       0.00      0.00      0.00         4
  pineapples       0.00      0.00      0.00         8
       plums       0.00      0.00      0.00         9
pomegranates       0.00      0.00      0.00         7
 raspberries       0.50      0.22      0.31         9
strawberries       0.05      0.88      0.09         8
    tomatoes       0.00      0.00      0.00         9
 watermelons       0.00      0.00      0.00         9

 avg / total       0.13      0.09      0.07       195

Classifier:  KNN  , dataset(features):  hist-1D

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn\_for)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Precision:  0.08
Classifier:  NaiveBayes  , dataset(features):  hist-1D
Precision:  0.11
Classifier:  DecisionTree  , dataset(features):  hist-1D
Precision:  0.17
Classifier:  RandomForest  , dataset(features):  hist-1D
Precision:  0.14
Classifier:  SVC  , dataset(features):  hist-1D
Precision:  0.04
Classifier:  LinearSVC  , dataset(features):  hist-1D
Precision:  0.24
Classifier: MLP, dataset(features):  hist-2D
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_25 (Dense)             (None, 500)               384500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_26 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_27 (Dense)             (None, 30)                15030     
=================================================================
Total params: 650,030
Trainable params: 650,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Train on 698 samples, validate on 78 samples
Epoch 1/50
698/698 [==============================] - 1s 1ms/step - loss: 3.4580 - acc: 0.0444 - val\_loss: 3.4739 - val\_acc: 0.0128
Epoch 2/50
698/698 [==============================] - 0s 392us/step - loss: 3.3651 - acc: 0.0344 - val\_loss: 3.4281 - val\_acc: 0.0385
Epoch 3/50
698/698 [==============================] - 0s 383us/step - loss: 3.3249 - acc: 0.0415 - val\_loss: 3.4067 - val\_acc: 0.0385
Epoch 4/50
698/698 [==============================] - 0s 375us/step - loss: 3.3057 - acc: 0.0487 - val\_loss: 3.3719 - val\_acc: 0.0385
Epoch 5/50
698/698 [==============================] - 0s 374us/step - loss: 3.2920 - acc: 0.0573 - val\_loss: 3.3660 - val\_acc: 0.0385
Epoch 6/50
698/698 [==============================] - 0s 392us/step - loss: 3.2761 - acc: 0.0745 - val\_loss: 3.3466 - val\_acc: 0.0513
Epoch 7/50
698/698 [==============================] - 0s 358us/step - loss: 3.2629 - acc: 0.0931 - val\_loss: 3.3351 - val\_acc: 0.0513
Epoch 8/50
698/698 [==============================] - 0s 365us/step - loss: 3.2476 - acc: 0.0960 - val\_loss: 3.3216 - val\_acc: 0.0513
Epoch 9/50
698/698 [==============================] - 0s 374us/step - loss: 3.2379 - acc: 0.1060 - val\_loss: 3.3007 - val\_acc: 0.0513
Epoch 10/50
698/698 [==============================] - 0s 313us/step - loss: 3.2265 - acc: 0.1117 - val\_loss: 3.2961 - val\_acc: 0.0641
Epoch 11/50
698/698 [==============================] - 0s 278us/step - loss: 3.2111 - acc: 0.1175 - val\_loss: 3.2771 - val\_acc: 0.0641
Epoch 12/50
698/698 [==============================] - 0s 296us/step - loss: 3.1962 - acc: 0.1175 - val\_loss: 3.2561 - val\_acc: 0.0769
Epoch 13/50
698/698 [==============================] - 0s 286us/step - loss: 3.1795 - acc: 0.1203 - val\_loss: 3.2465 - val\_acc: 0.0769
Epoch 14/50
698/698 [==============================] - 0s 321us/step - loss: 3.1706 - acc: 0.1246 - val\_loss: 3.2346 - val\_acc: 0.0769
Epoch 15/50
698/698 [==============================] - 0s 299us/step - loss: 3.1551 - acc: 0.1304 - val\_loss: 3.2281 - val\_acc: 0.0769
Epoch 16/50
698/698 [==============================] - 0s 277us/step - loss: 3.1448 - acc: 0.1433 - val\_loss: 3.2039 - val\_acc: 0.0769
Epoch 17/50
698/698 [==============================] - 0s 276us/step - loss: 3.1379 - acc: 0.1433 - val\_loss: 3.2046 - val\_acc: 0.0769
Epoch 18/50
698/698 [==============================] - 0s 307us/step - loss: 3.1225 - acc: 0.1533 - val\_loss: 3.2002 - val\_acc: 0.0769
Epoch 19/50
698/698 [==============================] - 0s 269us/step - loss: 3.1118 - acc: 0.1619 - val\_loss: 3.1882 - val\_acc: 0.0897
Epoch 20/50
698/698 [==============================] - 0s 281us/step - loss: 3.0991 - acc: 0.1691 - val\_loss: 3.1781 - val\_acc: 0.0897
Epoch 21/50
698/698 [==============================] - 0s 295us/step - loss: 3.0831 - acc: 0.1648 - val\_loss: 3.1648 - val\_acc: 0.0897
Epoch 22/50
698/698 [==============================] - 0s 281us/step - loss: 3.0729 - acc: 0.1705 - val\_loss: 3.1697 - val\_acc: 0.0897
Epoch 23/50
698/698 [==============================] - 0s 277us/step - loss: 3.0614 - acc: 0.1791 - val\_loss: 3.1469 - val\_acc: 0.1282
Epoch 24/50
698/698 [==============================] - 0s 275us/step - loss: 3.0488 - acc: 0.1819 - val\_loss: 3.1447 - val\_acc: 0.1410
Epoch 25/50
698/698 [==============================] - 0s 277us/step - loss: 3.0348 - acc: 0.1834 - val\_loss: 3.1402 - val\_acc: 0.1410
Epoch 26/50
698/698 [==============================] - 0s 278us/step - loss: 3.0248 - acc: 0.1877 - val\_loss: 3.1256 - val\_acc: 0.1538
Epoch 27/50
698/698 [==============================] - 0s 282us/step - loss: 3.0068 - acc: 0.1920 - val\_loss: 3.1236 - val\_acc: 0.1282
Epoch 28/50
698/698 [==============================] - 0s 281us/step - loss: 2.9992 - acc: 0.1877 - val\_loss: 3.1146 - val\_acc: 0.1410
Epoch 29/50
698/698 [==============================] - 0s 272us/step - loss: 2.9839 - acc: 0.2049 - val\_loss: 3.1114 - val\_acc: 0.1538
Epoch 30/50
698/698 [==============================] - 0s 324us/step - loss: 2.9792 - acc: 0.2034 - val\_loss: 3.0963 - val\_acc: 0.1410
Epoch 31/50
698/698 [==============================] - 0s 353us/step - loss: 2.9639 - acc: 0.2049 - val\_loss: 3.0923 - val\_acc: 0.1538
Epoch 32/50
698/698 [==============================] - 0s 388us/step - loss: 2.9479 - acc: 0.2106 - val\_loss: 3.0852 - val\_acc: 0.1410
Epoch 33/50
698/698 [==============================] - 0s 348us/step - loss: 2.9412 - acc: 0.2106 - val\_loss: 3.0915 - val\_acc: 0.1410
Epoch 34/50
698/698 [==============================] - 0s 341us/step - loss: 2.9347 - acc: 0.1848 - val\_loss: 3.1084 - val\_acc: 0.1410
Epoch 35/50
698/698 [==============================] - 0s 368us/step - loss: 2.9370 - acc: 0.1891 - val\_loss: 3.1099 - val\_acc: 0.1410
Epoch 36/50
698/698 [==============================] - 0s 363us/step - loss: 2.9147 - acc: 0.1905 - val\_loss: 3.1191 - val\_acc: 0.1282
Epoch 37/50
698/698 [==============================] - 0s 347us/step - loss: 2.9110 - acc: 0.1934 - val\_loss: 3.0826 - val\_acc: 0.1410
Epoch 38/50
698/698 [==============================] - 0s 354us/step - loss: 2.9041 - acc: 0.1948 - val\_loss: 3.0797 - val\_acc: 0.1410
Epoch 39/50
698/698 [==============================] - 0s 368us/step - loss: 2.8883 - acc: 0.1934 - val\_loss: 3.0853 - val\_acc: 0.1410
Epoch 40/50
698/698 [==============================] - 0s 358us/step - loss: 2.8823 - acc: 0.2049 - val\_loss: 3.0943 - val\_acc: 0.1410
Epoch 41/50
698/698 [==============================] - 0s 343us/step - loss: 2.8731 - acc: 0.2034 - val\_loss: 3.1034 - val\_acc: 0.1538
Epoch 42/50
698/698 [==============================] - 0s 350us/step - loss: 2.8585 - acc: 0.2120 - val\_loss: 3.0930 - val\_acc: 0.1538
Epoch 43/50
698/698 [==============================] - 0s 343us/step - loss: 2.8546 - acc: 0.2120 - val\_loss: 3.1148 - val\_acc: 0.1538
Epoch 44/50
698/698 [==============================] - 0s 372us/step - loss: 2.8491 - acc: 0.2221 - val\_loss: 3.1095 - val\_acc: 0.1538
Epoch 45/50
698/698 [==============================] - 0s 377us/step - loss: 2.8317 - acc: 0.2292 - val\_loss: 3.0856 - val\_acc: 0.1410
Epoch 46/50
698/698 [==============================] - 0s 432us/step - loss: 2.8245 - acc: 0.2350 - val\_loss: 3.0536 - val\_acc: 0.1538
Epoch 47/50
698/698 [==============================] - 0s 561us/step - loss: 2.8106 - acc: 0.2364 - val\_loss: 3.0649 - val\_acc: 0.1538
Epoch 48/50
698/698 [==============================] - 0s 579us/step - loss: 2.8042 - acc: 0.2364 - val\_loss: 3.0829 - val\_acc: 0.1410
Epoch 49/50
698/698 [==============================] - 0s 506us/step - loss: 2.7959 - acc: 0.2393 - val\_loss: 3.0557 - val\_acc: 0.1410
Epoch 50/50
698/698 [==============================] - 0s 503us/step - loss: 2.7859 - acc: 0.2464 - val\_loss: 3.0694 - val\_acc: 0.1410
              precision    recall  f1-score   support

    acerolas       0.00      0.00      0.00         4
      apples       0.00      0.00      0.00         5
    apricots       0.00      0.00      0.00         5
    avocados       0.00      0.00      0.00         4
     bananas       1.00      0.11      0.20         9
blackberries       0.50      0.25      0.33         8
 blueberries       1.00      0.25      0.40         8
 cantaloupes       0.25      0.20      0.22         5
    cherries       0.15      0.56      0.23         9
    coconuts       0.00      0.00      0.00         3
        figs       0.00      0.00      0.00         3
 grapefruits       0.00      0.00      0.00         5
      grapes       0.00      0.00      0.00         9
       guava       0.20      0.14      0.17         7
   kiwifruit       0.00      0.00      0.00         9
      lemons       0.00      0.00      0.00         4
       limes       0.50      0.25      0.33         8
      mangos       0.25      0.25      0.25         4
      olives       1.00      0.12      0.22         8
     oranges       0.75      0.43      0.55         7
passionfruit       0.00      0.00      0.00         2
     peaches       1.00      0.33      0.50         6
       pears       0.00      0.00      0.00         4
  pineapples       0.00      0.00      0.00         8
       plums       0.00      0.00      0.00         9
pomegranates       0.00      0.00      0.00         7
 raspberries       0.43      0.33      0.38         9
strawberries       0.04      0.50      0.08         8
    tomatoes       0.00      0.00      0.00         9
 watermelons       1.00      0.11      0.20         9

 avg / total       0.32      0.15      0.15       195

Classifier:  KNN  , dataset(features):  hist-2D

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn\_for)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Precision:  0.14
Classifier:  NaiveBayes  , dataset(features):  hist-2D
Precision:  0.18
Classifier:  DecisionTree  , dataset(features):  hist-2D
Precision:  0.3
Classifier:  RandomForest  , dataset(features):  hist-2D
Precision:  0.35
Classifier:  SVC  , dataset(features):  hist-2D
Precision:  0.05
Classifier:  LinearSVC  , dataset(features):  hist-2D
Precision:  0.38
Classifier: MLP, dataset(features):  hist-3D
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_28 (Dense)             (None, 500)               256500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_29 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_30 (Dense)             (None, 30)                15030     
=================================================================
Total params: 522,030
Trainable params: 522,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Train on 698 samples, validate on 78 samples
Epoch 1/50
698/698 [==============================] - 1s 2ms/step - loss: 3.5104 - acc: 0.0372 - val\_loss: 3.3549 - val\_acc: 0.0641
Epoch 2/50
698/698 [==============================] - 0s 421us/step - loss: 3.3451 - acc: 0.0415 - val\_loss: 3.3172 - val\_acc: 0.0769
Epoch 3/50
698/698 [==============================] - 0s 443us/step - loss: 3.3162 - acc: 0.0544 - val\_loss: 3.2890 - val\_acc: 0.1026
Epoch 4/50
698/698 [==============================] - 0s 354us/step - loss: 3.3010 - acc: 0.0673 - val\_loss: 3.2694 - val\_acc: 0.1154
Epoch 5/50
698/698 [==============================] - 0s 396us/step - loss: 3.2855 - acc: 0.0802 - val\_loss: 3.2520 - val\_acc: 0.1154
Epoch 6/50
698/698 [==============================] - 0s 437us/step - loss: 3.2719 - acc: 0.0931 - val\_loss: 3.2345 - val\_acc: 0.1282
Epoch 7/50
698/698 [==============================] - 0s 423us/step - loss: 3.2617 - acc: 0.0960 - val\_loss: 3.2091 - val\_acc: 0.1410
Epoch 8/50
698/698 [==============================] - 0s 394us/step - loss: 3.2517 - acc: 0.1132 - val\_loss: 3.2025 - val\_acc: 0.1538
Epoch 9/50
698/698 [==============================] - 0s 390us/step - loss: 3.2384 - acc: 0.1189 - val\_loss: 3.1865 - val\_acc: 0.1667
Epoch 10/50
698/698 [==============================] - 0s 377us/step - loss: 3.2212 - acc: 0.1318 - val\_loss: 3.1612 - val\_acc: 0.1667
Epoch 11/50
698/698 [==============================] - 0s 398us/step - loss: 3.2157 - acc: 0.1361 - val\_loss: 3.1590 - val\_acc: 0.1667
Epoch 12/50
698/698 [==============================] - 0s 349us/step - loss: 3.1997 - acc: 0.1447 - val\_loss: 3.1394 - val\_acc: 0.1667
Epoch 13/50
698/698 [==============================] - 0s 372us/step - loss: 3.1862 - acc: 0.1461 - val\_loss: 3.1185 - val\_acc: 0.1923
Epoch 14/50
698/698 [==============================] - 0s 314us/step - loss: 3.1763 - acc: 0.1461 - val\_loss: 3.1126 - val\_acc: 0.1923
Epoch 15/50
698/698 [==============================] - 0s 314us/step - loss: 3.1626 - acc: 0.1461 - val\_loss: 3.0923 - val\_acc: 0.2179
Epoch 16/50
698/698 [==============================] - 0s 380us/step - loss: 3.1563 - acc: 0.1562 - val\_loss: 3.0855 - val\_acc: 0.2179
Epoch 17/50
698/698 [==============================] - 0s 317us/step - loss: 3.1401 - acc: 0.1562 - val\_loss: 3.0699 - val\_acc: 0.2051
Epoch 18/50
698/698 [==============================] - 0s 458us/step - loss: 3.1289 - acc: 0.1633 - val\_loss: 3.0563 - val\_acc: 0.2051
Epoch 19/50
698/698 [==============================] - 0s 321us/step - loss: 3.1148 - acc: 0.1676 - val\_loss: 3.0570 - val\_acc: 0.2051
Epoch 20/50
698/698 [==============================] - 0s 342us/step - loss: 3.1080 - acc: 0.1777 - val\_loss: 3.0369 - val\_acc: 0.1923
Epoch 21/50
698/698 [==============================] - 0s 346us/step - loss: 3.0879 - acc: 0.1791 - val\_loss: 3.0125 - val\_acc: 0.1923
Epoch 22/50
698/698 [==============================] - 0s 385us/step - loss: 3.0811 - acc: 0.1791 - val\_loss: 3.0271 - val\_acc: 0.2051
Epoch 23/50
698/698 [==============================] - 0s 432us/step - loss: 3.0743 - acc: 0.1963 - val\_loss: 2.9949 - val\_acc: 0.1923
Epoch 24/50
698/698 [==============================] - 0s 365us/step - loss: 3.0557 - acc: 0.2006 - val\_loss: 2.9758 - val\_acc: 0.1923
Epoch 25/50
698/698 [==============================] - 0s 353us/step - loss: 3.0380 - acc: 0.2049 - val\_loss: 2.9499 - val\_acc: 0.2051
Epoch 26/50
698/698 [==============================] - 0s 377us/step - loss: 3.0160 - acc: 0.2307 - val\_loss: 2.9951 - val\_acc: 0.1923
Epoch 27/50
698/698 [==============================] - 0s 320us/step - loss: 3.0356 - acc: 0.1963 - val\_loss: 2.9412 - val\_acc: 0.1667
Epoch 28/50
698/698 [==============================] - 0s 331us/step - loss: 3.0096 - acc: 0.1905 - val\_loss: 2.9193 - val\_acc: 0.1795
Epoch 29/50
698/698 [==============================] - 0s 292us/step - loss: 2.9864 - acc: 0.2034 - val\_loss: 2.9139 - val\_acc: 0.1923
Epoch 30/50
698/698 [==============================] - 0s 237us/step - loss: 2.9703 - acc: 0.2034 - val\_loss: 2.8948 - val\_acc: 0.1923
Epoch 31/50
698/698 [==============================] - 0s 228us/step - loss: 2.9600 - acc: 0.2135 - val\_loss: 2.8985 - val\_acc: 0.2179
Epoch 32/50
698/698 [==============================] - 0s 227us/step - loss: 2.9473 - acc: 0.2321 - val\_loss: 2.8952 - val\_acc: 0.2179
Epoch 33/50
698/698 [==============================] - 0s 270us/step - loss: 2.9300 - acc: 0.2421 - val\_loss: 2.8807 - val\_acc: 0.2436
Epoch 34/50
698/698 [==============================] - 0s 237us/step - loss: 2.9081 - acc: 0.2564 - val\_loss: 2.8685 - val\_acc: 0.1923
Epoch 35/50
698/698 [==============================] - 0s 250us/step - loss: 2.9197 - acc: 0.2163 - val\_loss: 2.8632 - val\_acc: 0.2051
Epoch 36/50
698/698 [==============================] - 0s 260us/step - loss: 2.8923 - acc: 0.2292 - val\_loss: 2.8515 - val\_acc: 0.1923
Epoch 37/50
698/698 [==============================] - 0s 264us/step - loss: 2.8901 - acc: 0.2350 - val\_loss: 2.8432 - val\_acc: 0.2179
Epoch 38/50
698/698 [==============================] - 0s 290us/step - loss: 2.8677 - acc: 0.2479 - val\_loss: 2.8739 - val\_acc: 0.2308
Epoch 39/50
698/698 [==============================] - 0s 247us/step - loss: 2.8462 - acc: 0.2794 - val\_loss: 2.8271 - val\_acc: 0.2051
Epoch 40/50
698/698 [==============================] - 0s 254us/step - loss: 2.8479 - acc: 0.2264 - val\_loss: 2.8301 - val\_acc: 0.1923
Epoch 41/50
698/698 [==============================] - 0s 276us/step - loss: 2.8445 - acc: 0.2335 - val\_loss: 2.8185 - val\_acc: 0.1795
Epoch 42/50
698/698 [==============================] - 0s 285us/step - loss: 2.8179 - acc: 0.2393 - val\_loss: 2.8223 - val\_acc: 0.2051
Epoch 43/50
698/698 [==============================] - 0s 253us/step - loss: 2.8093 - acc: 0.2636 - val\_loss: 2.8291 - val\_acc: 0.2436
Epoch 44/50
698/698 [==============================] - 0s 292us/step - loss: 2.7892 - acc: 0.2765 - val\_loss: 2.7837 - val\_acc: 0.2436
Epoch 45/50
698/698 [==============================] - 0s 266us/step - loss: 2.7860 - acc: 0.2837 - val\_loss: 2.8032 - val\_acc: 0.2051
Epoch 46/50
698/698 [==============================] - 0s 250us/step - loss: 2.7889 - acc: 0.2536 - val\_loss: 2.7790 - val\_acc: 0.2179
Epoch 47/50
698/698 [==============================] - 0s 246us/step - loss: 2.7695 - acc: 0.2708 - val\_loss: 2.7717 - val\_acc: 0.2564
Epoch 48/50
698/698 [==============================] - 0s 242us/step - loss: 2.7431 - acc: 0.2966 - val\_loss: 2.7635 - val\_acc: 0.2692
Epoch 49/50
698/698 [==============================] - 0s 257us/step - loss: 2.7401 - acc: 0.2894 - val\_loss: 2.7851 - val\_acc: 0.2179
Epoch 50/50
698/698 [==============================] - 0s 267us/step - loss: 2.7381 - acc: 0.2708 - val\_loss: 2.7629 - val\_acc: 0.2308

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/xxx/anaconda2/envs/snakes/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn\_for)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

    acerolas       0.00      0.00      0.00         4
      apples       0.25      0.20      0.22         5
    apricots       0.00      0.00      0.00         5
    avocados       0.00      0.00      0.00         4
     bananas       0.50      0.22      0.31         9
blackberries       0.67      0.50      0.57         8
 blueberries       0.50      0.12      0.20         8
 cantaloupes       0.06      0.40      0.10         5
    cherries       0.00      0.00      0.00         9
    coconuts       0.50      0.33      0.40         3
        figs       0.00      0.00      0.00         3
 grapefruits       0.00      0.00      0.00         5
      grapes       0.00      0.00      0.00         9
       guava       0.08      0.29      0.12         7
   kiwifruit       0.00      0.00      0.00         9
      lemons       0.00      0.00      0.00         4
       limes       0.60      0.38      0.46         8
      mangos       0.00      0.00      0.00         4
      olives       0.50      0.12      0.20         8
     oranges       0.60      0.43      0.50         7
passionfruit       0.00      0.00      0.00         2
     peaches       0.00      0.00      0.00         6
       pears       0.00      0.00      0.00         4
  pineapples       0.00      0.00      0.00         8
       plums       0.04      0.11      0.06         9
pomegranates       0.00      0.00      0.00         7
 raspberries       0.33      0.11      0.17         9
strawberries       0.16      0.88      0.27         8
    tomatoes       0.00      0.00      0.00         9
 watermelons       1.00      0.11      0.20         9

 avg / total       0.23      0.15      0.14       195

Classifier:  KNN  , dataset(features):  hist-3D
Precision:  0.16
Classifier:  NaiveBayes  , dataset(features):  hist-3D
Precision:  0.22
Classifier:  DecisionTree  , dataset(features):  hist-3D
Precision:  0.32
Classifier:  RandomForest  , dataset(features):  hist-3D
Precision:  0.44
Classifier:  SVC  , dataset(features):  hist-3D
Precision:  0.05
Classifier:  LinearSVC  , dataset(features):  hist-3D
Precision:  0.39

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}310}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{scipy}
          \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{misc} \PY{k}{import} \PY{n}{imread}
          \PY{c+c1}{\PYZsh{}import cPickle as pickle}
          \PY{k+kn}{import} \PY{n+nn}{random}
          \PY{k+kn}{import} \PY{n+nn}{os}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
          
          \PY{c+c1}{\PYZsh{} Feature extractor}
          \PY{k}{def} \PY{n+nf}{extract\PYZus{}features}\PY{p}{(}\PY{n}{image\PYZus{}path}\PY{p}{,} \PY{n}{vector\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{)}\PY{p}{:}
              \PY{n}{image} \PY{o}{=} \PY{n}{imread}\PY{p}{(}\PY{n}{image\PYZus{}path}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RGB}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{k}{try}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} Using KAZE, cause SIFT, ORB and other was moved to additional module}
                  \PY{c+c1}{\PYZsh{} which is adding addtional pain during install}
                  \PY{n}{alg} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{KAZE\PYZus{}create}\PY{p}{(}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Dinding image keypoints}
                  \PY{n}{kps} \PY{o}{=} \PY{n}{alg}\PY{o}{.}\PY{n}{detect}\PY{p}{(}\PY{n}{image}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Getting first 32 of them. }
                  \PY{c+c1}{\PYZsh{} Number of keypoints is varies depend on image size and color pallet}
                  \PY{c+c1}{\PYZsh{} Sorting them based on keypoint response value(bigger is better)}
                  \PY{n}{kps} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{kps}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{n}{x}\PY{o}{.}\PY{n}{response}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{vector\PYZus{}size}\PY{p}{]}
                  \PY{c+c1}{\PYZsh{} computing descriptors vector}
                  \PY{n}{kps}\PY{p}{,} \PY{n}{dsc} \PY{o}{=} \PY{n}{alg}\PY{o}{.}\PY{n}{compute}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{kps}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Flatten all of them in one big vector \PYZhy{} our feature vector}
                  \PY{n}{dsc} \PY{o}{=} \PY{n}{dsc}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Making descriptor of same size}
                  \PY{c+c1}{\PYZsh{} Descriptor vector size is 64}
                  \PY{n}{needed\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{n}{vector\PYZus{}size} \PY{o}{*} \PY{l+m+mi}{64}\PY{p}{)}
                  \PY{k}{if} \PY{n}{dsc}\PY{o}{.}\PY{n}{size} \PY{o}{\PYZlt{}} \PY{n}{needed\PYZus{}size}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{} if we have less the 32 descriptors then just adding zeros at the}
                      \PY{c+c1}{\PYZsh{} end of our feature vector}
                      \PY{n}{dsc} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{dsc}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{needed\PYZus{}size} \PY{o}{\PYZhy{}} \PY{n}{dsc}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{]}\PY{p}{)}
              \PY{k}{except} \PY{n}{cv2}\PY{o}{.}\PY{n}{error} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Error: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{e}\PY{p}{)}
                  \PY{k}{return} \PY{k+kc}{None}
          
              \PY{k}{return} \PY{n}{dsc}
          
          
          \PY{k}{def} \PY{n+nf}{batch\PYZus{}extractor}\PY{p}{(}\PY{n}{images\PYZus{}path}\PY{p}{,} \PY{n}{pickled\PYZus{}db\PYZus{}path}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{features.pck}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
              \PY{n}{files} \PY{o}{=} \PY{p}{[}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{images\PYZus{}path}\PY{p}{,} \PY{n}{p}\PY{p}{)} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{n}{images\PYZus{}path}\PY{p}{)}\PY{p}{)}\PY{p}{]}
          
              \PY{n}{result} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
              \PY{k}{for} \PY{n}{f} \PY{o+ow}{in} \PY{n}{files}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Extracting features from image }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{f}\PY{p}{)}
                  \PY{n}{name} \PY{o}{=} \PY{n}{f}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
                  \PY{n}{result}\PY{p}{[}\PY{n}{name}\PY{p}{]} \PY{o}{=} \PY{n}{extract\PYZus{}features}\PY{p}{(}\PY{n}{f}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} saving all our feature vectors in pickled file}
              \PY{c+c1}{\PYZsh{}with open(pickled\PYZus{}db\PYZus{}path, \PYZsq{}w\PYZsq{}) as fp:}
              \PY{c+c1}{\PYZsh{}    pickle.dump(result, fp)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}142}]:} 
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_15 (Dense)             (None, 256)               131328    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_16 (Dense)             (None, 256)               65792     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_17 (Dense)             (None, 30)                7710      
=================================================================
Total params: 204,830
Trainable params: 204,830
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}141}]:} \PY{n}{images\PYZus{}flat} \PY{o}{=} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{images\PYZus{}flat}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}141}]:} 30
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}146}]:} \PY{n}{img\PYZus{}array}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}146}]:} (1050, 40, 100)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}150}]:} \PY{n}{dataOpenCV\PYZus{}3D}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}150}]:} (512,)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}153}]:} \PY{n}{train}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        AttributeError                            Traceback (most recent call last)

        <ipython-input-153-5e6a15ce28a5> in <module>()
    ----> 1 train.shape
    

        AttributeError: 'list' object has no attribute 'shape'

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
